import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.filterwarnings('ignore')


class ModelComparisonTool:
    """æ¨¡å‹å¯¹æ¯”éªŒè¯å·¥å…·"""

    def __init__(self):
        self.results = {}

    def prepare_features_original(self, df):
        """åŸå§‹ç‰ˆæœ¬çš„ç‰¹å¾å‡†å¤‡"""
        exclude_cols = ['HeightLabel', 'Train_OD_Project', 'ObjName', 'Direction']

        # åˆ›å»ºå·¥ç¨‹ç‰¹å¾ï¼ˆä¸åŸä»£ç ç›¸åŒï¼‰
        df_processed = df.copy()

        feature_ops = {
            'DeEcho_Ratio': lambda x: x['PosDeDis1'] / (x['PosDeDis2'] + 1e-8),
            'CeEcho_Ratio': lambda x: x['PosCeDis1'] / (x['PosCeDis2'] + 1e-8),
            'DeAmp_Ratio': lambda x: x['PosDeAmp1'] / (x['PosDeAmp2'] + 1e-8),
            'CeAmp_Ratio': lambda x: x['PosCeAmp1'] / (x['PosCeAmp2'] + 1e-8),
            'Total_DeEcho': lambda x: x['PosDeDis1'] + x['PosDeDis2'],
            'Total_CeEcho': lambda x: x['PosCeDis1'] + x['PosCeDis2'],
            'Total_DeAmp': lambda x: x['PosDeAmp1'] + x['PosDeAmp2'],
            'Total_CeAmp': lambda x: x['PosCeAmp1'] + x['PosCeAmp2'],
            'DeDis_Diff': lambda x: x['PosDeDis1'] - x['PosDeDis2'],
            'CeDis_Diff': lambda x: x['PosCeDis1'] - x['PosCeDis2'],
            'DeAmp_Diff': lambda x: x['PosDeAmp1'] - x['PosDeAmp2'],
            'CeAmp_Diff': lambda x: x['PosCeAmp1'] - x['PosCeAmp2'],
            'Avg_Echo_Strength': lambda x: (x['AvgDeEchoHigh_SameTx'] + x['AvgCeEchoHigh_SameTxRx']) / 2,
            'Distance_Ratio': lambda x: x['TrainObjDist'] / (x['AngleDist'] + 1e-8)
        }

        for feature_name, operation in feature_ops.items():
            df_processed[feature_name] = operation(df_processed)

        feature_names = [col for col in df_processed.columns if col not in exclude_cols]
        print(f"åŸå§‹ç‰ˆæœ¬ç‰¹å¾æ•°: {len(feature_names)}")
        print(f"åŒ…å«çš„é—®é¢˜ç‰¹å¾: {[f for f in ['Obj_ID', 'CurCyc', 'TxSensID'] if f in feature_names]}")

        return df_processed[feature_names], feature_names

    def prepare_features_improved(self, df):
        """æ”¹è¿›ç‰ˆæœ¬çš„ç‰¹å¾å‡†å¤‡"""
        exclude_cols = [
            'HeightLabel', 'Train_OD_Project', 'ObjName', 'Direction',
            'Obj_ID', 'CurCyc', 'TxSensID'  # æ–°å¢æ’é™¤
        ]

        df_processed = df.copy()

        feature_ops = {
            'DeEcho_Ratio': lambda x: x['PosDeDis1'] / (x['PosDeDis2'] + 1e-8),
            'CeEcho_Ratio': lambda x: x['PosCeDis1'] / (x['PosCeDis2'] + 1e-8),
            'DeAmp_Ratio': lambda x: x['PosDeAmp1'] / (x['PosDeAmp2'] + 1e-8),
            'CeAmp_Ratio': lambda x: x['PosCeAmp1'] / (x['PosCeAmp2'] + 1e-8),
            'Total_DeEcho': lambda x: x['PosDeDis1'] + x['PosDeDis2'],
            'Total_CeEcho': lambda x: x['PosCeDis1'] + x['PosCeDis2'],
            'Total_DeAmp': lambda x: x['PosDeAmp1'] + x['PosDeAmp2'],
            'Total_CeAmp': lambda x: x['PosCeAmp1'] + x['PosCeAmp2'],
            'DeDis_Diff': lambda x: x['PosDeDis1'] - x['PosDeDis2'],
            'CeDis_Diff': lambda x: x['PosCeDis1'] - x['PosCeDis2'],
            'DeAmp_Diff': lambda x: x['PosDeAmp1'] - x['PosDeAmp2'],
            'CeAmp_Diff': lambda x: x['PosCeAmp1'] - x['PosCeAmp2'],
            'Avg_Echo_Strength': lambda x: (x['AvgDeEchoHigh_SameTx'] + x['AvgCeEchoHigh_SameTxRx']) / 2,
            'Distance_Ratio': lambda x: x['TrainObjDist'] / (x['AngleDist'] + 1e-8),
            'Echo_Strength_Ratio': lambda x: x['AvgDeEchoHigh_SameTx'] / (x['AvgCeEchoHigh_SameTxRx'] + 1e-8),
            'Odo_Stability': lambda x: x['OdoDiffObjDis'] / (x['OdoDiffDeDis'] + 1e-8),
        }

        for feature_name, operation in feature_ops.items():
            try:
                df_processed[feature_name] = operation(df_processed)
            except:
                pass

        feature_names = [col for col in df_processed.columns if col not in exclude_cols]
        print(f"æ”¹è¿›ç‰ˆæœ¬ç‰¹å¾æ•°: {len(feature_names)}")
        print(f"å·²æ’é™¤é—®é¢˜ç‰¹å¾: {[f for f in ['Obj_ID', 'CurCyc', 'TxSensID'] if f not in feature_names]}")

        return df_processed[feature_names], feature_names

    def cross_validation_comparison(self, df, cv_folds=5):
        """äº¤å‰éªŒè¯å¯¹æ¯”ä¸¤ä¸ªç‰ˆæœ¬"""
        print("=== äº¤å‰éªŒè¯å¯¹æ¯” ===")

        # å‡†å¤‡æ•°æ®
        X_original, features_original = self.prepare_features_original(df)
        X_improved, features_improved = self.prepare_features_improved(df)
        y = df['HeightLabel']

        # LightGBMå‚æ•°
        params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'random_state': 42,
            'is_unbalance': True
        }

        # äº¤å‰éªŒè¯
        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)

        results = {
            'original': {'accuracy': [], 'auc': []},
            'improved': {'accuracy': [], 'auc': []}
        }

        print(f"å¼€å§‹ {cv_folds} æŠ˜äº¤å‰éªŒè¯...")

        for fold, (train_idx, val_idx) in enumerate(cv.split(X_original, y)):
            print(f"\nç¬¬ {fold + 1} æŠ˜:")

            # åŸå§‹ç‰ˆæœ¬
            X_train_orig, X_val_orig = X_original.iloc[train_idx], X_original.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

            train_data = lgb.Dataset(X_train_orig, label=y_train)
            # ä¿®å¤ï¼šç§»é™¤ verbose_eval å‚æ•°
            model_orig = lgb.train(params, train_data, num_boost_round=500)

            y_pred_orig = model_orig.predict(X_val_orig)
            y_pred_binary_orig = (y_pred_orig > 0.5).astype(int)

            acc_orig = accuracy_score(y_val, y_pred_binary_orig)
            auc_orig = roc_auc_score(y_val, y_pred_orig)

            results['original']['accuracy'].append(acc_orig)
            results['original']['auc'].append(auc_orig)

            # æ”¹è¿›ç‰ˆæœ¬
            X_train_imp, X_val_imp = X_improved.iloc[train_idx], X_improved.iloc[val_idx]

            train_data = lgb.Dataset(X_train_imp, label=y_train)
            # ä¿®å¤ï¼šç§»é™¤ verbose_eval å‚æ•°
            model_imp = lgb.train(params, train_data, num_boost_round=500)

            y_pred_imp = model_imp.predict(X_val_imp)
            y_pred_binary_imp = (y_pred_imp > 0.5).astype(int)

            acc_imp = accuracy_score(y_val, y_pred_binary_imp)
            auc_imp = roc_auc_score(y_val, y_pred_imp)

            results['improved']['accuracy'].append(acc_imp)
            results['improved']['auc'].append(auc_imp)

            print(f"  åŸå§‹ç‰ˆæœ¬ - å‡†ç¡®ç‡: {acc_orig:.4f}, AUC: {auc_orig:.4f}")
            print(f"  æ”¹è¿›ç‰ˆæœ¬ - å‡†ç¡®ç‡: {acc_imp:.4f}, AUC: {auc_imp:.4f}")

        # è®¡ç®—ç»Ÿè®¡ç»“æœ
        self.results = results
        return results

    def analyze_results(self):
        """åˆ†æå¯¹æ¯”ç»“æœ"""
        if not self.results:
            print("è¯·å…ˆè¿è¡Œäº¤å‰éªŒè¯")
            return

        results = self.results

        print("\n" + "=" * 60)
        print("=== äº¤å‰éªŒè¯ç»“æœåˆ†æ ===")
        print("=" * 60)

        # è®¡ç®—ç»Ÿè®¡å€¼
        orig_acc_mean = np.mean(results['original']['accuracy'])
        orig_acc_std = np.std(results['original']['accuracy'])
        orig_auc_mean = np.mean(results['original']['auc'])
        orig_auc_std = np.std(results['original']['auc'])

        imp_acc_mean = np.mean(results['improved']['accuracy'])
        imp_acc_std = np.std(results['improved']['accuracy'])
        imp_auc_mean = np.mean(results['improved']['auc'])
        imp_auc_std = np.std(results['improved']['auc'])

        print("ğŸ“Š å‡†ç¡®ç‡å¯¹æ¯”:")
        print(f"  åŸå§‹ç‰ˆæœ¬: {orig_acc_mean:.4f} Â± {orig_acc_std:.4f}")
        print(f"  æ”¹è¿›ç‰ˆæœ¬: {imp_acc_mean:.4f} Â± {imp_acc_std:.4f}")
        print(f"  å·®å¼‚: {imp_acc_mean - orig_acc_mean:+.4f}")

        print("\nğŸ“Š AUCå¯¹æ¯”:")
        print(f"  åŸå§‹ç‰ˆæœ¬: {orig_auc_mean:.4f} Â± {orig_auc_std:.4f}")
        print(f"  æ”¹è¿›ç‰ˆæœ¬: {imp_auc_mean:.4f} Â± {imp_auc_std:.4f}")
        print(f"  å·®å¼‚: {imp_auc_mean - orig_auc_mean:+.4f}")

        # ç¨³å®šæ€§åˆ†æ
        print(f"\nğŸ“ˆ æ¨¡å‹ç¨³å®šæ€§:")
        print(f"  åŸå§‹ç‰ˆæœ¬å‡†ç¡®ç‡æ ‡å‡†å·®: {orig_acc_std:.4f}")
        print(f"  æ”¹è¿›ç‰ˆæœ¬å‡†ç¡®ç‡æ ‡å‡†å·®: {imp_acc_std:.4f}")
        stability_improvement = orig_acc_std - imp_acc_std
        print(f"  ç¨³å®šæ€§æ”¹è¿›: {stability_improvement:+.4f} ({'æ›´ç¨³å®š' if stability_improvement > 0 else 'ç¨³å®šæ€§ä¸‹é™'})")

        # ç»“è®º
        print(f"\nğŸ¯ ç»“è®º:")
        if imp_acc_mean > orig_acc_mean:
            print("  âœ… æ”¹è¿›ç‰ˆæœ¬æ€§èƒ½æ›´å¥½")
        elif abs(imp_acc_mean - orig_acc_mean) < 0.01:
            print("  ğŸŸ¡ ä¸¤ç‰ˆæœ¬æ€§èƒ½ç›¸å½“ï¼Œä½†æ”¹è¿›ç‰ˆæœ¬é¿å…äº†æ•°æ®æ³„éœ²é£é™©")
        else:
            print("  ğŸ” åŸå§‹ç‰ˆæœ¬æ€§èƒ½çœ‹ä¼¼æ›´å¥½ï¼Œä½†éœ€è¦è¿›ä¸€æ­¥åˆ†æåŸå› ")

        print(f"\nğŸ’¡ åˆ†æå»ºè®®:")
        if orig_acc_mean > imp_acc_mean + 0.01:
            print("  1. åŸå§‹ç‰ˆæœ¬å¯èƒ½å­˜åœ¨æ•°æ®æ³„éœ²ï¼Œéœ€è¦åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸ŠéªŒè¯")
            print("  2. æ£€æŸ¥ Obj_IDã€CurCycã€TxSensID æ˜¯å¦åŒ…å«æ ‡ç­¾ä¿¡æ¯")
            print("  3. æ”¹è¿›ç‰ˆæœ¬è™½ç„¶æ€§èƒ½ç•¥ä½ï¼Œä½†æ›´å¯é å’Œå¯è§£é‡Š")
        else:
            print("  1. æ”¹è¿›ç‰ˆæœ¬æˆåŠŸæ¶ˆé™¤äº†æ•°æ®æ³„éœ²é£é™©")
            print("  2. æ€§èƒ½æ²¡æœ‰æ˜¾è‘—ä¸‹é™ï¼Œæ¨¡å‹æ›´åŠ å¯é ")

    def plot_comparison(self):
        """ç»˜åˆ¶å¯¹æ¯”å›¾"""
        if not self.results:
            print("è¯·å…ˆè¿è¡Œäº¤å‰éªŒè¯")
            return

        results = self.results

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

        # å‡†ç¡®ç‡å¯¹æ¯”
        ax1.boxplot([results['original']['accuracy'], results['improved']['accuracy']],
                    labels=['original version', 'improved version'])
        ax1.set_title('AUC comparison')
        ax1.set_ylabel('AUC')
        ax1.grid(True, alpha=0.3)

        # AUCå¯¹æ¯”
        ax2.boxplot([results['original']['auc'], results['improved']['auc']],
                    labels=['original version', 'improved version'])
        ax2.set_title('AUC comparison')
        ax2.set_ylabel('AUC')
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    def feature_importance_analysis(self, df):
        """ç‰¹å¾é‡è¦æ€§åˆ†æ"""
        print("\n=== ç‰¹å¾é‡è¦æ€§åˆ†æ ===")

        X_original, features_original = self.prepare_features_original(df)
        X_improved, features_improved = self.prepare_features_improved(df)
        y = df['HeightLabel']

        params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'verbose': -1,
            'random_state': 42,
            'is_unbalance': True
        }

        # è®­ç»ƒä¸¤ä¸ªæ¨¡å‹
        train_data_orig = lgb.Dataset(X_original, label=y)
        # ä¿®å¤ï¼šç§»é™¤ verbose_eval å‚æ•°
        model_orig = lgb.train(params, train_data_orig, num_boost_round=500)

        train_data_imp = lgb.Dataset(X_improved, label=y)
        # ä¿®å¤ï¼šç§»é™¤ verbose_eval å‚æ•°
        model_imp = lgb.train(params, train_data_imp, num_boost_round=500)

        # è·å–ç‰¹å¾é‡è¦æ€§
        importance_orig = model_orig.feature_importance(importance_type='gain')
        importance_imp = model_imp.feature_importance(importance_type='gain')

        # åˆ†æåŸå§‹ç‰ˆæœ¬ä¸­é—®é¢˜ç‰¹å¾çš„é‡è¦æ€§
        problematic_features = ['Obj_ID', 'CurCyc', 'TxSensID']

        print("åŸå§‹ç‰ˆæœ¬ä¸­é—®é¢˜ç‰¹å¾çš„é‡è¦æ€§:")
        for i, feature in enumerate(features_original):
            if feature in problematic_features:
                importance_rank = sorted(enumerate(importance_orig), key=lambda x: x[1], reverse=True)
                rank = next((idx for idx, (feat_idx, _) in enumerate(importance_rank) if feat_idx == i), -1)
                print(f"  {feature}: é‡è¦æ€§={importance_orig[i]:.1f}, æ’å={rank + 1}/{len(features_original)}")

        # æ˜¾ç¤ºæ”¹è¿›ç‰ˆæœ¬ Top ç‰¹å¾
        feature_imp_df = pd.DataFrame({
            'feature': features_improved,
            'importance': importance_imp
        }).sort_values('importance', ascending=False)

        print(f"\næ”¹è¿›ç‰ˆæœ¬ Top 10 é‡è¦ç‰¹å¾:")
        print(feature_imp_df.head(10).to_string(index=False))

        return model_orig, model_imp


def run_comparison(data_file):
    """è¿è¡Œå®Œæ•´å¯¹æ¯”åˆ†æ"""
    print("ğŸ”¬ æ¨¡å‹å¯¹æ¯”åˆ†æå·¥å…·")
    print("=" * 50)

    # åŠ è½½æ•°æ®
    try:
        df = pd.read_csv(data_file, encoding='utf-8-sig')
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {df.shape}")
    except:
        try:
            df = pd.read_csv(data_file, encoding='gbk')
            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {df.shape}")
        except:
            df = pd.read_csv(data_file)
            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {df.shape}")

    # åˆ›å»ºå¯¹æ¯”å·¥å…·
    comparator = ModelComparisonTool()

    # è¿è¡Œå¯¹æ¯”åˆ†æ
    print(f"\n1ï¸âƒ£ äº¤å‰éªŒè¯å¯¹æ¯”")
    results = comparator.cross_validation_comparison(df, cv_folds=5)

    print(f"\n2ï¸âƒ£ ç»“æœåˆ†æ")
    comparator.analyze_results()

    print(f"\n3ï¸âƒ£ ç»˜åˆ¶å¯¹æ¯”å›¾")
    comparator.plot_comparison()

    print(f"\n4ï¸âƒ£ ç‰¹å¾é‡è¦æ€§åˆ†æ")
    model_orig, model_imp = comparator.feature_importance_analysis(df)

    return comparator, model_orig, model_imp


if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    DATA_FILE = r'D:\PythonProject\data\processed_data\merged_train_data_fixed.csv'

    comparator, model_orig, model_imp = run_comparison(DATA_FILE)