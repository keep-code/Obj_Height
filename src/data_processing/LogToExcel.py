import re
import pandas as pd
from collections import defaultdict, OrderedDict
import numpy as np
import os
from pathlib import Path


def parse_log_line(line):
    """è§£æå•è¡Œæ—¥å¿—æ•°æ®"""
    pattern = r'zhao_od_ID:(\d+), ObjName:([^,]+), HeightLabel:(\d+), OdDir:(\d+), P1\(([^)]+)\), P2\(([^)]+)\), HeightProb:(\d+), HeightStatus:(\d+), Dist:(\d+), PosDeDis1:(\d+), l_TrainResult:(\d+), probability:(\d+)'
    match = re.match(pattern, line.strip())

    if match:
        return {
            'zhao_od_ID': int(match.group(1)),
            'ObjName': match.group(2),
            'HeightLabel': int(match.group(3)),
            'OdDir': int(match.group(4)),
            'P1': match.group(5),
            'P2': match.group(6),
            'HeightProb': int(match.group(7)),
            'HeightStatus': int(match.group(8)),
            'Dist': int(match.group(9)),
            'PosDeDis1': int(match.group(10)),
            'l_TrainResult': int(match.group(11)),
            'probability': int(match.group(12))
        }
    return None


def get_corrected_height_status_for_high_obstacles(data_list):
    """
    é’ˆå¯¹é«˜éšœç¢ç‰©çš„ç‰¹æ®Šæ ¡å¯¹è§„åˆ™ï¼š
    æŒ‰åŸå§‹logé¡ºåºéå†ï¼Œ2ç±³å†…å‡ºç°ç½®ä¿¡åº¦ä¸º80æˆ–è¿ç»­å°äº160çš„æ•°æ®ï¼Œå¯¹åº”çš„æ ¡å¯¹æ£€æµ‹çŠ¶æ€å…¨éƒ¨è®¾ç½®ä¸ºé«˜
    é™åˆ¶æ¡ä»¶ï¼šå¦‚æœæ¨¡å‹é¢„æµ‹(l_TrainResult)ä¸º0ï¼ˆä½ï¼‰ï¼Œåˆ™ä¸æ”¹å˜æ ¡å¯¹æ£€æµ‹çŠ¶æ€
    """
    # åˆ›å»ºä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨æ¯ä¸ªæ•°æ®ç‚¹çš„æ ¡å¯¹çŠ¶æ€
    corrected_status_map = {}

    # æŒ‰åŸå§‹é¡ºåºéå†
    for i, data in enumerate(data_list):
        # é¦–å…ˆåº”ç”¨åŸºç¡€è§„åˆ™
        if data['PosDeDis1'] <= 500:
            corrected_status_map[i] = 2  # è®¾ä¸ºé«˜
        else:
            corrected_status_map[i] = data['HeightStatus']

    # é’ˆå¯¹é«˜éšœç¢ç‰©çš„ç‰¹æ®Šè§„åˆ™
    i = 0
    while i < len(data_list):
        data = data_list[i]

        # æ£€æŸ¥æ˜¯å¦åœ¨2ç±³ï¼ˆ2000mmï¼‰ä»¥å†…
        if data['Dist'] <= 5000:
            # æ£€æŸ¥æ˜¯å¦å‡ºç°ç½®ä¿¡åº¦ä¸º80
            if data['HeightProb'] == 80:
                # æ–°å¢é™åˆ¶ï¼šå¦‚æœæ¨¡å‹é¢„æµ‹ä¸º0ï¼ˆä½ï¼‰ï¼Œåˆ™ä¸æ”¹å˜çŠ¶æ€
                if data['l_TrainResult'] != 0:
                    corrected_status_map[i] = 2  # è®¾ä¸ºé«˜
                i += 1
                continue

            # æ£€æŸ¥æ˜¯å¦å¼€å§‹è¿ç»­å°äº160çš„åºåˆ—
            if data['HeightProb'] < 160:
                # æ‰¾åˆ°è¿ç»­å°äº160çš„æ‰€æœ‰æ•°æ®ç‚¹
                start_idx = i
                while i < len(data_list) and data_list[i]['Dist'] <= 5000 and data_list[i]['HeightProb'] < 160:
                    # æ–°å¢é™åˆ¶ï¼šå¦‚æœæ¨¡å‹é¢„æµ‹ä¸º0ï¼ˆä½ï¼‰ï¼Œåˆ™ä¸æ”¹å˜çŠ¶æ€
                    if data_list[i]['l_TrainResult'] != 0:
                        corrected_status_map[i] = 2  # è®¾ä¸ºé«˜
                    i += 1

                # å¦‚æœæ‰¾åˆ°äº†è¿ç»­åºåˆ—ï¼Œç»§ç»­
                if i > start_idx:
                    continue

        i += 1

    return corrected_status_map


def get_corrected_height_status(height_status, pos_de_dis1):
    """è·å–æ ¡å¯¹åçš„æ£€æµ‹çŠ¶æ€ï¼ˆåŸºç¡€è§„åˆ™ï¼‰
    å½“é›·è¾¾è·ç¦»<=500æ—¶ï¼Œæ ¡å¯¹æ£€æµ‹çŠ¶æ€è®¾ä¸º2(é«˜)ï¼Œå¦åˆ™ä¿æŒåŸå€¼
    """
    if pos_de_dis1 <= 500:
        return 2  # è®¾ä¸ºé«˜
    else:
        return height_status


def filter_valid_data(data_list):
    """è¿‡æ»¤æ‰é›·è¾¾è·ç¦»ä¸º0çš„æ•°æ®"""
    filtered_data = [data for data in data_list if data['PosDeDis1'] != 0]
    filtered_count = len(data_list) - len(filtered_data)
    if filtered_count > 0:
        print(f"    è¿‡æ»¤æ‰é›·è¾¾è·ç¦»ä¸º0çš„æ•°æ®: {filtered_count} æ¡")
    return filtered_data


def identify_sessions_by_distance_only(all_data, height_label):
    """
    å®Œå…¨é‡å†™çš„é‡‡æ ·è¯†åˆ«ç®—æ³•ï¼š
    åŸºäºè·ç¦»å˜åŒ–çš„å…¨å±€åˆ†æï¼Œè¯†åˆ«å®Œæ•´çš„æ¥è¿‘å’Œè¿œç¦»è¿‡ç¨‹
    æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨æ ¡å¯¹åçš„HeightStatusè¿›è¡Œç®—æ³•å¤„ç†
    """
    # å…ˆè¿‡æ»¤æ‰é›·è¾¾è·ç¦»ä¸º0çš„æ•°æ®
    all_data = filter_valid_data(all_data)

    if len(all_data) <= 1:
        return [all_data], [(data, 1, 'approach') for data in all_data]

    # ä¸ºæ¯ä¸ªæ•°æ®ç‚¹æ·»åŠ æ ¡å¯¹åçš„æ£€æµ‹çŠ¶æ€
    if height_label == 1:  # é«˜éšœç¢ç‰©
        # ä½¿ç”¨é«˜éšœç¢ç‰©çš„ç‰¹æ®Šæ ¡å¯¹è§„åˆ™
        corrected_status_map = get_corrected_height_status_for_high_obstacles(all_data)
        for idx, data in enumerate(all_data):
            data['CorrectedHeightStatus'] = corrected_status_map[idx]
    else:  # ä½éšœç¢ç‰©
        # ä½¿ç”¨åŸºç¡€æ ¡å¯¹è§„åˆ™
        for data in all_data:
            data['CorrectedHeightStatus'] = get_corrected_height_status(data['HeightStatus'], data['PosDeDis1'])

    distances = [d['Dist'] for d in all_data]
    labeled_data = []
    approach_sessions = []

    print(f"    è°ƒè¯•ï¼šå¼€å§‹åˆ†æ {len(distances)} ä¸ªæ•°æ®ç‚¹ï¼Œè·ç¦»èŒƒå›´ {min(distances)}-{max(distances)}")

    # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨æ»‘åŠ¨çª—å£è®¡ç®—æ¯ä¸ªç‚¹çš„å±€éƒ¨è¶‹åŠ¿
    def calculate_local_trends(distances, window_size=8):
        """è®¡ç®—æ¯ä¸ªç‚¹çš„å±€éƒ¨è¶‹åŠ¿å€¼"""
        trends = []

        for i in range(len(distances)):
            # å‘å‰å’Œå‘åå„å–window_size//2ä¸ªç‚¹
            half_window = window_size // 2
            start_idx = max(0, i - half_window)
            end_idx = min(len(distances), i + half_window + 1)

            local_distances = distances[start_idx:end_idx]
            if len(local_distances) < 3:
                trends.append(0)  # æ•°æ®ç‚¹å¤ªå°‘ï¼Œè¶‹åŠ¿ä¸º0
                continue

            # è®¡ç®—çº¿æ€§å›å½’æ–œç‡ä½œä¸ºè¶‹åŠ¿å€¼
            x = list(range(len(local_distances)))
            y = local_distances
            n = len(x)

            sum_x = sum(x)
            sum_y = sum(y)
            sum_xy = sum(x[i] * y[i] for i in range(n))
            sum_x2 = sum(x[i] * x[i] for i in range(n))

            # æ–œç‡ = (n*Î£xy - Î£x*Î£y) / (n*Î£xÂ² - (Î£x)Â²)
            denominator = n * sum_x2 - sum_x * sum_x
            if denominator == 0:
                slope = 0
            else:
                slope = (n * sum_xy - sum_x * sum_y) / denominator

            trends.append(slope)

        return trends

    # è®¡ç®—å±€éƒ¨è¶‹åŠ¿
    trends = calculate_local_trends(distances)

    # ç¬¬äºŒæ­¥ï¼šåŸºäºè¶‹åŠ¿è¯†åˆ«æ¥è¿‘å’Œè¿œç¦»çš„é˜¶æ®µ
    def identify_phases(trends, distances, smoothing_window=5):
        """è¯†åˆ«æ¥è¿‘å’Œè¿œç¦»é˜¶æ®µ"""
        # å¯¹è¶‹åŠ¿è¿›è¡Œå¹³æ»‘å¤„ç†
        smoothed_trends = []
        for i in range(len(trends)):
            start = max(0, i - smoothing_window // 2)
            end = min(len(trends), i + smoothing_window // 2 + 1)
            smoothed_trends.append(sum(trends[start:end]) / (end - start))

        phases = []
        current_phase = 'approach'  # å‡è®¾å¼€å§‹æ˜¯æ¥è¿‘é˜¶æ®µ
        phase_start = 0

        # è®¾ç½®é˜ˆå€¼
        TREND_THRESHOLD = 5.0  # è¶‹åŠ¿é˜ˆå€¼ï¼Œæ­£å€¼è¡¨ç¤ºä¸Šå‡ï¼Œè´Ÿå€¼è¡¨ç¤ºä¸‹é™
        MIN_PHASE_LENGTH = 8  # æœ€å°é˜¶æ®µé•¿åº¦

        for i in range(1, len(smoothed_trends)):
            should_switch = False

            if current_phase == 'approach':
                # æ¥è¿‘é˜¶æ®µï¼šå¯»æ‰¾æ˜æ˜¾çš„ä¸Šå‡è¶‹åŠ¿ï¼ˆè¿œç¦»ï¼‰
                if smoothed_trends[i] > TREND_THRESHOLD and i - phase_start >= MIN_PHASE_LENGTH:
                    # æ£€æŸ¥åç»­å‡ ä¸ªç‚¹æ˜¯å¦ç¡®å®åœ¨ä¸Šå‡
                    look_ahead = min(len(distances) - i, 5)
                    if look_ahead >= 3:
                        future_trend = sum(smoothed_trends[i:i + look_ahead]) / look_ahead
                        if future_trend > TREND_THRESHOLD * 0.5:
                            should_switch = True
            else:  # retreat
                # è¿œç¦»é˜¶æ®µï¼šå¯»æ‰¾æ˜æ˜¾çš„ä¸‹é™è¶‹åŠ¿ï¼ˆæ¥è¿‘ï¼‰
                if smoothed_trends[i] < -TREND_THRESHOLD and i - phase_start >= MIN_PHASE_LENGTH:
                    # æ£€æŸ¥åç»­å‡ ä¸ªç‚¹æ˜¯å¦ç¡®å®åœ¨ä¸‹é™
                    look_ahead = min(len(distances) - i, 5)
                    if look_ahead >= 3:
                        future_trend = sum(smoothed_trends[i:i + look_ahead]) / look_ahead
                        if future_trend < -TREND_THRESHOLD * 0.5:
                            should_switch = True

            if should_switch:
                # è®°å½•å½“å‰é˜¶æ®µ
                phases.append({
                    'type': current_phase,
                    'start': phase_start,
                    'end': i - 1,
                    'length': i - phase_start
                })

                # åˆ‡æ¢åˆ°æ–°é˜¶æ®µ
                current_phase = 'retreat' if current_phase == 'approach' else 'approach'
                phase_start = i

        # æ·»åŠ æœ€åä¸€ä¸ªé˜¶æ®µ
        phases.append({
            'type': current_phase,
            'start': phase_start,
            'end': len(distances) - 1,
            'length': len(distances) - phase_start
        })

        return phases

    # è¯†åˆ«é˜¶æ®µ
    phases = identify_phases(trends, distances)

    print(f"    è°ƒè¯•ï¼šè¯†åˆ«åˆ° {len(phases)} ä¸ªé˜¶æ®µ")
    for i, phase in enumerate(phases):
        phase_type = "æ¥è¿‘" if phase['type'] == 'approach' else "è¿œç¦»"
        start_dist = distances[phase['start']]
        end_dist = distances[phase['end']]
        print(f"      ç¬¬{i + 1}ä¸ªé˜¶æ®µï¼š{phase_type}ï¼Œé•¿åº¦={phase['length']}ï¼Œè·ç¦»={start_dist}->{end_dist}")

    # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆæ ‡è®°æ•°æ®å’Œæå–æ¥è¿‘ä¼šè¯
    approach_count = 0
    retreat_count = 0

    for phase in phases:
        if phase['type'] == 'approach':
            approach_count += 1
            session_data = all_data[phase['start']:phase['end'] + 1]

            # åªæœ‰è¶³å¤Ÿé•¿çš„æ¥è¿‘é˜¶æ®µæ‰è¢«è®°å½•ä¸ºæœ‰æ•ˆä¼šè¯
            if len(session_data) >= 5:
                approach_sessions.append(session_data)

            # æ ‡è®°æ•°æ®
            for i in range(phase['start'], phase['end'] + 1):
                labeled_data.append((all_data[i], approach_count, 'approach'))
        else:  # retreat
            retreat_count += 1
            # æ ‡è®°è¿œç¦»æ•°æ®
            for i in range(phase['start'], phase['end'] + 1):
                labeled_data.append((all_data[i], retreat_count, 'retreat'))

    # ä¸ºæ¯ä¸ªæ¥è¿‘ä¼šè¯æŒ‰è·ç¦»æ’åºï¼ˆä»å¤§åˆ°å°ï¼‰
    for session in approach_sessions:
        session.sort(key=lambda x: x['Dist'], reverse=True)

    print(f"    è°ƒè¯•ï¼šæå–åˆ° {len(approach_sessions)} ä¸ªæœ‰æ•ˆæ¥è¿‘ä¼šè¯")

    return approach_sessions, labeled_data


def analyze_height_status_changes(session_data):
    """åˆ†æå•æ¬¡æµ‹è¯•ä¸­æ ¡å¯¹åHeightStatusçš„å˜åŒ–"""
    session_data.sort(key=lambda x: x['Dist'], reverse=True)

    changes = []
    if not session_data:
        return changes

    # ä½¿ç”¨æ ¡å¯¹åçš„æ£€æµ‹çŠ¶æ€
    current_status = session_data[0]['CorrectedHeightStatus']
    change_start_idx = 0

    for i, data in enumerate(session_data):
        if data['CorrectedHeightStatus'] != current_status:
            changes.append({
                'start_idx': change_start_idx,
                'end_idx': i - 1,
                'status': current_status,
                'start_dist': session_data[change_start_idx]['Dist'],
                'end_dist': session_data[i - 1]['Dist']
            })
            current_status = data['CorrectedHeightStatus']
            change_start_idx = i

    if change_start_idx < len(session_data):
        changes.append({
            'start_idx': change_start_idx,
            'end_idx': len(session_data) - 1,
            'status': current_status,
            'start_dist': session_data[change_start_idx]['Dist'],
            'end_dist': session_data[-1]['Dist']
        })

    return changes


def get_latest_error_distance(changes, height_label):
    """è·å–æœ€è¿‘é”™è¯¯è·ç¦»ï¼ˆæœ€åä¸€æ¬¡å‡ºç°é”™è¯¯çš„è·ç¦»ï¼‰"""
    latest_error_dist = -1  # å¦‚æœæ²¡æœ‰é”™è¯¯ï¼Œè¿”å›-1

    # ä»å‰å¾€åéå†æ‰€æœ‰å˜åŒ–æ®µï¼Œè®°å½•æœ€åä¸€ä¸ªé”™è¯¯æ®µçš„ç»“æŸè·ç¦»
    for change in changes:
        # åˆ¤æ–­å½“å‰æ®µæ˜¯å¦ä¸ºé”™è¯¯ï¼ˆåŸºäºæ ¡å¯¹åçš„çŠ¶æ€ï¼‰
        is_error = not ((height_label == 0 and change['status'] == 1) or
                        (height_label == 1 and change['status'] == 2))
        if is_error:
            latest_error_dist = change['end_dist']  # ä¿®æ”¹ï¼šä½¿ç”¨ç»“æŸè·ç¦»è€Œä¸æ˜¯èµ·å§‹è·ç¦»

    return latest_error_dist


def get_first_error_distance(changes, height_label):
    """è·å–é¦–æ¬¡é”™è¯¯è·ç¦»ï¼ˆç¬¬ä¸€æ¬¡å‡ºç°é”™è¯¯çš„è·ç¦»ï¼‰"""
    for change in changes:
        # åˆ¤æ–­å½“å‰æ®µæ˜¯å¦ä¸ºé”™è¯¯ï¼ˆåŸºäºæ ¡å¯¹åçš„çŠ¶æ€ï¼‰
        is_error = not ((height_label == 0 and change['status'] == 1) or
                        (height_label == 1 and change['status'] == 2))
        if is_error:
            return change['start_dist']  # è¿”å›ç¬¬ä¸€ä¸ªé”™è¯¯æ®µçš„èµ·å§‹è·ç¦»
    return -1  # æ²¡æœ‰é”™è¯¯æ—¶è¿”å›-1


def format_distance_ranges(changes, height_label):
    """æ ¼å¼åŒ–è·ç¦»åŒºé—´ä¿¡æ¯"""
    if not changes:
        return "æ— æ•°æ®"

    wrong_ranges = []
    correct_ranges = []
    first_wrong_dist = None

    for change in changes:
        status_desc = "æŠ¥é«˜" if change['status'] == 2 else "æŠ¥ä½"
        range_desc = f"Dist:{change['start_dist']}-{change['end_dist']}"

        # åˆ¤æ–­æ˜¯å¦ä¸çœŸå®æ ‡ç­¾ä¸€è‡´ï¼ˆåŸºäºæ ¡å¯¹åçš„çŠ¶æ€ï¼‰
        is_correct = (height_label == 0 and change['status'] == 1) or (height_label == 1 and change['status'] == 2)

        if is_correct:
            correct_ranges.append(f"{range_desc}({status_desc})")
        else:
            wrong_ranges.append(f"{range_desc}({status_desc})")
            if first_wrong_dist is None:
                first_wrong_dist = change['start_dist']

    descriptions = []
    if first_wrong_dist is not None:
        descriptions.append(f"é¦–æ¬¡é”™è¯¯æŠ¥å‘Šè·ç¦»:{first_wrong_dist}")

    if wrong_ranges:
        descriptions.append(f"é”™è¯¯åŒºé—´:{'; '.join(wrong_ranges)}")

    if correct_ranges:
        descriptions.append(f"æ­£ç¡®åŒºé—´:{'; '.join(correct_ranges)}")

    # ç®€åŒ–é€»è¾‘ï¼šå¯»æ‰¾æœ€åä¸€æ¬¡æŠ¥é”™åæ˜¯å¦å…¨ç¨‹æ­£ç¡®
    if len(changes) >= 2:  # è‡³å°‘æœ‰2æ®µæ‰å¯èƒ½æœ‰è½¬æŠ˜
        # ä»åå¾€å‰æ‰¾æœ€åä¸€ä¸ªé”™è¯¯æ®µ
        last_error_end_dist = None
        for i in range(len(changes) - 1, -1, -1):
            change = changes[i]
            is_correct = (height_label == 0 and change['status'] == 1) or (height_label == 1 and change['status'] == 2)
            if not is_correct:  # è¿™æ˜¯ä¸€ä¸ªé”™è¯¯æ®µ
                last_error_end_dist = change['end_dist']
                break

        # å¦‚æœæ‰¾åˆ°äº†æœ€åä¸€ä¸ªé”™è¯¯æ®µï¼Œæ£€æŸ¥ä¹‹åæ˜¯å¦å…¨ç¨‹æ­£ç¡®
        if last_error_end_dist is not None:
            # æ£€æŸ¥æœ€åä¸€ä¸ªé”™è¯¯æ®µä¹‹åçš„æ‰€æœ‰æ®µæ˜¯å¦éƒ½æ­£ç¡®
            all_correct_after_error = True
            for change in changes:
                # åªæ£€æŸ¥åœ¨æœ€åé”™è¯¯æ®µä¹‹åçš„æ®µ
                if change['start_dist'] <= last_error_end_dist:
                    is_correct = (height_label == 0 and change['status'] == 1) or (
                            height_label == 1 and change['status'] == 2)
                    if not is_correct:
                        all_correct_after_error = False
                        break

            # å¦‚æœæœ€åé”™è¯¯æ®µä¹‹åå…¨ç¨‹æ­£ç¡®ï¼Œå°±æ·»åŠ è¿™ä¸ªæè¿°
            if all_correct_after_error:
                expected_status = "æŠ¥ä½" if height_label == 0 else "æŠ¥é«˜"
                descriptions.append(f"Dist<{last_error_end_dist}åå…¨ç¨‹{expected_status}")

    return "; ".join(descriptions) if descriptions else "å…¨ç¨‹æ­£ç¡®"


def get_status_analysis(height_label, has_error, changes):
    """è·å–çŠ¶æ€åˆ†ææè¿°"""
    if not has_error:
        return "å…¨ç¨‹æ­£ç¡®"

    for change in changes:
        # åŸºäºæ ¡å¯¹åçš„çŠ¶æ€åˆ¤æ–­é”™è¯¯ç±»å‹
        is_error = not ((height_label == 0 and change['status'] == 1) or
                        (height_label == 1 and change['status'] == 2))
        if is_error:
            if height_label == 0 and change['status'] == 2:
                return "å­˜åœ¨æŠ¥é«˜"
            elif height_label == 1 and change['status'] == 1:
                return "å­˜åœ¨æŠ¥ä½"

    return "å­˜åœ¨é”™è¯¯"


def create_raw_data_sheet(labeled_data, obstacle_groups):
    """åˆ›å»ºåŸå§‹æ•°æ®è¡¨ï¼Œæ˜¾ç¤ºæ¯æ¡æ•°æ®çš„åˆ†ç»„æƒ…å†µï¼Œå¢åŠ æ ¡å¯¹æ£€æµ‹çŠ¶æ€åˆ—"""
    raw_results = []

    for data, count, phase in labeled_data:
        # è·³è¿‡é›·è¾¾è·ç¦»ä¸º0çš„æ•°æ®
        if data['PosDeDis1'] == 0:
            continue

        od_dir_str = "front" if data['OdDir'] == 1 else "rear"
        height_label_str = "ä½éšœç¢ç‰©" if data['HeightLabel'] == 0 else "é«˜éšœç¢ç‰©"
        height_status_str = "é«˜" if data['HeightStatus'] == 2 else "ä½"  # åŸå§‹æ£€æµ‹çŠ¶æ€

        # è·å–æ ¡å¯¹åçš„æ£€æµ‹çŠ¶æ€ï¼ˆå·²ç»åœ¨æ•°æ®ä¸­å­˜å‚¨ï¼‰
        corrected_status = data.get('CorrectedHeightStatus', data['HeightStatus'])
        corrected_status_str = "é«˜" if corrected_status == 2 else "ä½"  # æ ¡å¯¹æ£€æµ‹çŠ¶æ€

        phase_str = f"ç¬¬{count}æ¬¡æ¥è¿‘" if phase == 'approach' else f"ç¬¬{count}æ¬¡è¿œç¦»"

        # ç”ŸæˆpackåŒ…åç§°
        pack_name = f"HY11-{data['ObjName']}-{data['HeightLabel']}-{od_dir_str}"

        # åŸºäºæ ¡å¯¹åçš„çŠ¶æ€åˆ¤æ–­æ˜¯å¦æ­£ç¡®
        is_correct = ((data['HeightLabel'] == 0 and corrected_status == 1) or
                      (data['HeightLabel'] == 1 and corrected_status == 2))

        raw_results.append({
            'packåŒ…åç§°': pack_name,  # æ–°å¢åˆ—
            'éšœç¢ç‰©åç§°': data['ObjName'],
            'ä½ç½®': od_dir_str,
            'çœŸå®æ ‡ç­¾': height_label_str,
            'éšœç¢ç‰©ID': data['zhao_od_ID'],
            'P1åæ ‡': data['P1'],
            'P2åæ ‡': data['P2'],
            'è·ç¦»Dist': data['Dist'],
            'é›·è¾¾è·ç¦»': data['PosDeDis1'],
            'æ£€æµ‹çŠ¶æ€': height_status_str,  # åŸå§‹æ£€æµ‹çŠ¶æ€
            'æ ¡å¯¹æ£€æµ‹çŠ¶æ€': corrected_status_str,  # æ–°å¢ï¼šæ ¡å¯¹åçš„æ£€æµ‹çŠ¶æ€
            'ç½®ä¿¡åº¦': data['HeightProb'],
            'æ¨¡å‹é¢„æµ‹': data['l_TrainResult'],
            'æ¨¡å‹æ¦‚ç‡': data['probability'],
            'æ£€æµ‹é˜¶æ®µ': phase_str,
            'æ˜¯å¦æ­£ç¡®': "âœ“" if is_correct else "âœ—"  # åŸºäºæ ¡å¯¹åçŠ¶æ€åˆ¤æ–­
        })

    return pd.DataFrame(raw_results)


def ensure_directory_exists(file_path):
    """ç¡®ä¿æ–‡ä»¶æ‰€åœ¨ç›®å½•å­˜åœ¨"""
    directory = os.path.dirname(file_path)
    if directory and not os.path.exists(directory):
        try:
            os.makedirs(directory, exist_ok=True)
            print(f"âœ… åˆ›å»ºç›®å½•: {directory}")
        except Exception as e:
            print(f"âŒ åˆ›å»ºç›®å½•å¤±è´¥: {e}")
            return False
    return True


def save_to_excel(detailed_df, summary_df, raw_df, base_path, include_raw=False):
    """ä¿å­˜ä¸ºExcelæ ¼å¼ï¼Œè‡ªåŠ¨è°ƒæ•´åˆ—å®½"""
    excel_path = f"{base_path}_åˆ†æç»“æœ.xlsx"

    if not ensure_directory_exists(excel_path):
        return None

    try:
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            detailed_df.to_excel(writer, sheet_name='è¯¦ç»†åˆ†æ', index=False)
            summary_df.to_excel(writer, sheet_name='ç®€æ´æ±‡æ€»', index=False)

            if include_raw and raw_df is not None:
                raw_df.to_excel(writer, sheet_name='åŸå§‹æ•°æ®åˆ†ç»„', index=False)

            workbook = writer.book
            detailed_sheet = workbook['è¯¦ç»†åˆ†æ']
            adjust_column_width(detailed_sheet, detailed_df)

            summary_sheet = workbook['ç®€æ´æ±‡æ€»']
            adjust_column_width(summary_sheet, summary_df)

            if include_raw and raw_df is not None:
                raw_sheet = workbook['åŸå§‹æ•°æ®åˆ†ç»„']
                adjust_column_width(raw_sheet, raw_df)

            format_excel_sheets(workbook)

        sheet_info = "åŒ…å«åŸå§‹æ•°æ®åˆ†ç»„è¡¨" if include_raw else "ä¸åŒ…å«åŸå§‹æ•°æ®è¡¨"
        print(f"âœ… Excelæ–‡ä»¶å·²ä¿å­˜: {excel_path} ({sheet_info})")
        return excel_path

    except ImportError:
        print("âš ï¸  è­¦å‘Š: æœªå®‰è£… openpyxl åº“ï¼Œæ— æ³•ç”ŸæˆExcelæ–‡ä»¶")
        return None
    except Exception as e:
        print(f"âŒ ä¿å­˜Excelæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None


def adjust_column_width(sheet, df):
    """è‡ªåŠ¨è°ƒæ•´åˆ—å®½"""
    try:
        from openpyxl.utils import get_column_letter

        for col_idx, column in enumerate(df.columns, 1):
            column_letter = get_column_letter(col_idx)
            max_length = len(str(column))

            for value in df[column]:
                if pd.notna(value):
                    str_value = str(value)
                    length = len(str_value) + sum(1 for char in str_value if ord(char) > 127)
                    max_length = max(max_length, length)

            adjusted_width = min(max(max_length + 2, 10), 50)
            sheet.column_dimensions[column_letter].width = adjusted_width

    except ImportError:
        print("âš ï¸  è­¦å‘Š: æ— æ³•è°ƒæ•´åˆ—å®½ï¼Œè¯·ç¡®ä¿å®‰è£…äº† openpyxl åº“")
    except Exception as e:
        print(f"âš ï¸  è°ƒæ•´åˆ—å®½æ—¶å‡ºç°é—®é¢˜: {e}")


def format_excel_sheets(workbook):
    """æ ¼å¼åŒ–Excelå·¥ä½œè¡¨"""
    try:
        from openpyxl.styles import Font, PatternFill, Alignment, Border, Side

        header_font = Font(bold=True, color='FFFFFF')
        header_fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')
        border = Border(
            left=Side(style='thin'),
            right=Side(style='thin'),
            top=Side(style='thin'),
            bottom=Side(style='thin')
        )
        center_alignment = Alignment(horizontal='center', vertical='center')

        for sheet_name in workbook.sheetnames:
            sheet = workbook[sheet_name]

            for cell in sheet[1]:
                cell.font = header_font
                cell.fill = header_fill
                cell.alignment = center_alignment
                cell.border = border

            for row in sheet.iter_rows(min_row=2, max_row=sheet.max_row, max_col=sheet.max_column):
                for cell in row:
                    cell.border = border
                    cell.alignment = Alignment(vertical='center')

            sheet.freeze_panes = 'A2'

    except ImportError:
        print("âš ï¸  è­¦å‘Š: æ— æ³•åº”ç”¨Excelæ ¼å¼åŒ–")
    except Exception as e:
        print(f"âš ï¸  æ ¼å¼åŒ–Excelæ—¶å‡ºç°é—®é¢˜: {e}")


def save_to_csv(detailed_df, summary_df, raw_df, base_path, include_raw=False):
    """ä¿å­˜ä¸ºCSVæ ¼å¼"""
    detailed_csv = f"{base_path}_è¯¦ç»†åˆ†æ.csv"
    summary_csv = f"{base_path}_ç®€æ´æ±‡æ€».csv"
    raw_csv = f"{base_path}_åŸå§‹æ•°æ®åˆ†ç»„.csv"

    if not ensure_directory_exists(detailed_csv) or not ensure_directory_exists(summary_csv):
        return None, None, None

    try:
        detailed_df.to_csv(detailed_csv, index=False, encoding='utf-8-sig')
        summary_df.to_csv(summary_csv, index=False, encoding='utf-8-sig')

        saved_files = [detailed_csv, summary_csv]

        if include_raw and raw_df is not None:
            raw_df.to_csv(raw_csv, index=False, encoding='utf-8-sig')
            saved_files.append(raw_csv)

        print(f"âœ… CSVæ–‡ä»¶å·²ä¿å­˜:")
        print(f"  ğŸ“ è¯¦ç»†åˆ†æ: {detailed_csv}")
        print(f"  ğŸ“Š ç®€æ´æ±‡æ€»: {summary_csv}")
        if include_raw and raw_df is not None:
            print(f"  ğŸ“‹ åŸå§‹æ•°æ®åˆ†ç»„: {raw_csv}")

        return tuple(saved_files)

    except Exception as e:
        print(f"âŒ ä¿å­˜CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None, None, None


def analyze_obstacle_data(log_file_path, output_path, output_format='excel', include_raw_data=False):
    """
    ä¸»è¦åˆ†æå‡½æ•°
    """
    if not os.path.exists(log_file_path):
        print(f"âŒ é”™è¯¯: æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file_path}")
        return None, None, None

    print(f"ğŸ“‚ å¼€å§‹åˆ†ææ—¥å¿—æ–‡ä»¶: {log_file_path}")
    print(f"ğŸ”§ åŸå§‹æ•°æ®åˆ†ç»„è¡¨: {'å¯ç”¨' if include_raw_data else 'ç¦ç”¨'}")
    print(f"ğŸ”§ æ ¡å¯¹æ£€æµ‹çŠ¶æ€è§„åˆ™: ")
    print(f"   - åŸºç¡€è§„åˆ™ï¼šé›·è¾¾è·ç¦»<=500æ—¶ï¼Œæ£€æµ‹çŠ¶æ€è®¾ä¸º'é«˜'")
    print(f"   - é«˜éšœç¢ç‰©ç‰¹æ®Šè§„åˆ™ï¼š2ç±³å†…ç½®ä¿¡åº¦ä¸º80æˆ–è¿ç»­<160æ—¶ï¼Œæ£€æµ‹çŠ¶æ€è®¾ä¸º'é«˜'")
    print(f"ğŸ”§ æ•°æ®è¿‡æ»¤è§„åˆ™: å¿½ç•¥é›·è¾¾è·ç¦»ä¸º0çš„æ•°æ®")

    try:
        with open(log_file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        print(f"ğŸ“ è¯»å–åˆ° {len(lines)} è¡Œæ•°æ®")
    except Exception as e:
        print(f"âŒ è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
        return None, None, None

    all_data = []
    failed_lines = 0

    for line_num, line in enumerate(lines, 1):
        parsed = parse_log_line(line)
        if parsed:
            all_data.append(parsed)
        else:
            failed_lines += 1
            if failed_lines <= 3:
                print(f"âš ï¸  ç¬¬{line_num}è¡Œè§£æå¤±è´¥: {line.strip()[:100]}...")

    print(f"âœ… æˆåŠŸè§£æ {len(all_data)} æ¡æ•°æ®ï¼Œå¤±è´¥ {failed_lines} æ¡")

    if not all_data:
        print("âŒ é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ—¥å¿—æ•°æ®")
        return None, None, None

    obstacle_groups = defaultdict(list)
    for data in all_data:
        key = (data['ObjName'], data['OdDir'], data['HeightLabel'])
        obstacle_groups[key].append(data)

    print(f"ğŸ” è¯†åˆ«åˆ° {len(obstacle_groups)} ä¸ªä¸åŒçš„éšœç¢ç‰©ç»„åˆ")

    detailed_results = []
    summary_results = []
    all_labeled_data = []

    for (obj_name, od_dir, height_label), data_list in obstacle_groups.items():
        od_dir_str = "front" if od_dir == 1 else "rear"
        height_label_str = "ä½éšœç¢ç‰©" if height_label == 0 else "é«˜éšœç¢ç‰©"

        print(f"\nğŸ” åˆ†æéšœç¢ç‰©: {obj_name} ({od_dir_str}, {height_label_str})")

        approach_sessions, labeled_data = identify_sessions_by_distance_only(data_list, height_label)
        all_labeled_data.extend(labeled_data)

        print(f"  è¯†åˆ«å‡º {len(approach_sessions)} æ¬¡æ¥è¿‘è¿‡ç¨‹")

        id_groups = defaultdict(list)
        for session_idx, session in enumerate(approach_sessions, 1):
            for data in session:
                id_groups[data['zhao_od_ID']].append((data, session_idx))

        print(f"  æ¶‰åŠ {len(id_groups)} ä¸ªä¸åŒçš„éšœç¢ç‰©ID: {list(id_groups.keys())}")

        obstacle_has_any_error = False
        total_sessions_all_ids = len(approach_sessions)
        total_points_all_ids = sum(len(session) for session in approach_sessions)

        # ç”¨äºè®°å½•æ‰€æœ‰æœ€è¿‘é”™è¯¯è·ç¦»ï¼Œä»¥è®¡ç®—æœ€å°é”™è¯¯è·ç¦»
        all_latest_error_distances = []
        # ç”¨äºè®°å½•æ‰€æœ‰é¦–æ¬¡é”™è¯¯è·ç¦»ï¼Œä»¥è®¡ç®—æœ€å¤§é”™è¯¯è·ç¦»
        all_first_error_distances = []

        for zhao_od_id, id_data_with_session in id_groups.items():
            print(f"  åˆ†æID {zhao_od_id}: {len(id_data_with_session)} ä¸ªæ•°æ®ç‚¹")

            session_groups = defaultdict(list)
            for data, session_idx in id_data_with_session:
                session_groups[session_idx].append(data)

            all_sessions_correct = True
            incorrect_sessions = []

            for session_idx, session_data in session_groups.items():
                # åŸºäºæ ¡å¯¹åçš„çŠ¶æ€åˆ¤æ–­æ˜¯å¦æ­£ç¡®
                session_correct = all(
                    (height_label == 0 and data.get('CorrectedHeightStatus',
                                                    get_corrected_height_status(data['HeightStatus'],
                                                                                data['PosDeDis1'])) == 1) or
                    (height_label == 1 and data.get('CorrectedHeightStatus',
                                                    get_corrected_height_status(data['HeightStatus'],
                                                                                data['PosDeDis1'])) == 2)
                    for data in session_data
                )

                if not session_correct:
                    all_sessions_correct = False
                    incorrect_sessions.append(session_idx)
                    obstacle_has_any_error = True

            if all_sessions_correct:
                total_points = sum(len(session_data) for session_data in session_groups.values())
                all_distances = [data['Dist'] for session_data in session_groups.values() for data in session_data]
                min_dist = min(all_distances)
                max_dist = max(all_distances)

                detailed_results.append({
                    'éšœç¢ç‰©åç§°': obj_name,
                    'ä½ç½®': od_dir_str,
                    'çœŸå®æ ‡ç­¾': height_label_str,
                    'éšœç¢ç‰©ID': zhao_od_id,
                    'æ€»æµ‹è¯•æ¬¡æ•°': len(session_groups),
                    'æµ‹è¯•æ¬¡æ•°è¯´æ˜': f"ç¬¬{','.join(map(str, sorted(session_groups.keys())))}æ¬¡" if len(
                        session_groups) > 1 else "ç¬¬1æ¬¡",
                    'æ•°æ®ç‚¹æ€»æ•°': total_points,
                    'è·ç¦»èŒƒå›´(mm)': f"{max_dist}~{min_dist}",
                    'æ£€æµ‹çŠ¶æ€': "å…¨ç¨‹æ­£ç¡®",
                    'é¦–æ¬¡é”™è¯¯è·ç¦»(mm)': "-1",  # ä¿®æ”¹ï¼šæ— é”™è¯¯æ—¶æ˜¾ç¤º-1
                    'æœ€è¿‘é”™è¯¯è·ç¦»(mm)': "-1",  # æ–°å¢åˆ—ï¼šæ— é”™è¯¯æ—¶ä¸º-1
                    'é”™è¯¯è¯¦æƒ…': "æ— ",
                    'å¤‡æ³¨': f"å…±{len(session_groups)}æ¬¡æµ‹è¯•ï¼Œå…¨ç¨‹æ ¡å¯¹åHeightStatusä¸HeightLabelä¸€è‡´"
                })

            else:
                # å¯¹äºé”™è¯¯çš„æ¬¡æ•°ï¼Œåˆ†åˆ«è¾“å‡º
                for session_idx in incorrect_sessions:
                    session_data = session_groups[session_idx]
                    # ä¸ºsession_dataä¸­æ¯ä¸ªæ•°æ®æ·»åŠ æ ¡å¯¹åçš„çŠ¶æ€ï¼ˆå¦‚æœè¿˜æ²¡æœ‰çš„è¯ï¼‰
                    if 'CorrectedHeightStatus' not in session_data[0]:
                        if height_label == 1:  # é«˜éšœç¢ç‰©
                            corrected_status_map = get_corrected_height_status_for_high_obstacles(session_data)
                            for idx, data in enumerate(session_data):
                                data['CorrectedHeightStatus'] = corrected_status_map[idx]
                        else:  # ä½éšœç¢ç‰©
                            for data in session_data:
                                data['CorrectedHeightStatus'] = get_corrected_height_status(data['HeightStatus'],
                                                                                            data['PosDeDis1'])

                    changes = analyze_height_status_changes(session_data)
                    error_desc = format_distance_ranges(changes, height_label)
                    status_analysis = get_status_analysis(height_label, True, changes)

                    # è·å–é¦–æ¬¡é”™è¯¯è·ç¦»
                    first_error_dist = get_first_error_distance(changes, height_label)

                    # è·å–æœ€è¿‘é”™è¯¯è·ç¦»
                    latest_error_dist = get_latest_error_distance(changes, height_label)

                    # è®°å½•é”™è¯¯è·ç¦»ç”¨äºæ±‡æ€»è®¡ç®—
                    if first_error_dist > 0:
                        all_first_error_distances.append(first_error_dist)
                    if latest_error_dist > 0:
                        all_latest_error_distances.append(latest_error_dist)

                    detailed_results.append({
                        'éšœç¢ç‰©åç§°': obj_name,
                        'ä½ç½®': od_dir_str,
                        'çœŸå®æ ‡ç­¾': height_label_str,
                        'éšœç¢ç‰©ID': zhao_od_id,
                        'æ€»æµ‹è¯•æ¬¡æ•°': len(session_groups),
                        'æµ‹è¯•æ¬¡æ•°è¯´æ˜': f"ç¬¬{session_idx}æ¬¡",
                        'æ•°æ®ç‚¹æ€»æ•°': len(session_data),
                        'è·ç¦»èŒƒå›´(mm)': f"{max(d['Dist'] for d in session_data)}~{min(d['Dist'] for d in session_data)}",
                        'æ£€æµ‹çŠ¶æ€': status_analysis,
                        'é¦–æ¬¡é”™è¯¯è·ç¦»(mm)': str(first_error_dist) if first_error_dist > 0 else "-1",  # ä¿®æ”¹ï¼šæ— é”™è¯¯æ—¶æ˜¾ç¤º-1
                        'æœ€è¿‘é”™è¯¯è·ç¦»(mm)': str(latest_error_dist) if latest_error_dist > 0 else "-1",  # æ–°å¢åˆ—
                        'é”™è¯¯è¯¦æƒ…': error_desc,
                        'å¤‡æ³¨': f"ç¬¬{session_idx}æ¬¡æµ‹è¯•ä¸­æ ¡å¯¹åHeightStatusä¸HeightLabelä¸ä¸€è‡´"
                    })

                # å¯¹äºæ­£ç¡®çš„æ¬¡æ•°ï¼Œåˆå¹¶è¾“å‡ºä¸€æ¡è®°å½•
                correct_sessions = [i for i in session_groups.keys() if i not in incorrect_sessions]
                if correct_sessions:
                    correct_data = [session_groups[i] for i in correct_sessions]
                    total_points = sum(len(session_data) for session_data in correct_data)
                    all_distances = [data['Dist'] for session_data in correct_data for data in session_data]
                    min_dist = min(all_distances)
                    max_dist = max(all_distances)

                    detailed_results.append({
                        'éšœç¢ç‰©åç§°': obj_name,
                        'ä½ç½®': od_dir_str,
                        'çœŸå®æ ‡ç­¾': height_label_str,
                        'éšœç¢ç‰©ID': zhao_od_id,
                        'æ€»æµ‹è¯•æ¬¡æ•°': len(session_groups),
                        'æµ‹è¯•æ¬¡æ•°è¯´æ˜': f"ç¬¬{','.join(map(str, correct_sessions))}æ¬¡",
                        'æ•°æ®ç‚¹æ€»æ•°': total_points,
                        'è·ç¦»èŒƒå›´(mm)': f"{max_dist}~{min_dist}",
                        'æ£€æµ‹çŠ¶æ€': "å…¨ç¨‹æ­£ç¡®",
                        'é¦–æ¬¡é”™è¯¯è·ç¦»(mm)': "-1",  # ä¿®æ”¹ï¼šæ— é”™è¯¯æ—¶æ˜¾ç¤º-1
                        'æœ€è¿‘é”™è¯¯è·ç¦»(mm)': "-1",  # æ–°å¢åˆ—ï¼šæ— é”™è¯¯æ—¶ä¸º-1
                        'é”™è¯¯è¯¦æƒ…': "æ— ",
                        'å¤‡æ³¨': f"ç¬¬{','.join(map(str, correct_sessions))}æ¬¡æµ‹è¯•å…¨ç¨‹æ­£ç¡®"
                    })

        # ç”Ÿæˆç®€æ´ç‰ˆç»“æœ
        if not obstacle_has_any_error:
            all_distances = [data['Dist'] for data in data_list]
            # ç”ŸæˆpackåŒ…åç§°
            od_dir_pack_str = "front" if od_dir == 1 else "rear"
            pack_name = f"HY11-{obj_name}-{height_label}-{od_dir_pack_str}"

            # æ ¹æ®çœŸå®æ ‡ç­¾ç”Ÿæˆå¤‡æ³¨
            if height_label == 0:  # ä½éšœç¢ç‰©
                remark = f"é‡å¤{total_sessions_all_ids}æ¬¡ï¼ŒæœªæŠ¥é«˜"
            else:  # é«˜éšœç¢ç‰©
                remark = f"é‡å¤{total_sessions_all_ids}æ¬¡ï¼ŒæœªæŠ¥ä½"

            summary_results.append({
                'packåŒ…åç§°': pack_name,  # æ–°å¢åˆ—
                'éšœç¢ç‰©åç§°': obj_name,
                'ä½ç½®': od_dir_str,
                'çœŸå®æ ‡ç­¾': height_label_str,
                'æ€»æµ‹è¯•æ¬¡æ•°': total_sessions_all_ids,
                'æ¶‰åŠIDæ•°é‡': len(id_groups),
                'æ•°æ®ç‚¹æ€»æ•°': total_points_all_ids,
                'è·ç¦»èŒƒå›´(mm)': f"{max(all_distances)}~{min(all_distances)}",
                'æ£€æµ‹ç»“æœ': "âœ“ å…¨ç¨‹æ­£ç¡®",
                'æœ€å¤§é”™è¯¯è·ç¦»(mm)': "-1",  # æ–°å¢åˆ—ï¼šæ— é”™è¯¯æ—¶ä¸º-1
                'æœ€å°é”™è¯¯è·ç¦»(mm)': "-1",  # æ–°å¢åˆ—ï¼šæ— é”™è¯¯æ—¶ä¸º-1
                'å¤‡æ³¨': remark
            })
        else:
            error_type = "å­˜åœ¨æŠ¥é«˜" if height_label == 0 else "å­˜åœ¨æŠ¥ä½"
            all_distances = [data['Dist'] for data in data_list]
            # ç”ŸæˆpackåŒ…åç§°
            od_dir_pack_str = "front" if od_dir == 1 else "rear"
            pack_name = f"HY11-{obj_name}-{height_label}-{od_dir_pack_str}"

            # è®¡ç®—æœ€å¤§å’Œæœ€å°é”™è¯¯è·ç¦»
            max_error_dist = max(all_first_error_distances) if all_first_error_distances else -1
            min_error_dist = min(all_latest_error_distances) if all_latest_error_distances else -1

            # æ ¹æ®çœŸå®æ ‡ç­¾ç”Ÿæˆå¤‡æ³¨
            if height_label == 0:  # ä½éšœç¢ç‰©
                if max_error_dist > 0 and min_error_dist > 0:
                    remark = f"é‡å¤{total_sessions_all_ids}æ¬¡ï¼Œå­˜åœ¨æŠ¥é«˜ï¼Œæœ€è¿œ{max_error_dist}ï¼Œæœ€è¿‘{min_error_dist}"
                else:
                    remark = f"é‡å¤{total_sessions_all_ids}æ¬¡ï¼Œå­˜åœ¨æŠ¥é«˜"
            else:  # é«˜éšœç¢ç‰©
                if max_error_dist > 0 and min_error_dist > 0:
                    remark = f"é‡å¤{total_sessions_all_ids}æ¬¡ï¼Œå­˜åœ¨æŠ¥ä½ï¼Œæœ€è¿œ{max_error_dist}ï¼Œæœ€è¿‘{min_error_dist}"
                else:
                    remark = f"é‡å¤{total_sessions_all_ids}æ¬¡ï¼Œå­˜åœ¨æŠ¥ä½"

            summary_results.append({
                'packåŒ…åç§°': pack_name,  # æ–°å¢åˆ—
                'éšœç¢ç‰©åç§°': obj_name,
                'ä½ç½®': od_dir_str,
                'çœŸå®æ ‡ç­¾': height_label_str,
                'æ€»æµ‹è¯•æ¬¡æ•°': total_sessions_all_ids,
                'æ¶‰åŠIDæ•°é‡': len(id_groups),
                'æ•°æ®ç‚¹æ€»æ•°': total_points_all_ids,
                'è·ç¦»èŒƒå›´(mm)': f"{max(all_distances)}~{min(all_distances)}",
                'æ£€æµ‹ç»“æœ': f"âœ— {error_type}",
                'æœ€å¤§é”™è¯¯è·ç¦»(mm)': str(max_error_dist) if max_error_dist > 0 else "-1",  # æ–°å¢åˆ—
                'æœ€å°é”™è¯¯è·ç¦»(mm)': str(min_error_dist) if min_error_dist > 0 else "-1",  # æ–°å¢åˆ—
                'å¤‡æ³¨': remark
            })

    if not detailed_results:
        print("âŒ é”™è¯¯: æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„åˆ†æç»“æœ")
        return None, None, None

    detailed_df = pd.DataFrame(detailed_results)
    summary_df = pd.DataFrame(summary_results)

    raw_df = None
    if include_raw_data:
        raw_df = create_raw_data_sheet(all_labeled_data, obstacle_groups)
        print(f"ğŸ“‹ ç”ŸæˆåŸå§‹æ•°æ®åˆ†ç»„è¡¨: {len(raw_df)}æ¡è®°å½•")

    print(f"\nğŸ“Š ç”Ÿæˆåˆ†æç»“æœ: è¯¦ç»†è®°å½•{len(detailed_results)}æ¡ï¼Œæ±‡æ€»è®°å½•{len(summary_results)}æ¡")

    if output_path.endswith('.csv') or output_path.endswith('.xlsx'):
        base_path = os.path.splitext(output_path)[0]
    else:
        base_path = output_path

    saved_files = []

    if output_format.lower() in ['excel', 'both']:
        excel_file = save_to_excel(detailed_df, summary_df, raw_df, base_path, include_raw_data)
        if excel_file:
            saved_files.append(excel_file)

    if output_format.lower() in ['csv', 'both']:
        csv_files = save_to_csv(detailed_df, summary_df, raw_df, base_path, include_raw_data)
        if csv_files and csv_files[0]:
            saved_files.extend([f for f in csv_files if f])

    print("=" * 80)
    print("ğŸ¯ éšœç¢ç‰©æ£€æµ‹æ—¥å¿—åˆ†æå®Œæˆ!")
    print("=" * 80)

    if saved_files:
        print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
        for file in saved_files:
            print(f"  â€¢ {file}")
    else:
        print("âŒ è­¦å‘Š: æ²¡æœ‰æˆåŠŸä¿å­˜ä»»ä½•æ–‡ä»¶")

    print()
    print("ğŸ“ˆ ç»Ÿè®¡æ¦‚è§ˆ:")
    print("-" * 50)
    print(f"ğŸ” åˆ†æéšœç¢ç‰©ç±»å‹æ€»æ•°: {len(obstacle_groups)}")
    print(f"ğŸ“ è¯¦ç»†è®°å½•æ¡æ•°: {len(detailed_results)}")
    print(f"ğŸ“‹ æ±‡æ€»è®°å½•æ¡æ•°: {len(summary_results)}")
    print(f"âœ… å…¨ç¨‹æ­£ç¡®çš„éšœç¢ç‰©: {len(summary_df[summary_df['æ£€æµ‹ç»“æœ'].str.contains('å…¨ç¨‹æ­£ç¡®')])}")
    print(f"âŒ å­˜åœ¨é”™è¯¯çš„éšœç¢ç‰©: {len(summary_df[summary_df['æ£€æµ‹ç»“æœ'].str.contains('å­˜åœ¨')])}")
    if include_raw_data and raw_df is not None:
        print(f"ğŸ“‹ åŸå§‹æ•°æ®è®°å½•æ¡æ•°: {len(raw_df)}")

    print("\nğŸ“‹ ç®€æ´æ±‡æ€»è¡¨é¢„è§ˆ:")
    print("-" * 50)
    if len(summary_df) > 0:
        pd.set_option('display.max_columns', None)
        pd.set_option('display.width', None)
        pd.set_option('display.max_colwidth', 30)
        print(summary_df.head(10).to_string(index=False))
        if len(summary_df) > 10:
            print(f"\n... è¿˜æœ‰ {len(summary_df) - 10} è¡Œæ•°æ®ï¼Œè¯¦è§è¾“å‡ºæ–‡ä»¶")
    else:
        print("æ— æ•°æ®")

    print("\n" + "=" * 80)

    print("ğŸ†• æ•°æ®è¿‡æ»¤è¯´æ˜:")
    print("-" * 50)
    print("ğŸ”§ æ ¸å¿ƒæ”¹è¿›:")
    print("  â€¢ è‡ªåŠ¨è¿‡æ»¤é›·è¾¾è·ç¦»ä¸º0çš„æ•°æ®ï¼Œè¿™äº›æ•°æ®ä¸å‚ä¸é”™è¯¯è¯†åˆ«")
    print("  â€¢ åœ¨åŸå§‹æ•°æ®åˆ†ç»„è¡¨ä¸­ä¹Ÿä¼šè‡ªåŠ¨æ’é™¤é›·è¾¾è·ç¦»ä¸º0çš„è®°å½•")
    print("  â€¢ ç¡®ä¿åˆ†æç»“æœçš„å‡†ç¡®æ€§å’Œå¯é æ€§")

    print("\nğŸ†• é«˜éšœç¢ç‰©ç‰¹æ®Šæ ¡å¯¹è§„åˆ™:")
    print("-" * 50)
    print("ğŸ”§ ç½®ä¿¡åº¦æ ¡å¯¹å¢å¼º:")
    print("  â€¢ å¯¹äºçœŸå®æ ‡ç­¾ä¸ºé«˜çš„éšœç¢ç‰©ï¼Œå¢åŠ ç½®ä¿¡åº¦æ ¡å¯¹è§„åˆ™")
    print("  â€¢ åœ¨2ç±³ï¼ˆ2000mmï¼‰ä»¥å†…:")
    print("    - å‡ºç°ç½®ä¿¡åº¦ä¸º80çš„æ•°æ®ï¼Œæ ¡å¯¹æ£€æµ‹çŠ¶æ€è®¾ä¸º'é«˜'")
    print("    - è¿ç»­å‡ºç°ç½®ä¿¡åº¦<160çš„æ•°æ®ï¼Œæ ¡å¯¹æ£€æµ‹çŠ¶æ€å…¨éƒ¨è®¾ä¸º'é«˜'")
    print("  â€¢ æ­¤è§„åˆ™ä»…é€‚ç”¨äºé«˜éšœç¢ç‰©ï¼Œä½éšœç¢ç‰©ä¿æŒåŸæœ‰æ ¡å¯¹è§„åˆ™")

    print("\nğŸ†• æ˜¾ç¤ºæ ¼å¼ä¼˜åŒ–:")
    print("-" * 50)
    print("ğŸ”§ è¯¦ç»†åˆ†æè¡¨æ”¹è¿›:")
    print("  â€¢ 'é¦–æ¬¡é”™è¯¯è·ç¦»(mm)'ï¼šæ— é”™è¯¯æ—¶æ˜¾ç¤º'-1'è€Œé'æ— '")
    print("  â€¢ ç»Ÿä¸€æ•°å€¼æ ¼å¼ï¼Œä¾¿äºæ•°æ®å¤„ç†å’Œæ’åº")

    print("\nğŸ†• ç®€æ´æ±‡æ€»è¡¨æ–°å¢åˆ—:")
    print("-" * 50)
    print("ğŸ”§ è·ç¦»ç»Ÿè®¡å¢å¼º:")
    print("  â€¢ 'æœ€å¤§é”™è¯¯è·ç¦»(mm)'ï¼šè¯¥éšœç¢ç‰©æ‰€æœ‰æµ‹è¯•ä¸­é¦–æ¬¡é”™è¯¯è·ç¦»çš„æœ€å¤§å€¼")
    print("    - è®¡ç®—æ¥æºï¼šè¯¦ç»†åˆ†æè¡¨ä¸­æ‰€æœ‰'é¦–æ¬¡é”™è¯¯è·ç¦»(mm)' > 0çš„æœ€å¤§å€¼")
    print("    - åæ˜ æœ€è¿œçš„é”™è¯¯å‘ç”Ÿä½ç½®")
    print("  â€¢ 'æœ€å°é”™è¯¯è·ç¦»(mm)'ï¼šè¯¥éšœç¢ç‰©æ‰€æœ‰æµ‹è¯•ä¸­æœ€è¿‘é”™è¯¯è·ç¦»çš„æœ€å°å€¼")
    print("    - è®¡ç®—æ¥æºï¼šè¯¦ç»†åˆ†æè¡¨ä¸­æ‰€æœ‰'æœ€è¿‘é”™è¯¯è·ç¦»(mm)' > 0çš„æœ€å°å€¼")
    print("    - åæ˜ æœ€è¿‘çš„é”™è¯¯å‘ç”Ÿä½ç½®")

    print("\nğŸ†• å¤‡æ³¨æ ¼å¼ä¼˜åŒ–:")
    print("-" * 50)
    print("ğŸ”§ å¤‡æ³¨å†…å®¹æ ‡å‡†åŒ–:")
    print("  â€¢ ä½éšœç¢ç‰©æ— é”™è¯¯ï¼š'é‡å¤Xæ¬¡ï¼ŒæœªæŠ¥é«˜'")
    print("  â€¢ ä½éšœç¢ç‰©æœ‰é”™è¯¯ï¼š'é‡å¤Xæ¬¡ï¼Œå­˜åœ¨æŠ¥é«˜ï¼Œæœ€è¿œYï¼Œæœ€è¿‘Z'")
    print("  â€¢ é«˜éšœç¢ç‰©æ— é”™è¯¯ï¼š'é‡å¤Xæ¬¡ï¼ŒæœªæŠ¥ä½'")
    print("  â€¢ é«˜éšœç¢ç‰©æœ‰é”™è¯¯ï¼š'é‡å¤Xæ¬¡ï¼Œå­˜åœ¨æŠ¥ä½ï¼Œæœ€è¿œYï¼Œæœ€è¿‘Z'")
    print("  â€¢ X = æ€»æµ‹è¯•æ¬¡æ•°ï¼ŒY = æœ€å¤§é”™è¯¯è·ç¦»ï¼ŒZ = æœ€å°é”™è¯¯è·ç¦»")

    print("\nğŸ’¡ ç®—æ³•ä¿æŒä¸å˜:")
    print("-" * 50)
    print("ğŸ”§ æ ¸å¿ƒé€»è¾‘:")
    print("  â€¢ è·ç¦»è¿‡æ»¤ï¼šè¿‡æ»¤é›·è¾¾è·ç¦»ä¸º0çš„æ•°æ®")
    print("  â€¢ çŠ¶æ€æ ¡å¯¹ï¼š")
    print("    - åŸºç¡€è§„åˆ™ï¼šé›·è¾¾è·ç¦»<=500æ—¶æ£€æµ‹çŠ¶æ€è®¾ä¸º'é«˜'")
    print("    - é«˜éšœç¢ç‰©ç‰¹æ®Šè§„åˆ™ï¼š2ç±³å†…ç‰¹å®šç½®ä¿¡åº¦æ¡ä»¶ä¸‹è®¾ä¸º'é«˜'")
    print("  â€¢ ä¼šè¯è¯†åˆ«ï¼šåŸºäºçº¿æ€§å›å½’çš„è¶‹åŠ¿åˆ†æ")
    print("  â€¢ é”™è¯¯ç»Ÿè®¡ï¼šåŸºäºæ ¡å¯¹åçŠ¶æ€çš„å‡†ç¡®åˆ¤æ–­")

    return detailed_df, summary_df, raw_df


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    log_file_path = r"D:\PythonProject\data\log_files\1_40.log"
    output_path = r"D:\PythonProject\data\csv_files\6\1_40"

    try:
        print("ğŸš€ å¼€å§‹æ ¡å¯¹æ£€æµ‹çŠ¶æ€çš„æ—¥å¿—åˆ†æ...")
        print(f"ğŸ“‚ æ—¥å¿—æ–‡ä»¶: {log_file_path}")
        print(f"ğŸ“ è¾“å‡ºè·¯å¾„: {output_path}")
        print("-" * 80)

        include_raw_data = True

        detailed_df, summary_df, raw_df = analyze_obstacle_data(
            log_file_path,
            output_path,
            output_format='excel',
            include_raw_data=include_raw_data
        )

        if detailed_df is not None and summary_df is not None:
            print("\nğŸ” è¯¦ç»†åˆ†æè¡¨å­—æ®µè¯´æ˜:")
            print("-" * 50)
            field_descriptions = {
                'éšœç¢ç‰©åç§°': 'éšœç¢ç‰©ç±»å‹åç§°',
                'ä½ç½®': 'front(è½¦å‰) / rear(è½¦å)',
                'çœŸå®æ ‡ç­¾': 'å®é™…çš„é«˜ä½æ ‡ç­¾',
                'éšœç¢ç‰©ID': 'zhao_od_IDç¼–å·',
                'æ€»æµ‹è¯•æ¬¡æ•°': 'è¯¥IDåœ¨æ‰€æœ‰æ¥è¿‘è¿‡ç¨‹ä¸­çš„æµ‹è¯•æ¬¡æ•°',
                'æµ‹è¯•æ¬¡æ•°è¯´æ˜': 'å½“å‰è®°å½•å¯¹åº”çš„æµ‹è¯•æ¬¡æ•°',
                'æ•°æ®ç‚¹æ€»æ•°': 'æ•°æ®è®°å½•æ¡æ•°',
                'è·ç¦»èŒƒå›´(mm)': 'æµ‹è¯•è·ç¦»èŒƒå›´(æœ€è¿œ~æœ€è¿‘)',
                'æ£€æµ‹çŠ¶æ€': 'å…¨ç¨‹æ­£ç¡®/å­˜åœ¨æŠ¥é«˜/å­˜åœ¨æŠ¥ä½(åŸºäºæ ¡å¯¹åçŠ¶æ€)',
                'é¦–æ¬¡é”™è¯¯è·ç¦»(mm)': 'ç¬¬ä¸€æ¬¡å‡ºç°é”™è¯¯æ—¶çš„è·ç¦»(åŸºäºæ ¡å¯¹åçŠ¶æ€ï¼Œ-1è¡¨ç¤ºæ— é”™è¯¯)',
                'æœ€è¿‘é”™è¯¯è·ç¦»(mm)': 'æœ€åä¸€æ¬¡å‡ºç°é”™è¯¯æ—¶çš„è·ç¦»(åŸºäºæ ¡å¯¹åçŠ¶æ€ï¼Œ-1è¡¨ç¤ºæ— é”™è¯¯)',
                'é”™è¯¯è¯¦æƒ…': 'é”™è¯¯çš„å…·ä½“è·ç¦»åŒºé—´ä¿¡æ¯(åŸºäºæ ¡å¯¹åçŠ¶æ€)',
                'å¤‡æ³¨': 'é¢å¤–è¯´æ˜ä¿¡æ¯'
            }

            for field, desc in field_descriptions.items():
                print(f"  â€¢ {field}: {desc}")

            print("\nğŸ“Š ç®€æ´æ±‡æ€»è¡¨å­—æ®µè¯´æ˜:")
            print("-" * 50)
            summary_field_descriptions = {
                'packåŒ…åç§°': 'HY11-éšœç¢ç‰©åç§°-æ ‡ç­¾-ä½ç½®æ ¼å¼',
                'éšœç¢ç‰©åç§°': 'éšœç¢ç‰©ç±»å‹åç§°',
                'ä½ç½®': 'front(è½¦å‰) / rear(è½¦å)',
                'çœŸå®æ ‡ç­¾': 'å®é™…çš„é«˜ä½æ ‡ç­¾',
                'æ€»æµ‹è¯•æ¬¡æ•°': 'è¯¥éšœç¢ç‰©çš„æ€»æµ‹è¯•æ¬¡æ•°',
                'æ¶‰åŠIDæ•°é‡': 'æ¶‰åŠçš„ä¸åŒéšœç¢ç‰©IDæ•°é‡',
                'æ•°æ®ç‚¹æ€»æ•°': 'æ€»æ•°æ®è®°å½•æ¡æ•°',
                'è·ç¦»èŒƒå›´(mm)': 'æµ‹è¯•è·ç¦»èŒƒå›´(æœ€è¿œ~æœ€è¿‘)',
                'æ£€æµ‹ç»“æœ': 'âœ“å…¨ç¨‹æ­£ç¡® / âœ—å­˜åœ¨æŠ¥é«˜ / âœ—å­˜åœ¨æŠ¥ä½',
                'æœ€å¤§é”™è¯¯è·ç¦»(mm)': 'æ‰€æœ‰æµ‹è¯•ä¸­é¦–æ¬¡é”™è¯¯è·ç¦»çš„æœ€å¤§å€¼(-1è¡¨ç¤ºæ— é”™è¯¯)',
                'æœ€å°é”™è¯¯è·ç¦»(mm)': 'æ‰€æœ‰æµ‹è¯•ä¸­æœ€è¿‘é”™è¯¯è·ç¦»çš„æœ€å°å€¼(-1è¡¨ç¤ºæ— é”™è¯¯)',
                'å¤‡æ³¨': 'æ ‡å‡†åŒ–æ ¼å¼çš„æµ‹è¯•ç»“æœæè¿°'
            }

            for field, desc in summary_field_descriptions.items():
                print(f"  â€¢ {field}: {desc}")

            if include_raw_data and raw_df is not None:
                print("\nğŸ“‹ åŸå§‹æ•°æ®åˆ†ç»„è¡¨å­—æ®µè¯´æ˜:")
                print("-" * 50)
                raw_field_descriptions = {
                    'packåŒ…åç§°': 'HY11-éšœç¢ç‰©åç§°-æ ‡ç­¾-ä½ç½®æ ¼å¼',
                    'éšœç¢ç‰©åç§°': 'éšœç¢ç‰©ç±»å‹åç§°',
                    'ä½ç½®': 'front(è½¦å‰) / rear(è½¦å)',
                    'çœŸå®æ ‡ç­¾': 'å®é™…çš„é«˜ä½æ ‡ç­¾',
                    'éšœç¢ç‰©ID': 'zhao_od_IDç¼–å·',
                    'P1åæ ‡': 'éšœç¢ç‰©åæ ‡ç‚¹1',
                    'P2åæ ‡': 'éšœç¢ç‰©åæ ‡ç‚¹2',
                    'è·ç¦»Dist': 'è½¦è¾†åˆ°éšœç¢ç‰©çš„è·ç¦»',
                    'é›·è¾¾è·ç¦»': 'é›·è¾¾ç›´æ¥å›æ³¢è·ç¦»(å·²è¿‡æ»¤0å€¼)',
                    'æ£€æµ‹çŠ¶æ€': 'ç³»ç»ŸæŠ¥å‘Šçš„åŸå§‹é«˜ä½çŠ¶æ€',
                    'æ ¡å¯¹æ£€æµ‹çŠ¶æ€': 'åŸºäºé›·è¾¾è·ç¦»å’Œç½®ä¿¡åº¦æ ¡å¯¹åçš„é«˜ä½çŠ¶æ€',
                    'ç½®ä¿¡åº¦': 'ç³»ç»Ÿç½®ä¿¡åº¦',
                    'æ¨¡å‹é¢„æµ‹': 'æ¨¡å‹é¢„æµ‹ç»“æœ',
                    'æ¨¡å‹æ¦‚ç‡': 'æ¨¡å‹é¢„æµ‹æ¦‚ç‡å€¼',
                    'æ£€æµ‹é˜¶æ®µ': 'ç¬¬å‡ æ¬¡æ¥è¿‘æˆ–è¿œç¦»è¿‡ç¨‹',
                    'æ˜¯å¦æ­£ç¡®': 'æ ¡å¯¹åæ£€æµ‹ç»“æœæ˜¯å¦ä¸çœŸå®æ ‡ç­¾ä¸€è‡´'
                }

                for field, desc in raw_field_descriptions.items():
                    print(f"  â€¢ {field}: {desc}")

        else:
            print("\nâŒ åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥:")
            print("  1. æ—¥å¿—æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®")
            print("  2. è¾“å‡ºç›®å½•æ˜¯å¦æœ‰å†™å…¥æƒé™")
            print("  3. æ˜¯å¦å®‰è£…äº†å¿…è¦çš„ä¾èµ–åº“")

    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ—¥å¿—æ–‡ä»¶ '{log_file_path}'")
        print("è¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œæˆ–å°†æ—¥å¿—å†…å®¹ä¿å­˜ä¸ºå¯¹åº”çš„æ—¥å¿—æ–‡ä»¶")
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()