import re
import pandas as pd
from collections import defaultdict, OrderedDict
import numpy as np
import os
from pathlib import Path


def parse_log_line(line):
    """è§£æå•è¡Œæ—¥å¿—æ•°æ®"""
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å„ä¸ªå­—æ®µ
    pattern = r'zhao_od_ID:(\d+), ObjName:([^,]+), HeightLabel:(\d+), OdDir:(\d+), P1\(([^)]+)\), P2\(([^)]+)\), HeightProb:(\d+), HeightStatus:(\d+), Dist:(\d+), PosDeDis1:(\d+), l_TrainResult:(\d+), probability:(\d+)'
    match = re.match(pattern, line.strip())

    if match:
        return {
            'zhao_od_ID': int(match.group(1)),
            'ObjName': match.group(2),
            'HeightLabel': int(match.group(3)),
            'OdDir': int(match.group(4)),
            'P1': match.group(5),
            'P2': match.group(6),
            'HeightProb': int(match.group(7)),
            'HeightStatus': int(match.group(8)),
            'Dist': int(match.group(9)),
            'PosDeDis1': int(match.group(10)),
            'l_TrainResult': int(match.group(11)),
            'probability': int(match.group(12))
        }
    return None


def identify_test_sessions(data_list):
    """
    è¯†åˆ«æµ‹è¯•æ¬¡æ•°ï¼Œæ ¹æ®Distè¿ç»­å˜åŒ–è¶‹åŠ¿åˆ¤æ–­
    æ”¹è¿›ç‰ˆæœ¬ï¼šèƒ½å¤Ÿè¯†åˆ«è¿ç»­çš„æ¥è¿‘è¿‡ç¨‹ï¼Œè¿‡æ»¤è¿œç¦»è¿‡ç¨‹å’Œå¹²æ‰°æ•°æ®
    """
    if len(data_list) <= 1:
        return [data_list]

    def analyze_trend_window(distances, start_idx, window_size=5):
        """
        åˆ†ææŒ‡å®šçª—å£å†…çš„è·ç¦»å˜åŒ–è¶‹åŠ¿
        è¿”å›: ('decreasing', score) æˆ– ('increasing', score) æˆ– ('mixed', score)
        score è¡¨ç¤ºè¶‹åŠ¿çš„å¼ºåº¦ï¼ŒèŒƒå›´0-1
        """
        end_idx = min(start_idx + window_size, len(distances))
        if end_idx - start_idx < 3:
            return 'mixed', 0.0

        window_distances = distances[start_idx:end_idx]
        decreasing_count = 0
        increasing_count = 0
        total_changes = len(window_distances) - 1

        for i in range(1, len(window_distances)):
            if window_distances[i] < window_distances[i - 1]:
                decreasing_count += 1
            elif window_distances[i] > window_distances[i - 1]:
                increasing_count += 1

        if total_changes == 0:
            return 'mixed', 0.0

        decreasing_ratio = decreasing_count / total_changes
        increasing_ratio = increasing_count / total_changes

        if decreasing_ratio >= 0.6:  # 60%ä»¥ä¸Šçš„ç‚¹åœ¨ä¸‹é™
            return 'decreasing', decreasing_ratio
        elif increasing_ratio >= 0.6:  # 60%ä»¥ä¸Šçš„ç‚¹åœ¨ä¸Šå‡
            return 'increasing', increasing_ratio
        else:
            return 'mixed', max(decreasing_ratio, increasing_ratio)

    def is_significant_change(current_dist, prev_dist, threshold_ratio=0.1):
        """åˆ¤æ–­è·ç¦»å˜åŒ–æ˜¯å¦æ˜¾è‘—"""
        if prev_dist == 0:
            return True
        change_ratio = abs(current_dist - prev_dist) / prev_dist
        return change_ratio > threshold_ratio

    # é¦–å…ˆæŒ‰åŸå§‹é¡ºåºæ’åºï¼Œä»¥ä¿æŒæ—¶é—´åºåˆ—ç‰¹æ€§
    # ä½†æˆ‘ä»¬éœ€è¦æ ¹æ®Distæ¥åˆ¤æ–­ï¼Œæ‰€ä»¥å…ˆæŒ‰Distç²—ç•¥æ’åº
    data_list.sort(key=lambda x: x['Dist'], reverse=True)

    if len(data_list) < 3:
        return [data_list]

    distances = [d['Dist'] for d in data_list]
    sessions = []
    current_session = []
    i = 0

    print(f"    è°ƒè¯•ï¼šæ€»æ•°æ®ç‚¹ {len(distances)}ï¼Œè·ç¦»èŒƒå›´ {min(distances)}-{max(distances)}")

    while i < len(data_list):
        if not current_session:
            # å¼€å§‹æ–°çš„ä¼šè¯
            current_session = [data_list[i]]
            i += 1
            continue

        # åˆ†æå½“å‰ä½ç½®çš„è¶‹åŠ¿
        current_trend, trend_score = analyze_trend_window(distances, max(0, i - 2), 5)

        prev_dist = distances[i - 1]
        curr_dist = distances[i]

        # åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»§ç»­å½“å‰ä¼šè¯
        if len(current_session) == 1:
            # ç¬¬äºŒä¸ªç‚¹ï¼Œç›´æ¥åŠ å…¥
            current_session.append(data_list[i])
        else:
            # åˆ†æè¶‹åŠ¿å†³å®šæ˜¯å¦ç»§ç»­
            if current_trend == 'decreasing' and trend_score > 0.5:
                # å¼ºçƒˆçš„ä¸‹é™è¶‹åŠ¿ï¼Œç»§ç»­å½“å‰ä¼šè¯
                current_session.append(data_list[i])
            elif current_trend == 'increasing' and trend_score > 0.6:
                # å¼ºçƒˆçš„ä¸Šå‡è¶‹åŠ¿ï¼Œå¯èƒ½æ˜¯è¿œç¦»è¿‡ç¨‹ï¼Œç»“æŸå½“å‰ä¼šè¯
                if len(current_session) >= 3:
                    sessions.append(current_session)
                    print(
                        f"    è°ƒè¯•ï¼šå®Œæˆä¸€æ¬¡é‡‡æ ·ï¼Œæ•°æ®ç‚¹æ•° {len(current_session)}ï¼Œè·ç¦»èŒƒå›´ {max(d['Dist'] for d in current_session)}-{min(d['Dist'] for d in current_session)}")

                # è·³è¿‡ä¸Šå‡é˜¶æ®µï¼Œå¯»æ‰¾ä¸‹ä¸€ä¸ªä¸‹é™å¼€å§‹ç‚¹
                while i < len(data_list) - 1:
                    next_trend, next_score = analyze_trend_window(distances, i, 3)
                    if next_trend == 'decreasing' and next_score > 0.5:
                        break
                    i += 1

                current_session = []
                continue
            else:
                # æ··åˆè¶‹åŠ¿ï¼Œæ ¹æ®è·ç¦»å˜åŒ–åˆ¤æ–­
                if curr_dist < prev_dist or not is_significant_change(curr_dist, prev_dist):
                    # ä»åœ¨ä¸‹é™æˆ–å˜åŒ–ä¸å¤§ï¼Œç»§ç»­
                    current_session.append(data_list[i])
                else:
                    # å¼€å§‹ä¸Šå‡ï¼Œç»“æŸå½“å‰ä¼šè¯
                    if len(current_session) >= 3:
                        sessions.append(current_session)
                        print(
                            f"    è°ƒè¯•ï¼šå®Œæˆä¸€æ¬¡é‡‡æ ·ï¼Œæ•°æ®ç‚¹æ•° {len(current_session)}ï¼Œè·ç¦»èŒƒå›´ {max(d['Dist'] for d in current_session)}-{min(d['Dist'] for d in current_session)}")
                    current_session = []
                    continue

        i += 1

    # å¤„ç†æœ€åä¸€ä¸ªä¼šè¯
    if current_session and len(current_session) >= 3:
        sessions.append(current_session)
        print(
            f"    è°ƒè¯•ï¼šå®Œæˆæœ€åä¸€æ¬¡é‡‡æ ·ï¼Œæ•°æ®ç‚¹æ•° {len(current_session)}ï¼Œè·ç¦»èŒƒå›´ {max(d['Dist'] for d in current_session)}-{min(d['Dist'] for d in current_session)}")

    # ä¸ºæ¯ä¸ªä¼šè¯é‡æ–°æŒ‰è·ç¦»æ’åºï¼ˆä»å¤§åˆ°å°ï¼Œç¡®ä¿æ˜¯æ¥è¿‘è¿‡ç¨‹ï¼‰
    for session in sessions:
        session.sort(key=lambda x: x['Dist'], reverse=True)

    print(f"    è°ƒè¯•ï¼šæ€»å…±è¯†åˆ«å‡º {len(sessions)} æ¬¡æµ‹è¯•")
    return sessions if sessions else [data_list]


def analyze_height_status_changes(session_data):
    """åˆ†æå•æ¬¡æµ‹è¯•ä¸­HeightStatusçš„å˜åŒ–"""
    # æŒ‰è·ç¦»æ’åºï¼ˆä»å¤§åˆ°å°ï¼Œæ¥è¿‘è¿‡ç¨‹ï¼‰
    session_data.sort(key=lambda x: x['Dist'], reverse=True)

    changes = []
    if not session_data:
        return changes

    current_status = session_data[0]['HeightStatus']
    change_start_idx = 0

    for i, data in enumerate(session_data):
        if data['HeightStatus'] != current_status:
            # è®°å½•çŠ¶æ€å˜åŒ–
            changes.append({
                'start_idx': change_start_idx,
                'end_idx': i - 1,
                'status': current_status,
                'start_dist': session_data[change_start_idx]['Dist'],
                'end_dist': session_data[i - 1]['Dist']
            })
            current_status = data['HeightStatus']
            change_start_idx = i

    # æ·»åŠ æœ€åä¸€æ®µ
    if change_start_idx < len(session_data):
        changes.append({
            'start_idx': change_start_idx,
            'end_idx': len(session_data) - 1,
            'status': current_status,
            'start_dist': session_data[change_start_idx]['Dist'],
            'end_dist': session_data[-1]['Dist']
        })

    return changes


def format_distance_ranges(changes, height_label):
    """æ ¼å¼åŒ–è·ç¦»åŒºé—´ä¿¡æ¯"""
    if not changes:
        return "æ— æ•°æ®"

    # æ‰¾å‡ºé”™è¯¯æŠ¥å‘Šçš„åŒºé—´ï¼ˆHeightStatusä¸HeightLabelä¸ä¸€è‡´ï¼‰
    wrong_ranges = []
    correct_ranges = []
    first_wrong_dist = None

    for change in changes:
        status_desc = "æŠ¥é«˜" if change['status'] == 2 else "æŠ¥ä½"
        range_desc = f"Dist:{change['start_dist']}-{change['end_dist']}"

        # åˆ¤æ–­æ˜¯å¦ä¸çœŸå®æ ‡ç­¾ä¸€è‡´
        is_correct = (height_label == 0 and change['status'] == 1) or (height_label == 1 and change['status'] == 2)

        if is_correct:
            correct_ranges.append(f"{range_desc}({status_desc})")
        else:
            wrong_ranges.append(f"{range_desc}({status_desc})")
            if first_wrong_dist is None:
                first_wrong_dist = change['start_dist']

    # æ„å»ºæè¿°
    descriptions = []
    if first_wrong_dist is not None:
        descriptions.append(f"é¦–æ¬¡é”™è¯¯æŠ¥å‘Šè·ç¦»:{first_wrong_dist}")

    if wrong_ranges:
        descriptions.append(f"é”™è¯¯åŒºé—´:{'; '.join(wrong_ranges)}")

    if correct_ranges:
        descriptions.append(f"æ­£ç¡®åŒºé—´:{'; '.join(correct_ranges)}")

    # æ£€æŸ¥æ˜¯å¦åœ¨æŸä¸ªè·ç¦»åå…¨ç¨‹æ­£ç¡®
    if len(changes) > 1:
        last_change = changes[-1]
        last_is_correct = (height_label == 0 and last_change['status'] == 1) or (
                height_label == 1 and last_change['status'] == 2)
        if last_is_correct and last_change['end_dist'] < last_change['start_dist'] * 0.7:
            descriptions.append(f"Dist<{last_change['start_dist']}åå…¨ç¨‹{'æŠ¥ä½' if height_label == 0 else 'æŠ¥é«˜'}")

    return "; ".join(descriptions) if descriptions else "å…¨ç¨‹æ­£ç¡®"


def get_status_analysis(height_label, has_error, changes):
    """è·å–çŠ¶æ€åˆ†ææè¿°"""
    if not has_error:
        return "å…¨ç¨‹æ­£ç¡®"

    # æ£€æŸ¥å…·ä½“çš„é”™è¯¯ç±»å‹
    for change in changes:
        is_error = not ((height_label == 0 and change['status'] == 1) or
                        (height_label == 1 and change['status'] == 2))
        if is_error:
            if height_label == 0 and change['status'] == 2:  # ä½éšœç¢ç‰©ä½†æŠ¥é«˜
                return "å­˜åœ¨æŠ¥é«˜"
            elif height_label == 1 and change['status'] == 1:  # é«˜éšœç¢ç‰©ä½†æŠ¥ä½
                return "å­˜åœ¨æŠ¥ä½"

    return "å­˜åœ¨é”™è¯¯"


def ensure_directory_exists(file_path):
    """ç¡®ä¿æ–‡ä»¶æ‰€åœ¨ç›®å½•å­˜åœ¨"""
    directory = os.path.dirname(file_path)
    if directory and not os.path.exists(directory):
        try:
            os.makedirs(directory, exist_ok=True)
            print(f"âœ… åˆ›å»ºç›®å½•: {directory}")
        except Exception as e:
            print(f"âŒ åˆ›å»ºç›®å½•å¤±è´¥: {e}")
            return False
    return True


def save_to_excel(detailed_df, summary_df, base_path):
    """ä¿å­˜ä¸ºExcelæ ¼å¼ï¼Œè‡ªåŠ¨è°ƒæ•´åˆ—å®½"""
    excel_path = f"{base_path}_åˆ†æç»“æœ.xlsx"

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    if not ensure_directory_exists(excel_path):
        return None

    try:
        # ä½¿ç”¨ openpyxl å¼•æ“å†™å…¥Excel
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            # å†™å…¥è¯¦ç»†åˆ†æè¡¨
            detailed_df.to_excel(writer, sheet_name='è¯¦ç»†åˆ†æ', index=False)

            # å†™å…¥ç®€æ´æ±‡æ€»è¡¨
            summary_df.to_excel(writer, sheet_name='ç®€æ´æ±‡æ€»', index=False)

            # è·å–å·¥ä½œç°¿å’Œå·¥ä½œè¡¨
            workbook = writer.book

            # è‡ªåŠ¨è°ƒæ•´è¯¦ç»†åˆ†æè¡¨çš„åˆ—å®½
            detailed_sheet = workbook['è¯¦ç»†åˆ†æ']
            adjust_column_width(detailed_sheet, detailed_df)

            # è‡ªåŠ¨è°ƒæ•´ç®€æ´æ±‡æ€»è¡¨çš„åˆ—å®½
            summary_sheet = workbook['ç®€æ´æ±‡æ€»']
            adjust_column_width(summary_sheet, summary_df)

            # æ·»åŠ æ ¼å¼åŒ–
            format_excel_sheets(workbook)

        print(f"âœ… Excelæ–‡ä»¶å·²ä¿å­˜: {excel_path}")
        return excel_path

    except ImportError:
        print("âš ï¸  è­¦å‘Š: æœªå®‰è£… openpyxl åº“ï¼Œæ— æ³•ç”ŸæˆExcelæ–‡ä»¶")
        print("è¯·è¿è¡Œ: pip install openpyxl")
        return None
    except Exception as e:
        print(f"âŒ ä¿å­˜Excelæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None


def adjust_column_width(sheet, df):
    """è‡ªåŠ¨è°ƒæ•´åˆ—å®½"""
    try:
        from openpyxl.utils import get_column_letter

        # éå†æ‰€æœ‰åˆ—
        for col_idx, column in enumerate(df.columns, 1):
            column_letter = get_column_letter(col_idx)

            # è®¡ç®—åˆ—å®½ï¼šè€ƒè™‘åˆ—åå’Œæ•°æ®å†…å®¹
            max_length = len(str(column))  # åˆ—åé•¿åº¦

            # æ£€æŸ¥è¯¥åˆ—æ‰€æœ‰æ•°æ®çš„é•¿åº¦
            for value in df[column]:
                if pd.notna(value):
                    # å¯¹äºä¸­æ–‡å­—ç¬¦ï¼Œæ¯ä¸ªå­—ç¬¦æŒ‰2ä¸ªå­—ç¬¦è®¡ç®—å®½åº¦
                    str_value = str(value)
                    length = len(str_value) + sum(1 for char in str_value if ord(char) > 127)
                    max_length = max(max_length, length)

            # è®¾ç½®åˆ—å®½ï¼Œæœ€å°å®½åº¦ä¸º10ï¼Œæœ€å¤§å®½åº¦ä¸º50
            adjusted_width = min(max(max_length + 2, 10), 50)
            sheet.column_dimensions[column_letter].width = adjusted_width

    except ImportError:
        print("âš ï¸  è­¦å‘Š: æ— æ³•è°ƒæ•´åˆ—å®½ï¼Œè¯·ç¡®ä¿å®‰è£…äº† openpyxl åº“")
    except Exception as e:
        print(f"âš ï¸  è°ƒæ•´åˆ—å®½æ—¶å‡ºç°é—®é¢˜: {e}")


def format_excel_sheets(workbook):
    """æ ¼å¼åŒ–Excelå·¥ä½œè¡¨"""
    try:
        from openpyxl.styles import Font, PatternFill, Alignment, Border, Side

        # å®šä¹‰æ ·å¼
        header_font = Font(bold=True, color='FFFFFF')
        header_fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')
        border = Border(
            left=Side(style='thin'),
            right=Side(style='thin'),
            top=Side(style='thin'),
            bottom=Side(style='thin')
        )
        center_alignment = Alignment(horizontal='center', vertical='center')

        # æ ¼å¼åŒ–æ¯ä¸ªå·¥ä½œè¡¨
        for sheet_name in workbook.sheetnames:
            sheet = workbook[sheet_name]

            # æ ¼å¼åŒ–æ ‡é¢˜è¡Œ
            for cell in sheet[1]:
                cell.font = header_font
                cell.fill = header_fill
                cell.alignment = center_alignment
                cell.border = border

            # ä¸ºæ‰€æœ‰æ•°æ®æ·»åŠ è¾¹æ¡†
            for row in sheet.iter_rows(min_row=2, max_row=sheet.max_row, max_col=sheet.max_column):
                for cell in row:
                    cell.border = border
                    cell.alignment = Alignment(vertical='center')

            # å†»ç»“é¦–è¡Œ
            sheet.freeze_panes = 'A2'

    except ImportError:
        print("âš ï¸  è­¦å‘Š: æ— æ³•åº”ç”¨Excelæ ¼å¼åŒ–ï¼Œè¯·ç¡®ä¿å®‰è£…äº† openpyxl åº“")
    except Exception as e:
        print(f"âš ï¸  æ ¼å¼åŒ–Excelæ—¶å‡ºç°é—®é¢˜: {e}")


def save_to_csv(detailed_df, summary_df, base_path):
    """ä¿å­˜ä¸ºCSVæ ¼å¼"""
    detailed_csv = f"{base_path}_è¯¦ç»†åˆ†æ.csv"
    summary_csv = f"{base_path}_ç®€æ´æ±‡æ€».csv"

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    if not ensure_directory_exists(detailed_csv) or not ensure_directory_exists(summary_csv):
        return None, None

    try:
        # ä¿å­˜è¯¦ç»†åˆ†æè¡¨
        detailed_df.to_csv(detailed_csv, index=False, encoding='utf-8-sig')

        # ä¿å­˜ç®€æ´æ±‡æ€»è¡¨
        summary_df.to_csv(summary_csv, index=False, encoding='utf-8-sig')

        print(f"âœ… CSVæ–‡ä»¶å·²ä¿å­˜:")
        print(f"  ğŸ“ è¯¦ç»†åˆ†æ: {detailed_csv}")
        print(f"  ğŸ“Š ç®€æ´æ±‡æ€»: {summary_csv}")

        return detailed_csv, summary_csv

    except Exception as e:
        print(f"âŒ ä¿å­˜CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None, None


def analyze_obstacle_data(log_file_path, output_path, output_format='excel'):
    """
    ä¸»è¦åˆ†æå‡½æ•°

    Args:
        log_file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆä¸åŒ…å«æ‰©å±•åï¼‰
        output_format: è¾“å‡ºæ ¼å¼ ('excel', 'csv', 'both')
    """
    # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(log_file_path):
        print(f"âŒ é”™è¯¯: æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file_path}")
        return None, None

    print(f"ğŸ“‚ å¼€å§‹åˆ†ææ—¥å¿—æ–‡ä»¶: {log_file_path}")

    # è¯»å–æ—¥å¿—æ–‡ä»¶
    try:
        with open(log_file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        print(f"ğŸ“ è¯»å–åˆ° {len(lines)} è¡Œæ•°æ®")
    except Exception as e:
        print(f"âŒ è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
        return None, None

    # è§£ææ‰€æœ‰æ•°æ®
    all_data = []
    failed_lines = 0

    for line_num, line in enumerate(lines, 1):
        parsed = parse_log_line(line)
        if parsed:
            all_data.append(parsed)
        else:
            failed_lines += 1
            if failed_lines <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªå¤±è´¥çš„è¡Œ
                print(f"âš ï¸  ç¬¬{line_num}è¡Œè§£æå¤±è´¥: {line.strip()[:100]}...")

    print(f"âœ… æˆåŠŸè§£æ {len(all_data)} æ¡æ•°æ®ï¼Œå¤±è´¥ {failed_lines} æ¡")

    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
    if not all_data:
        print("âŒ é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ—¥å¿—æ•°æ®")
        print("è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")
        return None, None

    # æŒ‰éšœç¢ç‰©åˆ†ç»„ (ObjName + OdDir + HeightLabel)
    obstacle_groups = defaultdict(list)
    for data in all_data:
        key = (data['ObjName'], data['OdDir'], data['HeightLabel'])
        obstacle_groups[key].append(data)

    print(f"ğŸ” è¯†åˆ«åˆ° {len(obstacle_groups)} ä¸ªä¸åŒçš„éšœç¢ç‰©ç»„åˆ")

    # è¯¦ç»†ç»“æœå’Œç®€æ´ç»“æœ
    detailed_results = []
    summary_results = []

    for (obj_name, od_dir, height_label), data_list in obstacle_groups.items():
        # è½¬æ¢OdDirä¸ºå¯è¯»æ ¼å¼
        od_dir_str = "front" if od_dir == 1 else "rear"
        height_label_str = "ä½éšœç¢ç‰©" if height_label == 0 else "é«˜éšœç¢ç‰©"

        print(f"\nğŸ” åˆ†æéšœç¢ç‰©: {obj_name} ({od_dir_str}, {height_label_str})")

        # æŒ‰zhao_od_IDåˆ†ç»„
        id_groups = defaultdict(list)
        for data in data_list:
            id_groups[data['zhao_od_ID']].append(data)

        print(f"  å‘ç° {len(id_groups)} ä¸ªä¸åŒçš„éšœç¢ç‰©ID: {list(id_groups.keys())}")

        # ç”¨äºç®€æ´ç‰ˆç»Ÿè®¡
        obstacle_has_any_error = False
        total_sessions_all_ids = 0
        total_points_all_ids = 0

        for zhao_od_id, id_data in id_groups.items():
            print(f"  åˆ†æID {zhao_od_id}: {len(id_data)} ä¸ªæ•°æ®ç‚¹")

            # è¯†åˆ«æµ‹è¯•æ¬¡æ•°
            sessions = identify_test_sessions(id_data)
            total_sessions_all_ids += len(sessions)

            # æ£€æŸ¥æ‰€æœ‰æ¬¡æ•°æ˜¯å¦éƒ½æ­£ç¡®
            all_sessions_correct = True
            incorrect_sessions = []

            for session_idx, session in enumerate(sessions, 1):
                session_correct = all(
                    (height_label == 0 and data['HeightStatus'] == 1) or
                    (height_label == 1 and data['HeightStatus'] == 2)
                    for data in session
                )

                if not session_correct:
                    all_sessions_correct = False
                    incorrect_sessions.append(session_idx)
                    obstacle_has_any_error = True

            # ç»Ÿè®¡æ€»æ•°æ®ç‚¹
            total_points_all_ids += sum(len(session) for session in sessions)

            # å¦‚æœæ‰€æœ‰æ¬¡æ•°éƒ½æ­£ç¡®ï¼Œåªè¾“å‡ºä¸€æ¡è®°å½•
            if all_sessions_correct:
                total_points = sum(len(session) for session in sessions)
                min_dist = min(data['Dist'] for session in sessions for data in session)
                max_dist = max(data['Dist'] for session in sessions for data in session)

                detailed_results.append({
                    'éšœç¢ç‰©åç§°': obj_name,
                    'ä½ç½®': od_dir_str,
                    'çœŸå®æ ‡ç­¾': height_label_str,
                    'éšœç¢ç‰©ID': zhao_od_id,
                    'æ€»æµ‹è¯•æ¬¡æ•°': len(sessions),
                    'æµ‹è¯•æ¬¡æ•°è¯´æ˜': f"ç¬¬{','.join(map(str, range(1, len(sessions) + 1)))}æ¬¡" if len(
                        sessions) > 1 else "ç¬¬1æ¬¡",
                    'æ•°æ®ç‚¹æ€»æ•°': total_points,
                    'è·ç¦»èŒƒå›´(mm)': f"{max_dist}~{min_dist}",
                    'æ£€æµ‹çŠ¶æ€': "å…¨ç¨‹æ­£ç¡®",
                    'é¦–æ¬¡é”™è¯¯è·ç¦»(mm)': "æ— ",
                    'é”™è¯¯è¯¦æƒ…': "æ— ",
                    'å¤‡æ³¨': f"å…±{len(sessions)}æ¬¡æµ‹è¯•ï¼Œå…¨ç¨‹HeightStatusä¸HeightLabelä¸€è‡´"
                })

            else:
                # å¯¹äºé”™è¯¯çš„æ¬¡æ•°ï¼Œåˆ†åˆ«è¾“å‡º
                for session_idx, session in enumerate(sessions, 1):
                    if session_idx in incorrect_sessions:
                        changes = analyze_height_status_changes(session)
                        error_desc = format_distance_ranges(changes, height_label)
                        status_analysis = get_status_analysis(height_label, True, changes)

                        first_error_dist = None
                        for change in changes:
                            is_error = not ((height_label == 0 and change['status'] == 1) or
                                            (height_label == 1 and change['status'] == 2))
                            if is_error:
                                first_error_dist = change['start_dist']
                                break

                        detailed_results.append({
                            'éšœç¢ç‰©åç§°': obj_name,
                            'ä½ç½®': od_dir_str,
                            'çœŸå®æ ‡ç­¾': height_label_str,
                            'éšœç¢ç‰©ID': zhao_od_id,
                            'æ€»æµ‹è¯•æ¬¡æ•°': len(sessions),
                            'æµ‹è¯•æ¬¡æ•°è¯´æ˜': f"ç¬¬{session_idx}æ¬¡",
                            'æ•°æ®ç‚¹æ€»æ•°': len(session),
                            'è·ç¦»èŒƒå›´(mm)': f"{max(d['Dist'] for d in session)}~{min(d['Dist'] for d in session)}",
                            'æ£€æµ‹çŠ¶æ€': status_analysis,
                            'é¦–æ¬¡é”™è¯¯è·ç¦»(mm)': str(first_error_dist) if first_error_dist else "æ— ",
                            'é”™è¯¯è¯¦æƒ…': error_desc,
                            'å¤‡æ³¨': f"ç¬¬{session_idx}æ¬¡æµ‹è¯•ä¸­HeightStatusä¸HeightLabelä¸ä¸€è‡´"
                        })

                # å¯¹äºæ­£ç¡®çš„æ¬¡æ•°ï¼Œåˆå¹¶è¾“å‡ºä¸€æ¡è®°å½•
                correct_sessions = [i for i in range(1, len(sessions) + 1) if i not in incorrect_sessions]
                if correct_sessions:
                    correct_data = [sessions[i - 1] for i in correct_sessions]
                    total_points = sum(len(session) for session in correct_data)
                    min_dist = min(data['Dist'] for session in correct_data for data in session)
                    max_dist = max(data['Dist'] for session in correct_data for data in session)

                    detailed_results.append({
                        'éšœç¢ç‰©åç§°': obj_name,
                        'ä½ç½®': od_dir_str,
                        'çœŸå®æ ‡ç­¾': height_label_str,
                        'éšœç¢ç‰©ID': zhao_od_id,
                        'æ€»æµ‹è¯•æ¬¡æ•°': len(sessions),
                        'æµ‹è¯•æ¬¡æ•°è¯´æ˜': f"ç¬¬{','.join(map(str, correct_sessions))}æ¬¡",
                        'æ•°æ®ç‚¹æ€»æ•°': total_points,
                        'è·ç¦»èŒƒå›´(mm)': f"{max_dist}~{min_dist}",
                        'æ£€æµ‹çŠ¶æ€': "å…¨ç¨‹æ­£ç¡®",
                        'é¦–æ¬¡é”™è¯¯è·ç¦»(mm)': "æ— ",
                        'é”™è¯¯è¯¦æƒ…': "æ— ",
                        'å¤‡æ³¨': f"ç¬¬{','.join(map(str, correct_sessions))}æ¬¡æµ‹è¯•å…¨ç¨‹æ­£ç¡®"
                    })

        # ç”Ÿæˆç®€æ´ç‰ˆç»“æœ
        if not obstacle_has_any_error:
            # å…¨ç¨‹æ— é”™è¯¯ï¼Œä¸åŒºåˆ†ID
            all_distances = [data['Dist'] for data in data_list]
            summary_results.append({
                'éšœç¢ç‰©åç§°': obj_name,
                'ä½ç½®': od_dir_str,
                'çœŸå®æ ‡ç­¾': height_label_str,
                'æ€»æµ‹è¯•æ¬¡æ•°': total_sessions_all_ids,
                'æ¶‰åŠIDæ•°é‡': len(id_groups),
                'æ•°æ®ç‚¹æ€»æ•°': total_points_all_ids,
                'è·ç¦»èŒƒå›´(mm)': f"{max(all_distances)}~{min(all_distances)}",
                'æ£€æµ‹ç»“æœ': "âœ“ å…¨ç¨‹æ­£ç¡®",
                'å¤‡æ³¨': f"æ‰€æœ‰ID({','.join(map(str, sorted(id_groups.keys())))})å…¨ç¨‹æ£€æµ‹æ­£ç¡®"
            })
        else:
            # å­˜åœ¨é”™è¯¯ï¼Œéœ€è¦æŸ¥çœ‹è¯¦ç»†è¡¨
            error_type = "å­˜åœ¨æŠ¥é«˜" if height_label == 0 else "å­˜åœ¨æŠ¥ä½"
            all_distances = [data['Dist'] for data in data_list]
            summary_results.append({
                'éšœç¢ç‰©åç§°': obj_name,
                'ä½ç½®': od_dir_str,
                'çœŸå®æ ‡ç­¾': height_label_str,
                'æ€»æµ‹è¯•æ¬¡æ•°': total_sessions_all_ids,
                'æ¶‰åŠIDæ•°é‡': len(id_groups),
                'æ•°æ®ç‚¹æ€»æ•°': total_points_all_ids,
                'è·ç¦»èŒƒå›´(mm)': f"{max(all_distances)}~{min(all_distances)}",
                'æ£€æµ‹ç»“æœ': f"âœ— {error_type}",
                'å¤‡æ³¨': "å­˜åœ¨æ£€æµ‹é”™è¯¯ï¼Œè¯¦æƒ…è¯·æŸ¥çœ‹è¯¦ç»†åˆ†æè¡¨"
            })

    # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
    if not detailed_results:
        print("âŒ é”™è¯¯: æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„åˆ†æç»“æœ")
        return None, None

    # è½¬æ¢ä¸ºDataFrame
    detailed_df = pd.DataFrame(detailed_results)
    summary_df = pd.DataFrame(summary_results)

    print(f"\nğŸ“Š ç”Ÿæˆåˆ†æç»“æœ: è¯¦ç»†è®°å½•{len(detailed_results)}æ¡ï¼Œæ±‡æ€»è®°å½•{len(summary_results)}æ¡")

    # å¤„ç†è¾“å‡ºè·¯å¾„
    if output_path.endswith('.csv') or output_path.endswith('.xlsx'):
        base_path = os.path.splitext(output_path)[0]
    else:
        base_path = output_path

    # æ ¹æ®æ ¼å¼é€‰æ‹©ä¿å­˜æ–¹å¼
    saved_files = []

    if output_format.lower() in ['excel', 'both']:
        excel_file = save_to_excel(detailed_df, summary_df, base_path)
        if excel_file:
            saved_files.append(excel_file)

    if output_format.lower() in ['csv', 'both']:
        csv_files = save_to_csv(detailed_df, summary_df, base_path)
        if csv_files[0] and csv_files[1]:
            saved_files.extend(csv_files)

    # ç¾åŒ–è¾“å‡º
    print("=" * 80)
    print("ğŸ¯ éšœç¢ç‰©æ£€æµ‹æ—¥å¿—åˆ†æå®Œæˆ!")
    print("=" * 80)

    if saved_files:
        print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
        for file in saved_files:
            print(f"  â€¢ {file}")
    else:
        print("âŒ è­¦å‘Š: æ²¡æœ‰æˆåŠŸä¿å­˜ä»»ä½•æ–‡ä»¶")

    print()
    print("ğŸ“ˆ ç»Ÿè®¡æ¦‚è§ˆ:")
    print("-" * 50)
    print(f"ğŸ” åˆ†æéšœç¢ç‰©ç±»å‹æ€»æ•°: {len(obstacle_groups)}")
    print(f"ğŸ“ è¯¦ç»†è®°å½•æ¡æ•°: {len(detailed_results)}")
    print(f"ğŸ“‹ æ±‡æ€»è®°å½•æ¡æ•°: {len(summary_results)}")
    print(f"âœ… å…¨ç¨‹æ­£ç¡®çš„éšœç¢ç‰©: {len(summary_df[summary_df['æ£€æµ‹ç»“æœ'].str.contains('å…¨ç¨‹æ­£ç¡®')])}")
    print(f"âŒ å­˜åœ¨é”™è¯¯çš„éšœç¢ç‰©: {len(summary_df[summary_df['æ£€æµ‹ç»“æœ'].str.contains('å­˜åœ¨')])}")
    print()

    print("ğŸ“‹ ç®€æ´æ±‡æ€»è¡¨é¢„è§ˆ:")
    print("-" * 50)
    if len(summary_df) > 0:
        # è®¾ç½®pandasæ˜¾ç¤ºé€‰é¡¹ä»¥æ›´å¥½åœ°æ˜¾ç¤ºä¸­æ–‡
        pd.set_option('display.max_columns', None)
        pd.set_option('display.width', None)
        pd.set_option('display.max_colwidth', 30)
        print(summary_df.head(10).to_string(index=False))
        if len(summary_df) > 10:
            print(f"\n... è¿˜æœ‰ {len(summary_df) - 10} è¡Œæ•°æ®ï¼Œè¯¦è§è¾“å‡ºæ–‡ä»¶")
    else:
        print("æ— æ•°æ®")

    print("\n" + "=" * 80)

    # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
    print("ğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("-" * 50)
    print("ğŸ“Š Excelæ ¼å¼ç‰¹ç‚¹:")
    print("  â€¢ è‡ªåŠ¨è°ƒæ•´åˆ—å®½ï¼Œä¾¿äºé˜…è¯»")
    print("  â€¢ åŒ…å«æ ¼å¼åŒ–æ ·å¼ï¼ˆæ ‡é¢˜è¡Œé«˜äº®ã€è¾¹æ¡†ç­‰ï¼‰")
    print("  â€¢ å†»ç»“é¦–è¡Œï¼Œæ–¹ä¾¿æŸ¥çœ‹æ•°æ®")
    print("  â€¢ ä¸¤ä¸ªå·¥ä½œè¡¨ï¼šè¯¦ç»†åˆ†æ å’Œ ç®€æ´æ±‡æ€»")
    print("\nğŸ“„ CSVæ ¼å¼ç‰¹ç‚¹:")
    print("  â€¢ çº¯æ–‡æœ¬æ ¼å¼ï¼Œå…¼å®¹æ€§å¥½")
    print("  â€¢ å¯ç”¨ä»»ä½•æ–‡æœ¬ç¼–è¾‘å™¨æ‰“å¼€")
    print("  â€¢ åˆ†ä¸ºä¸¤ä¸ªæ–‡ä»¶ï¼šè¯¦ç»†åˆ†æ.csv å’Œ ç®€æ´æ±‡æ€».csv")
    print("\nğŸ”§ ä¾èµ–åº“æç¤º:")
    print("  â€¢ ExcelåŠŸèƒ½éœ€è¦ openpyxl åº“: pip install openpyxl")
    print("  â€¢ å¦‚æœåªéœ€è¦CSVæ ¼å¼ï¼Œæ— éœ€é¢å¤–å®‰è£…")
    print("\nğŸ“Š æ”¹è¿›è¯´æ˜:")
    print("  â€¢ ä¼˜åŒ–äº†æµ‹è¯•æ¬¡æ•°è¯†åˆ«ç®—æ³•ï¼Œèƒ½å‡†ç¡®è¯†åˆ«è¿ç»­çš„æ¥è¿‘è¿‡ç¨‹")
    print("  â€¢ è‡ªåŠ¨è¿‡æ»¤è¿œç¦»éšœç¢ç‰©é˜¶æ®µçš„æ•°æ®")
    print("  â€¢ å¢å¼ºäº†å¯¹è·ç¦»å˜åŒ–å¹²æ‰°çš„æŠ—æ€§")
    print("  â€¢ å¢åŠ äº†è°ƒè¯•ä¿¡æ¯ï¼Œä¾¿äºéªŒè¯è¯†åˆ«ç»“æœ")

    return detailed_df, summary_df


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ä¿®æ”¹è¿™é‡Œçš„è·¯å¾„ä¸ºä½ çš„å®é™…æ–‡ä»¶è·¯å¾„
    log_file_path = r"D:\PythonProject\data\log_files\2.log"  # è¾“å…¥çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„
    output_path = r"D:\PythonProject\data\csv_files\2"  # è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆä¸åŒ…å«æ‰©å±•åï¼‰

    try:
        print("ğŸš€ å¼€å§‹æ”¹è¿›ç‰ˆæ—¥å¿—åˆ†æ...")
        print(f"ğŸ“‚ æ—¥å¿—æ–‡ä»¶: {log_file_path}")
        print(f"ğŸ“ è¾“å‡ºè·¯å¾„: {output_path}")
        print("-" * 80)

        # é»˜è®¤è¾“å‡ºExcelæ ¼å¼
        detailed_df, summary_df = analyze_obstacle_data(
            log_file_path,
            output_path,
            output_format='excel'  # å¯é€‰: 'excel', 'csv', 'both'
        )

        if detailed_df is not None and summary_df is not None:
            print("\nğŸ” è¯¦ç»†åˆ†æè¡¨å­—æ®µè¯´æ˜:")
            print("-" * 50)
            field_descriptions = {
                'éšœç¢ç‰©åç§°': 'éšœç¢ç‰©ç±»å‹åç§°',
                'ä½ç½®': 'front(è½¦å‰) / rear(è½¦å)',
                'çœŸå®æ ‡ç­¾': 'å®é™…çš„é«˜ä½æ ‡ç­¾',
                'éšœç¢ç‰©ID': 'zhao_od_IDç¼–å·',
                'æ€»æµ‹è¯•æ¬¡æ•°': 'è¯¥IDçš„æµ‹è¯•æ¬¡æ•°',
                'æµ‹è¯•æ¬¡æ•°è¯´æ˜': 'å½“å‰è®°å½•å¯¹åº”çš„æµ‹è¯•æ¬¡æ•°',
                'æ•°æ®ç‚¹æ€»æ•°': 'æ•°æ®è®°å½•æ¡æ•°',
                'è·ç¦»èŒƒå›´(mm)': 'æµ‹è¯•è·ç¦»èŒƒå›´(æœ€è¿œ~æœ€è¿‘)',
                'æ£€æµ‹çŠ¶æ€': 'å…¨ç¨‹æ­£ç¡®/å­˜åœ¨æŠ¥é«˜/å­˜åœ¨æŠ¥ä½',
                'é¦–æ¬¡é”™è¯¯è·ç¦»(mm)': 'ç¬¬ä¸€æ¬¡å‡ºç°é”™è¯¯æ—¶çš„è·ç¦»',
                'é”™è¯¯è¯¦æƒ…': 'é”™è¯¯çš„å…·ä½“è·ç¦»åŒºé—´ä¿¡æ¯',
                'å¤‡æ³¨': 'é¢å¤–è¯´æ˜ä¿¡æ¯'
            }

            for field, desc in field_descriptions.items():
                print(f"  â€¢ {field}: {desc}")

            print("\nğŸ“Š ç®€æ´æ±‡æ€»è¡¨è¯´æ˜:")
            print("-" * 50)
            print("  â€¢ å…¨ç¨‹æ— é”™è¯¯çš„éšœç¢ç‰©ï¼šä¸åŒºåˆ†ä¸åŒIDï¼Œåˆå¹¶æ˜¾ç¤º")
            print("  â€¢ å­˜åœ¨é”™è¯¯çš„éšœç¢ç‰©ï¼šæ ‡æ³¨é”™è¯¯ç±»å‹ï¼Œè¯¦æƒ…è§è¯¦ç»†åˆ†æè¡¨")
            print("  â€¢ âœ“ è¡¨ç¤ºæ£€æµ‹æ­£ç¡®ï¼Œâœ— è¡¨ç¤ºå­˜åœ¨é”™è¯¯")

            print("\nğŸ¨ æ ¼å¼é€‰æ‹©:")
            print("-" * 50)
            print("  â€¢ output_format='excel': ä»…è¾“å‡ºExcelæ ¼å¼ï¼ˆé»˜è®¤æ¨èï¼‰")
            print("  â€¢ output_format='csv': ä»…è¾“å‡ºCSVæ ¼å¼")
            print("  â€¢ output_format='both': åŒæ—¶è¾“å‡ºExcelå’ŒCSVæ ¼å¼")

            print("\nğŸ”§ æ–°åŠŸèƒ½ç‰¹ç‚¹:")
            print("-" * 50)
            print("  â€¢ æ™ºèƒ½è¯†åˆ«è¿ç»­æ¥è¿‘è¿‡ç¨‹ï¼Œå‡†ç¡®åŒºåˆ†æµ‹è¯•æ¬¡æ•°")
            print("  â€¢ è‡ªåŠ¨è¿‡æ»¤è½¦è¾†è¿œç¦»éšœç¢ç‰©é˜¶æ®µçš„æ— æ•ˆæ•°æ®")
            print("  â€¢ å¢å¼ºæŠ—å¹²æ‰°èƒ½åŠ›ï¼Œå¤„ç†è·ç¦»å˜åŒ–ä¸­çš„å¼‚å¸¸ç‚¹")
            print("  â€¢ æä¾›è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ï¼Œä¾¿äºéªŒè¯åˆ†æç»“æœ")
        else:
            print("\nâŒ åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥:")
            print("  1. æ—¥å¿—æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®")
            print("  2. è¾“å‡ºç›®å½•æ˜¯å¦æœ‰å†™å…¥æƒé™")
            print("  3. æ˜¯å¦å®‰è£…äº†å¿…è¦çš„ä¾èµ–åº“")

    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ—¥å¿—æ–‡ä»¶ '{log_file_path}'")
        print("è¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œæˆ–å°†æ—¥å¿—å†…å®¹ä¿å­˜ä¸ºå¯¹åº”çš„æ—¥å¿—æ–‡ä»¶")
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()