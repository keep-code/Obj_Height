import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import joblib
import os
import json
from datetime import datetime

warnings.filterwarnings('ignore')

# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class LightGBMToCConverter:
    """LightGBMæ¨¡å‹è½¬Cè¯­è¨€ä»£ç ç”Ÿæˆå™¨"""

    def __init__(self, model, feature_names, output_dir="./c_model"):
        self.model = model
        self.feature_names = feature_names
        self.output_dir = output_dir
        self.num_features = len(feature_names)
        os.makedirs(output_dir, exist_ok=True)

    def extract_model_info(self):
        """æå–æ¨¡å‹ä¿¡æ¯"""
        model_dict = self.model.dump_model()
        self.num_trees = model_dict['num_tree_per_iteration']
        self.trees = model_dict['tree_info']
        self.objective = model_dict.get('objective', 'binary')

        print(f"æ¨¡å‹ä¿¡æ¯:")
        print(f"- æ ‘çš„æ•°é‡: {len(self.trees)}")
        print(f"- ç‰¹å¾æ•°é‡: {self.num_features}")
        print(f"- ç›®æ ‡å‡½æ•°: {self.objective}")

        return model_dict

    def generate_tree_function(self, tree_dict, tree_id):
        """ç”Ÿæˆå•ä¸ªæ ‘çš„Cå‡½æ•°"""

        def generate_node_code(node, indent=0):
            spaces = "    " * indent

            if 'leaf_value' in node:
                return f"{spaces}return {node['leaf_value']:.6f};\n"
            else:
                feature_idx = node['split_feature']
                threshold = node['threshold']
                left_child = node['left_child']
                right_child = node['right_child']

                code = f"{spaces}if (features[{feature_idx}] <= {threshold:.6f}) {{\n"
                code += generate_node_code(left_child, indent + 1)
                code += f"{spaces}}} else {{\n"
                code += generate_node_code(right_child, indent + 1)
                code += f"{spaces}}}\n"

                return code

        func_code = f"""
static double tree_{tree_id}(const double *features) {{
{generate_node_code(tree_dict['tree_structure'], 1)}}}
"""
        return func_code

    def generate_prediction_function(self):
        """ç”Ÿæˆé¢„æµ‹å‡½æ•°"""
        tree_calls = []
        for i in range(len(self.trees)):
            tree_calls.append(f"    sum += tree_{i}(features);")

        tree_calls_str = "\n".join(tree_calls)

        prediction_code = f"""
double predict_raw(const double *features) {{
    double sum = 0.0;
{tree_calls_str}
    return sum;
}}

double predict_probability(const double *features) {{
    double raw_score = predict_raw(features);
    return 1.0 / (1.0 + exp(-raw_score));  // sigmoidå‡½æ•°
}}

int predict_class(const double *features) {{
    return predict_probability(features) > 0.5 ? 1 : 0;
}}

double predict_confidence(const double *features) {{
    double prob = predict_probability(features);
    return prob > 0.5 ? prob : (1.0 - prob);  // ç½®ä¿¡åº¦
}}
"""
        return prediction_code

    def generate_header_file(self):
        """ç”Ÿæˆå¤´æ–‡ä»¶"""
        header_content = f"""#ifndef OBSTACLE_HEIGHT_MODEL_H
#define OBSTACLE_HEIGHT_MODEL_H

#include <math.h>

// æ¨¡å‹ä¿¡æ¯
#define NUM_FEATURES {self.num_features}
#define NUM_TREES {len(self.trees)}

// ç‰¹å¾ç´¢å¼•å®šä¹‰
"""
        for i, feature_name in enumerate(self.feature_names):
            feature_macro = feature_name.upper().replace(' ', '_').replace('-', '_').replace('(', '').replace(')', '')
            header_content += f"#define FEATURE_{feature_macro} {i}\n"

        header_content += f"""
// å‡½æ•°å£°æ˜
double predict_raw(const double *features);
double predict_probability(const double *features);
int predict_class(const double *features);
double predict_confidence(const double *features);

// é¢„æµ‹ç»“æœç»“æ„ä½“
typedef struct {{
    int height_label;          // é«˜åº¦æ ‡ç­¾ (0=ä½éšœç¢ç‰©, 1=é«˜éšœç¢ç‰©)
    double probability;        // é¢„æµ‹æ¦‚ç‡
    double confidence;         // ç½®ä¿¡åº¦
    double raw_score;         // åŸå§‹åˆ†æ•°
}} ObstacleHeightPrediction;

ObstacleHeightPrediction predict_obstacle_height(const double *features);

#endif // OBSTACLE_HEIGHT_MODEL_H
"""
        return header_content

    def generate_source_file(self):
        """ç”Ÿæˆæºæ–‡ä»¶"""
        model_info = self.extract_model_info()

        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        source_content = f"""/*
 * éšœç¢ç‰©é«˜åº¦åˆ†ç±»å™¨ - LightGBMæ¨¡å‹Cè¯­è¨€å®ç°
 * ç”Ÿæˆæ—¶é—´: {timestamp}
 * ç‰¹å¾æ•°é‡: {self.num_features}
 * æ ‘çš„æ•°é‡: {len(self.trees)}
 * 
 * ä½¿ç”¨è¯´æ˜:
 * - è¾“å…¥: doubleæ•°ç»„ï¼Œé•¿åº¦ä¸ºNUM_FEATURESï¼ŒåŒ…å«æ‰€æœ‰ç‰¹å¾å€¼
 * - è¾“å‡º: ObstacleHeightPredictionç»“æ„ä½“ï¼ŒåŒ…å«é¢„æµ‹ç»“æœå’Œç½®ä¿¡åº¦
 */

#include "obstacle_height_model.h"
#include <math.h>

"""

        # ç”Ÿæˆæ‰€æœ‰æ ‘çš„å‡½æ•°
        for i, tree in enumerate(self.trees):
            source_content += self.generate_tree_function(tree, i)

        # ç”Ÿæˆé¢„æµ‹å‡½æ•°
        source_content += self.generate_prediction_function()

        # ç”Ÿæˆä¾¿åˆ©å‡½æ•°
        source_content += """
ObstacleHeightPrediction predict_obstacle_height(const double *features) {
    ObstacleHeightPrediction result;
    result.raw_score = predict_raw(features);
    result.probability = predict_probability(features);
    result.height_label = predict_class(features);
    result.confidence = predict_confidence(features);
    return result;
}
"""

        return source_content

    def generate_test_file(self):
        """ç”Ÿæˆæµ‹è¯•æ–‡ä»¶"""
        test_content = f"""/*
 * éšœç¢ç‰©é«˜åº¦åˆ†ç±»å™¨æµ‹è¯•ç¨‹åº
 */

#include <stdio.h>
#include <stdlib.h>
#include "obstacle_height_model.h"

int main() {{
    printf("=== éšœç¢ç‰©é«˜åº¦åˆ†ç±»å™¨ Cè¯­è¨€ç‰ˆæœ¬ ===\\n");
    printf("ç‰¹å¾æ•°é‡: %d\\n", NUM_FEATURES);
    printf("æ ‘çš„æ•°é‡: %d\\n", NUM_TREES);
    printf("\\n");

    // ç¤ºä¾‹ç‰¹å¾æ•°æ® (è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹)
    double features[NUM_FEATURES] = {{0}};

    // ç¤ºä¾‹1: è®¾ç½®ä¸€äº›ç¤ºä¾‹ç‰¹å¾å€¼
    printf("ç¤ºä¾‹1: æµ‹è¯•æ•°æ®\\n");
"""

        # ä¸ºå‰å‡ ä¸ªç‰¹å¾è®¾ç½®ç¤ºä¾‹å€¼
        for i in range(min(5, self.num_features)):
            test_content += f"    features[{i}] = {100.0 * (i + 1):.1f};  // {self.feature_names[i]}\n"

        test_content += f"""    
    // è¿›è¡Œé¢„æµ‹
    ObstacleHeightPrediction result = predict_obstacle_height(features);

    printf("é¢„æµ‹ç»“æœ:\\n");
    printf("- é«˜åº¦æ ‡ç­¾: %d (%s)\\n", result.height_label, 
           result.height_label ? "é«˜éšœç¢ç‰©" : "ä½éšœç¢ç‰©");
    printf("- æ¦‚ç‡: %.6f\\n", result.probability);
    printf("- ç½®ä¿¡åº¦: %.6f\\n", result.confidence);
    printf("- åŸå§‹åˆ†æ•°: %.6f\\n", result.raw_score);
    printf("\\n");

    // ç¤ºä¾‹2: å¦ä¸€ç»„æµ‹è¯•æ•°æ®
    printf("ç¤ºä¾‹2: å¦ä¸€ç»„æµ‹è¯•æ•°æ®\\n");
    for(int i = 0; i < NUM_FEATURES; i++) {{
        features[i] = (double)(i * 50 + 200);  // ç®€å•çš„æµ‹è¯•æ•°æ®
    }}

    result = predict_obstacle_height(features);
    printf("é¢„æµ‹ç»“æœ:\\n");
    printf("- é«˜åº¦æ ‡ç­¾: %d (%s)\\n", result.height_label, 
           result.height_label ? "é«˜éšœç¢ç‰©" : "ä½éšœç¢ç‰©");
    printf("- æ¦‚ç‡: %.6f\\n", result.probability);
    printf("- ç½®ä¿¡åº¦: %.6f\\n", result.confidence);
    printf("- åŸå§‹åˆ†æ•°: %.6f\\n", result.raw_score);

    return 0;
}}
"""
        return test_content

    def convert(self):
        """æ‰§è¡Œè½¬æ¢"""
        print(f"ğŸš€ å¼€å§‹è½¬æ¢LightGBMæ¨¡å‹åˆ°Cè¯­è¨€...")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")

        # ç”Ÿæˆå¤´æ–‡ä»¶
        header_content = self.generate_header_file()
        header_file = os.path.join(self.output_dir, "obstacle_height_model.h")
        with open(header_file, 'w') as f:
            f.write(header_content)
        print(f"âœ… ç”Ÿæˆå¤´æ–‡ä»¶: {header_file}")

        # ç”Ÿæˆæºæ–‡ä»¶
        source_content = self.generate_source_file()
        source_file = os.path.join(self.output_dir, "obstacle_height_model.c")
        with open(source_file, 'w') as f:
            f.write(source_content)
        print(f"âœ… ç”Ÿæˆæºæ–‡ä»¶: {source_file}")

        # ç”Ÿæˆæµ‹è¯•æ–‡ä»¶
        test_content = self.generate_test_file()
        test_file = os.path.join(self.output_dir, "test_obstacle_height.c")
        with open(test_file, 'w') as f:
            f.write(test_content)
        print(f"âœ… ç”Ÿæˆæµ‹è¯•æ–‡ä»¶: {test_file}")

        # ç”ŸæˆMakefile
        makefile_content = self._generate_makefile()
        makefile_file = os.path.join(self.output_dir, "Makefile")
        with open(makefile_file, 'w') as f:
            f.write(makefile_content)
        print(f"âœ… ç”ŸæˆMakefile: {makefile_file}")

        # ä¿å­˜ç‰¹å¾æ˜ å°„
        self._save_feature_mapping()
        print(f"âœ… ä¿å­˜ç‰¹å¾æ˜ å°„")

        # ç”Ÿæˆä½¿ç”¨è¯´æ˜
        self._generate_usage_guide()

        print(f"\nğŸ‰ è½¬æ¢å®Œæˆ!")
        print(f"ğŸ“‚ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {self.output_dir}")
        print(f"ğŸ”¨ ç¼–è¯‘å‘½ä»¤: cd {self.output_dir} && make")
        print(f"ğŸš€ è¿è¡Œæµ‹è¯•: cd {self.output_dir} && ./test_obstacle_height")

    def _generate_makefile(self):
        """ç”ŸæˆMakefile"""
        return """# éšœç¢ç‰©é«˜åº¦åˆ†ç±»å™¨ Cè¯­è¨€ç‰ˆæœ¬ Makefile

CC = gcc
CFLAGS = -Wall -O2 -std=c99
LDFLAGS = -lm

TARGET = test_obstacle_height
SOURCES = obstacle_height_model.c test_obstacle_height.c
HEADERS = obstacle_height_model.h

$(TARGET): $(SOURCES) $(HEADERS)
	$(CC) $(CFLAGS) -o $(TARGET) $(SOURCES) $(LDFLAGS)

clean:
	rm -f $(TARGET)

.PHONY: clean
"""

    def _save_feature_mapping(self):
        """ä¿å­˜ç‰¹å¾æ˜ å°„ä¿¡æ¯"""
        feature_mapping = {
            "feature_names": self.feature_names,
            "feature_count": self.num_features,
            "feature_indices": {name: i for i, name in enumerate(self.feature_names)},
            "model_info": {
                "trees": len(self.trees),
                "objective": self.objective
            }
        }

        mapping_file = os.path.join(self.output_dir, "feature_mapping.json")
        with open(mapping_file, 'w', encoding='utf-8') as f:
            json.dump(feature_mapping, f, indent=2, ensure_ascii=False)

        # ç”Ÿæˆç‰¹å¾åˆ—è¡¨æ–‡ä»¶
        readme_file = os.path.join(self.output_dir, "feature_list.txt")
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write("éšœç¢ç‰©é«˜åº¦åˆ†ç±»å™¨ç‰¹å¾åˆ—è¡¨\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"ç‰¹å¾æ€»æ•°: {self.num_features}\n")
            f.write(f"æ ‘çš„æ•°é‡: {len(self.trees)}\n\n")
            f.write("ç‰¹å¾ç´¢å¼•å’Œåç§°:\n")
            for i, name in enumerate(self.feature_names):
                f.write(f"{i:2d}: {name}\n")

    def _generate_usage_guide(self):
        """ç”Ÿæˆä½¿ç”¨è¯´æ˜"""
        guide_content = f"""# éšœç¢ç‰©é«˜åº¦åˆ†ç±»å™¨ Cè¯­è¨€ç‰ˆæœ¬

## é¡¹ç›®è¯´æ˜
æœ¬é¡¹ç›®å°†è®­ç»ƒå¥½çš„LightGBMéšœç¢ç‰©é«˜åº¦åˆ†ç±»æ¨¡å‹è½¬æ¢ä¸ºCè¯­è¨€å®ç°ï¼Œå¯ä»¥éƒ¨ç½²åˆ°åµŒå…¥å¼ç³»ç»Ÿæˆ–å…¶ä»–C/C++é¡¹ç›®ä¸­ã€‚

## æ–‡ä»¶ç»“æ„
- `obstacle_height_model.h` - å¤´æ–‡ä»¶ï¼ŒåŒ…å«å‡½æ•°å£°æ˜å’Œå®å®šä¹‰
- `obstacle_height_model.c` - æºæ–‡ä»¶ï¼ŒåŒ…å«æ¨¡å‹å®ç°
- `test_obstacle_height.c` - æµ‹è¯•ç¨‹åº
- `Makefile` - ç¼–è¯‘é…ç½®æ–‡ä»¶
- `feature_mapping.json` - ç‰¹å¾æ˜ å°„ä¿¡æ¯
- `feature_list.txt` - ç‰¹å¾åˆ—è¡¨
- `README.md` - ä½¿ç”¨è¯´æ˜

## å¿«é€Ÿå¼€å§‹

### ç¼–è¯‘å’Œæµ‹è¯•
```bash
# ç¼–è¯‘
make

# è¿è¡Œæµ‹è¯•
./test_obstacle_height

# æ¸…ç†ç¼–è¯‘æ–‡ä»¶
make clean
```

### åœ¨æ‚¨çš„é¡¹ç›®ä¸­é›†æˆ

1. å°†ä»¥ä¸‹æ–‡ä»¶å¤åˆ¶åˆ°æ‚¨çš„é¡¹ç›®ä¸­ï¼š
   - `obstacle_height_model.h`
   - `obstacle_height_model.c`

2. åœ¨æ‚¨çš„ä»£ç ä¸­åŒ…å«å¤´æ–‡ä»¶ï¼š
   ```c
   #include "obstacle_height_model.h"
   ```

3. ç¼–è¯‘æ—¶é“¾æ¥æ•°å­¦åº“ï¼š
   ```bash
   gcc -o your_program your_program.c obstacle_height_model.c -lm
   ```

## API ä½¿ç”¨è¯´æ˜

### æ ¸å¿ƒå‡½æ•°
- `predict_obstacle_height(features)` - ä¸»è¦é¢„æµ‹å‡½æ•°ï¼Œè¿”å›å®Œæ•´ç»“æœ
- `predict_class(features)` - ä»…è¿”å›åˆ†ç±»ç»“æœ (0æˆ–1)
- `predict_probability(features)` - è¿”å›é¢„æµ‹æ¦‚ç‡ (0.0-1.0)
- `predict_confidence(features)` - è¿”å›ç½®ä¿¡åº¦ (0.5-1.0)

### ä½¿ç”¨ç¤ºä¾‹
```c
#include "obstacle_height_model.h"
#include <stdio.h>

int main() {{
    // å‡†å¤‡ç‰¹å¾æ•°æ® (å…±{self.num_features}ä¸ªç‰¹å¾)
    double features[NUM_FEATURES];

    // è®¾ç½®ç‰¹å¾å€¼ (æ ¹æ®æ‚¨çš„å®é™…ä¼ æ„Ÿå™¨æ•°æ®)"""

        # æ·»åŠ ç‰¹å¾ç¤ºä¾‹
        for i, feature_name in enumerate(self.feature_names[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ªä½œä¸ºç¤ºä¾‹
            guide_content += f"""
    features[{i}] = your_sensor_data_{i};  // {feature_name}"""

        if len(self.feature_names) > 5:
            guide_content += f"""
    // ... è®¾ç½®å‰©ä½™çš„ {len(self.feature_names) - 5} ä¸ªç‰¹å¾"""

        guide_content += f"""

    // è¿›è¡Œé¢„æµ‹
    ObstacleHeightPrediction result = predict_obstacle_height(features);

    // ä½¿ç”¨é¢„æµ‹ç»“æœ
    if (result.height_label == 1) {{
        printf("æ£€æµ‹åˆ°é«˜éšœç¢ç‰©ï¼æ¦‚ç‡: %.2f, ç½®ä¿¡åº¦: %.2f\\n", 
               result.probability, result.confidence);
        // æ‰§è¡Œé«˜éšœç¢ç‰©å¤„ç†é€»è¾‘
    }} else {{
        printf("æ£€æµ‹åˆ°ä½éšœç¢ç‰©ã€‚æ¦‚ç‡: %.2f, ç½®ä¿¡åº¦: %.2f\\n", 
               1.0 - result.probability, result.confidence);
        // æ‰§è¡Œä½éšœç¢ç‰©å¤„ç†é€»è¾‘
    }}

    return 0;
}}
```

## ç‰¹å¾è¯´æ˜

æ¨¡å‹éœ€è¦ {self.num_features} ä¸ªç‰¹å¾è¾“å…¥ï¼Œç‰¹å¾ç´¢å¼•å’Œå«ä¹‰å¦‚ä¸‹ï¼š

| ç´¢å¼• | ç‰¹å¾åç§° | è¯´æ˜ |
|------|----------|------|"""

        for i, name in enumerate(self.feature_names):
            guide_content += f"""
| {i} | {name} | ä¼ æ„Ÿå™¨ç‰¹å¾ |"""

        guide_content += f"""

## é¢„æµ‹ç»“æœè¯´æ˜

### ObstacleHeightPrediction ç»“æ„ä½“
```c
typedef struct {{
    int height_label;      // 0=ä½éšœç¢ç‰©, 1=é«˜éšœç¢ç‰©
    double probability;    // é¢„æµ‹ä¸ºé«˜éšœç¢ç‰©çš„æ¦‚ç‡ (0.0-1.0)
    double confidence;     // é¢„æµ‹ç½®ä¿¡åº¦ (0.5-1.0)
    double raw_score;      // æ¨¡å‹åŸå§‹è¾“å‡ºåˆ†æ•°
}} ObstacleHeightPrediction;
```

### ç»“æœè§£é‡Š
- **height_label**: æœ€ç»ˆåˆ†ç±»ç»“æœ
  - 0: ä½éšœç¢ç‰©
  - 1: é«˜éšœç¢ç‰©
- **probability**: é¢„æµ‹ä¸ºé«˜éšœç¢ç‰©çš„æ¦‚ç‡
  - è¶Šæ¥è¿‘1.0ï¼Œè¶Šå¯èƒ½æ˜¯é«˜éšœç¢ç‰©
  - è¶Šæ¥è¿‘0.0ï¼Œè¶Šå¯èƒ½æ˜¯ä½éšœç¢ç‰©
- **confidence**: é¢„æµ‹ç½®ä¿¡åº¦
  - èŒƒå›´ 0.5-1.0
  - è¶Šæ¥è¿‘1.0ï¼Œé¢„æµ‹è¶Šå¯é 
- **raw_score**: æ¨¡å‹åŸå§‹è¾“å‡ºï¼Œå¯ç”¨äºè°ƒè¯•

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **ç‰¹å¾é¢„å¤„ç†**: ç¡®ä¿è¾“å…¥ç‰¹å¾çš„æ•°å€¼èŒƒå›´ä¸è®­ç»ƒæ—¶ä¸€è‡´
2. **å†…å­˜ä½¿ç”¨**: æ¨¡å‹ä¸ºé™æ€å®ç°ï¼Œä¸éœ€è¦åŠ¨æ€å†…å­˜åˆ†é…
3. **è®¡ç®—å¤æ‚åº¦**: O(æ ‘çš„æ•°é‡ Ã— å¹³å‡æ ‘æ·±åº¦)
4. **çº¿ç¨‹å®‰å…¨**: æ‰€æœ‰å‡½æ•°éƒ½æ˜¯çº¿ç¨‹å®‰å…¨çš„

## æ³¨æ„äº‹é¡¹

1. ç‰¹å¾æ•°ç»„å¿…é¡»åŒ…å« {self.num_features} ä¸ªdoubleç±»å‹çš„å€¼
2. ç‰¹å¾é¡ºåºå¿…é¡»ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
3. éœ€è¦é“¾æ¥æ•°å­¦åº“ (-lm)
4. è¾“å…¥ç‰¹å¾å€¼åº”åœ¨åˆç†èŒƒå›´å†…ï¼Œé¿å…æç«¯å€¼

## æ•…éšœæ’é™¤

### ç¼–è¯‘é”™è¯¯
- ç¡®ä¿åŒ…å«äº† `<math.h>` å¤´æ–‡ä»¶
- ç¡®ä¿é“¾æ¥äº†æ•°å­¦åº“ (`-lm`)

### é¢„æµ‹ç»“æœå¼‚å¸¸
- æ£€æŸ¥ç‰¹å¾å€¼æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
- ç¡®è®¤ç‰¹å¾é¡ºåºä¸è®­ç»ƒæ—¶ä¸€è‡´
- éªŒè¯è¾“å…¥æ•°æ®æ ¼å¼æ­£ç¡®

## æ¨¡å‹ä¿¡æ¯

- **ç‰¹å¾æ•°é‡**: {self.num_features}
- **æ ‘çš„æ•°é‡**: {len(self.trees) if hasattr(self, 'trees') else 'N/A'}
- **ç›®æ ‡å‡½æ•°**: äºŒåˆ†ç±» (binary classification)
- **è¾“å‡ºç±»åˆ«**: 0 (ä½éšœç¢ç‰©), 1 (é«˜éšœç¢ç‰©)
"""

        guide_file = os.path.join(self.output_dir, "README.md")
        with open(guide_file, 'w', encoding='utf-8') as f:
            f.write(guide_content)
        print(f"âœ… ç”Ÿæˆä½¿ç”¨æŒ‡å—: {guide_file}")


class ObstacleHeightClassifier:
    """éšœç¢ç‰©é«˜åº¦åˆ†ç±»å™¨ - å¢å¼ºç‰ˆæœ¬ï¼ˆå«Cè¯­è¨€è½¬æ¢åŠŸèƒ½ï¼‰"""

    def __init__(self, config=None):
        # é»˜è®¤é…ç½®
        self.config = {
            'model_dir': './models',
            'plot_dir': './plots',
            'results_dir': './results',
            'misclassified_dir': './misclassified',
            'c_model_dir': './c_model',  # æ–°å¢ï¼šCè¯­è¨€æ¨¡å‹è¾“å‡ºç›®å½•
            'auto_create_dirs': True,
            'encoding': 'utf-8-sig',
            'test_size': 0.3,
            'random_state': 42,
            'save_model': True,
            'auto_convert_to_c': True,  # æ–°å¢ï¼šè‡ªåŠ¨è½¬æ¢ä¸ºCè¯­è¨€
        }

        # æ›´æ–°ç”¨æˆ·é…ç½®
        if config:
            self.config.update(config)

        # æ¨¡å‹ç›¸å…³å±æ€§
        self.model = None
        self.feature_names = []
        self.test_results = {}

        # åˆ›å»ºè¾“å‡ºç›®å½•
        if self.config['auto_create_dirs']:
            self._create_directories()

    def _create_directories(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        dirs = [
            self.config['model_dir'],
            self.config['plot_dir'],
            self.config['results_dir'],
            self.config['misclassified_dir'],
            self.config['c_model_dir']  # æ–°å¢Cè¯­è¨€æ¨¡å‹ç›®å½•
        ]
        for dir_path in dirs:
            os.makedirs(dir_path, exist_ok=True)
            print(f"ç¡®ä¿ç›®å½•å­˜åœ¨: {dir_path}")

    def _save_csv(self, df, filepath, encoding=None):
        """ä¿å­˜CSVæ–‡ä»¶ï¼Œè§£å†³ä¸­æ–‡ä¹±ç é—®é¢˜"""
        encoding = encoding or self.config['encoding']
        try:
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            df.to_csv(filepath, index=False, encoding=encoding)
            print(f"æ–‡ä»¶å·²ä¿å­˜åˆ°: {filepath}")
            print(f"æ–‡ä»¶å¤§å°: {os.path.getsize(filepath)} bytes")
            return True
        except Exception as e:
            print(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            try:
                df.to_csv(filepath, index=False, encoding='gbk')
                print(f"ä½¿ç”¨GBKç¼–ç ä¿å­˜åˆ°: {filepath}")
                return True
            except Exception as e2:
                try:
                    df.to_csv(filepath, index=False, encoding='utf-8')
                    print(f"ä½¿ç”¨UTF-8ç¼–ç ä¿å­˜åˆ°: {filepath}")
                    return True
                except Exception as e3:
                    print(f"æ‰€æœ‰ç¼–ç å°è¯•å¤±è´¥: {e3}")
                    return False

    def load_data(self, filepath):
        """åŠ è½½æ•°æ®"""
        print(f"åŠ è½½æ•°æ®: {filepath}")
        try:
            df = pd.read_csv(filepath, encoding='utf-8-sig')
        except:
            try:
                df = pd.read_csv(filepath, encoding='gbk')
            except:
                df = pd.read_csv(filepath)

        print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
        if 'HeightLabel' in df.columns:
            print(f"HeightLabelåˆ†å¸ƒ: {df['HeightLabel'].value_counts().to_dict()}")
            print(f"é«˜éšœç¢ç‰©æ¯”ä¾‹: {df['HeightLabel'].mean():.2%}")

        return df

    def feature_engineering(self, df):
        """ç‰¹å¾å·¥ç¨‹ - æ”¹è¿›ç‰ˆæœ¬"""
        print("è¿›è¡Œç‰¹å¾å·¥ç¨‹...")
        df_processed = df.copy()

        # æ’é™¤æè¿°æ€§å’Œæ— å…³ç‰¹å¾
        exclude_cols = [
            'HeightLabel',
            'Train_OD_Project',
            'ObjName',
            'Direction',
            'Obj_ID',
            'CurCyc',
            'TxSensID',
        ]

        print(f"æ’é™¤çš„ç‰¹å¾åˆ—: {exclude_cols}")

        # è·å–åŸºç¡€ç‰¹å¾åˆ—
        base_features = [col for col in df_processed.columns if col not in exclude_cols]
        print(f"åŸºç¡€ç‰¹å¾åˆ—: {base_features}")

        # åˆ›å»ºæ–°ç‰¹å¾
        feature_ops = {
            'DeEcho_Ratio': lambda x: x['PosDeDis1'] / (x['PosDeDis2'] + 1e-8),
            'CeEcho_Ratio': lambda x: x['PosCeDis1'] / (x['PosCeDis2'] + 1e-8),
            'DeAmp_Ratio': lambda x: x['PosDeAmp1'] / (x['PosDeAmp2'] + 1e-8),
            'CeAmp_Ratio': lambda x: x['PosCeAmp1'] / (x['PosCeAmp2'] + 1e-8),
            'Total_DeEcho': lambda x: x['PosDeDis1'] + x['PosDeDis2'],
            'Total_CeEcho': lambda x: x['PosCeDis1'] + x['PosCeDis2'],
            'Total_DeAmp': lambda x: x['PosDeAmp1'] + x['PosDeAmp2'],
            'Total_CeAmp': lambda x: x['PosCeAmp1'] + x['PosCeAmp2'],
            'DeDis_Diff': lambda x: x['PosDeDis1'] - x['PosDeDis2'],
            'CeDis_Diff': lambda x: x['PosCeDis1'] - x['PosCeDis2'],
            'DeAmp_Diff': lambda x: x['PosDeAmp1'] - x['PosDeAmp2'],
            'CeAmp_Diff': lambda x: x['PosCeAmp1'] - x['PosCeAmp2'],
            'Avg_Echo_Strength': lambda x: (x['AvgDeEchoHigh_SameTx'] + x['AvgCeEchoHigh_SameTxRx']) / 2,
            'Distance_Ratio': lambda x: x['TrainObjDist'] / (x['AngleDist'] + 1e-8),
            'Echo_Strength_Ratio': lambda x: x['AvgDeEchoHigh_SameTx'] / (x['AvgCeEchoHigh_SameTxRx'] + 1e-8),
            'Odo_Stability': lambda x: x['OdoDiffObjDis'] / (x['OdoDiffDeDis'] + 1e-8),
        }

        # åº”ç”¨ç‰¹å¾å·¥ç¨‹
        for feature_name, operation in feature_ops.items():
            try:
                df_processed[feature_name] = operation(df_processed)
                print(f"åˆ›å»ºç‰¹å¾: {feature_name}")
            except Exception as e:
                print(f"ç‰¹å¾ {feature_name} åˆ›å»ºå¤±è´¥: {e}")

        # æ›´æ–°ç‰¹å¾åˆ—è¡¨
        self.feature_names = [col for col in df_processed.columns if col not in exclude_cols]
        print(f"ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œå…± {len(self.feature_names)} ä¸ªç‰¹å¾")

        # ç‰¹å¾è´¨é‡æ£€æŸ¥
        self._check_feature_quality(df_processed)

        return df_processed

    def _check_feature_quality(self, df):
        """æ£€æŸ¥ç‰¹å¾è´¨é‡"""
        print("\n=== ç‰¹å¾è´¨é‡æ£€æŸ¥ ===")

        # æ£€æŸ¥ç¼ºå¤±å€¼
        missing_stats = df[self.feature_names].isnull().sum()
        if missing_stats.sum() > 0:
            print("å‘ç°ç¼ºå¤±å€¼:")
            print(missing_stats[missing_stats > 0])
        else:
            print("âœ“ æ— ç¼ºå¤±å€¼")

        # æ£€æŸ¥å¸¸æ•°ç‰¹å¾
        constant_features = []
        for feature in self.feature_names:
            if df[feature].nunique() <= 1:
                constant_features.append(feature)

        if constant_features:
            print(f"å‘ç°å¸¸æ•°ç‰¹å¾: {constant_features}")
            self.feature_names = [f for f in self.feature_names if f not in constant_features]
            print(f"ç§»é™¤å¸¸æ•°ç‰¹å¾åå‰©ä½™: {len(self.feature_names)} ä¸ªç‰¹å¾")
        else:
            print("âœ“ æ— å¸¸æ•°ç‰¹å¾")

        # æ£€æŸ¥æ— ç©·å€¼
        inf_features = []
        for feature in self.feature_names:
            if np.isinf(df[feature]).any():
                inf_features.append(feature)

        if inf_features:
            print(f"å‘ç°æ— ç©·å€¼ç‰¹å¾: {inf_features}")
            for feature in inf_features:
                df[feature] = df[feature].replace([np.inf, -np.inf], np.nan)
                df[feature] = df[feature].fillna(df[feature].median())
            print("å·²æ›¿æ¢æ— ç©·å€¼ä¸ºä¸­ä½æ•°")
        else:
            print("âœ“ æ— æ— ç©·å€¼")

    def train(self, train_df):
        """è®­ç»ƒæ¨¡å‹"""
        print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")

        # ä¿å­˜åŸå§‹è®­ç»ƒæ•°æ®
        self.train_df_original = train_df.copy()

        # å‡†å¤‡æ•°æ®
        X = train_df[self.feature_names]
        y = train_df['HeightLabel']

        print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: X{X.shape}, y{y.shape}")

        # åˆ’åˆ†æ•°æ®é›†
        if self.config['test_size'] > 0:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=self.config['test_size'],
                random_state=self.config['random_state'], stratify=y
            )
            print(f"è®­ç»ƒé›†: {X_train.shape[0]}, æµ‹è¯•é›†: {X_test.shape[0]}")
        else:
            X_train, y_train = X, y
            X_test = y_test = None
            print(f"ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ: {X_train.shape[0]}")

        # LightGBMå‚æ•°
        params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'random_state': self.config['random_state'],
            'is_unbalance': True,
            'min_child_samples': 20,
            'min_child_weight': 0.001,
            'subsample_for_bin': 200000,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
        }

        # è®­ç»ƒ
        train_data = lgb.Dataset(X_train, label=y_train)
        if X_test is not None:
            valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
            valid_sets = [train_data, valid_data]
            valid_names = ['train', 'valid']
        else:
            valid_sets = [train_data]
            valid_names = ['train']

        self.model = lgb.train(
            params, train_data, valid_sets=valid_sets, valid_names=valid_names,
            num_boost_round=1000, callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]
        )

        # ä¿å­˜æµ‹è¯•ç»“æœ
        if X_test is not None:
            y_pred_proba = self.model.predict(X_test, num_iteration=self.model.best_iteration)
            y_pred = (y_pred_proba > 0.5).astype(int)

            self.test_results = {
                'X_test': X_test, 'y_test': y_test,
                'y_pred': y_pred, 'y_pred_proba': y_pred_proba
            }

            # è¯„ä¼°
            self._evaluate_model()

        # ä¿å­˜æ¨¡å‹
        if self.config['save_model']:
            model_path = self.save_model()

            # è‡ªåŠ¨è½¬æ¢ä¸ºCè¯­è¨€
            if self.config['auto_convert_to_c']:
                print(f"\nğŸ”„ å¼€å§‹è‡ªåŠ¨è½¬æ¢ä¸ºCè¯­è¨€...")
                self.convert_to_c_language()

        return self.test_results if X_test is not None else None

    def convert_to_c_language(self):
        """å°†è®­ç»ƒå¥½çš„æ¨¡å‹è½¬æ¢ä¸ºCè¯­è¨€ä»£ç """
        if self.model is None:
            print("âŒ æ¨¡å‹æœªè®­ç»ƒï¼Œæ— æ³•è½¬æ¢")
            return False

        try:
            print(f"ğŸ“ è½¬æ¢LightGBMæ¨¡å‹ä¸ºCè¯­è¨€ä»£ç ...")

            # åˆ›å»ºè½¬æ¢å™¨
            converter = LightGBMToCConverter(
                model=self.model,
                feature_names=self.feature_names,
                output_dir=self.config['c_model_dir']
            )

            # æ‰§è¡Œè½¬æ¢
            converter.convert()

            print(f"\nğŸ‰ Cè¯­è¨€è½¬æ¢å®Œæˆ!")
            print(f"ğŸ“ Cè¯­è¨€ä»£ç ä¿å­˜åœ¨: {self.config['c_model_dir']}")
            print(f"ğŸ”¨ ç¼–è¯‘å‘½ä»¤: cd {self.config['c_model_dir']} && make")
            print(f"ğŸš€ è¿è¡Œæµ‹è¯•: cd {self.config['c_model_dir']} && ./test_obstacle_height")

            return True

        except Exception as e:
            print(f"âŒ Cè¯­è¨€è½¬æ¢å¤±è´¥: {e}")
            return False

    def _evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        if not self.test_results:
            return

        y_test = self.test_results['y_test']
        y_pred = self.test_results['y_pred']
        y_pred_proba = self.test_results['y_pred_proba']

        print("\n=== æ¨¡å‹è¯„ä¼°ç»“æœ ===")
        print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")
        print(f"AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred, target_names=['ä½éšœç¢ç‰©', 'é«˜éšœç¢ç‰©']))

        # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        self.plot_confusion_matrix()

        # ä¿å­˜é”™è¯¯åˆ†ç±»æ ·æœ¬
        self._save_misclassified_samples(self.train_df_original)

    def plot_confusion_matrix(self, title_suffix=""):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        if not self.test_results:
            print("æ— æµ‹è¯•æ•°æ®ï¼Œè·³è¿‡æ··æ·†çŸ©é˜µç»˜åˆ¶")
            return

        y_test = self.test_results['y_test']
        y_pred = self.test_results['y_pred']
        cm = confusion_matrix(y_test, y_pred)

        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=['ä½éšœç¢ç‰©', 'é«˜éšœç¢ç‰©'],
                    yticklabels=['ä½éšœç¢ç‰©', 'é«˜éšœç¢ç‰©'])
        plt.title(f'æ··æ·†çŸ©é˜µ{title_suffix}')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')

        save_path = os.path.join(self.config['plot_dir'], f'confusion_matrix{title_suffix.replace(" ", "_")}.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {save_path}")
        plt.show()

        return cm

    def plot_feature_importance(self, top_n=20):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§"""
        if self.model is None:
            print("æ¨¡å‹æœªè®­ç»ƒ")
            return

        importance = self.model.feature_importance(importance_type='gain')
        feature_imp = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)

        plt.figure(figsize=(10, 8))
        sns.barplot(data=feature_imp.head(top_n), x='importance', y='feature')
        plt.title(f'å‰{top_n}ä¸ªé‡è¦ç‰¹å¾')
        plt.xlabel('é‡è¦æ€§')

        save_path = os.path.join(self.config['plot_dir'], f'feature_importance_top{top_n}.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜åˆ°: {save_path}")
        plt.show()

        return feature_imp

    def save_model(self, filepath=None):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None:
            print("æ¨¡å‹æœªè®­ç»ƒ")
            return

        if filepath is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = os.path.join(self.config['model_dir'], f'obstacle_classifier_{timestamp}.joblib')

        model_data = {
            'model': self.model,
            'feature_names': self.feature_names,
            'config': self.config,
            'timestamp': datetime.now().isoformat()
        }

        joblib.dump(model_data, filepath)
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")

        return filepath

    def _save_misclassified_samples(self, train_df=None, suffix=""):
        """ä¿å­˜é”™è¯¯åˆ†ç±»æ ·æœ¬"""
        if not self.test_results:
            print("æ— æµ‹è¯•ç»“æœï¼Œè·³è¿‡é”™è¯¯æ ·æœ¬ä¿å­˜")
            return

        X_test = self.test_results['X_test']
        y_test = self.test_results['y_test']
        y_pred = self.test_results['y_pred']
        y_pred_proba = self.test_results['y_pred_proba']

        misclassified_mask = (y_test != y_pred)
        if not any(misclassified_mask):
            print("æ‰€æœ‰æ ·æœ¬é¢„æµ‹æ­£ç¡®ï¼Œæ— é”™è¯¯åˆ†ç±»æ ·æœ¬")
            return

        print(f"å‘ç° {sum(misclassified_mask)} ä¸ªé”™è¯¯åˆ†ç±»æ ·æœ¬")

        misclassified_indices = X_test[misclassified_mask].index

        if train_df is not None:
            misclassified_samples = train_df.loc[misclassified_indices].copy()
        else:
            misclassified_samples = X_test[misclassified_mask].copy()

        y_test_mis = y_test[misclassified_mask].reset_index(drop=True)
        y_pred_mis = y_pred[misclassified_mask]
        y_pred_proba_mis = y_pred_proba[misclassified_mask]

        misclassified_samples = misclassified_samples.reset_index(drop=True)
        misclassified_samples['True_Label'] = y_test_mis
        misclassified_samples['Predicted_Label'] = y_pred_mis
        misclassified_samples['Prediction_Probability'] = y_pred_proba_mis
        misclassified_samples['Confidence'] = np.abs(y_pred_proba_mis - 0.5) * 2
        misclassified_samples['Error_Type'] = ['False_Positive' if true_label == 0 else 'False_Negative'
                                               for true_label in y_test_mis]

        filename = f'misclassified_samples{suffix}.csv'
        save_path = os.path.join(self.config['misclassified_dir'], filename)

        os.makedirs(self.config['misclassified_dir'], exist_ok=True)
        success = self._save_csv(misclassified_samples, save_path)

        if success:
            false_positive_count = sum(misclassified_samples['Error_Type'] == 'False_Positive')
            false_negative_count = sum(misclassified_samples['Error_Type'] == 'False_Negative')

            print(f"é”™è¯¯åˆ†ç±»æ ·æœ¬æ•°: {len(misclassified_samples)}")
            print(f"å‡é˜³æ€§æ ·æœ¬æ•°: {false_positive_count}")
            print(f"å‡é˜´æ€§æ ·æœ¬æ•°: {false_negative_count}")

        return misclassified_samples


# å¿«é€Ÿè¿è¡Œå‡½æ•° - å¢å¼ºç‰ˆæœ¬
def quick_run_with_c_conversion(train_file, test_file=None, config=None):
    """å¿«é€Ÿè¿è¡Œå‡½æ•° - å«è‡ªåŠ¨Cè¯­è¨€è½¬æ¢"""
    print("=== éšœç¢ç‰©é«˜åº¦åˆ†ç±»å™¨ - å¢å¼ºç‰ˆæœ¬ï¼ˆå«Cè¯­è¨€è½¬æ¢ï¼‰===\n")

    # é»˜è®¤é…ç½®
    default_config = {
        'model_dir': './models_enhanced',
        'plot_dir': './plots_enhanced',
        'results_dir': './results_enhanced',
        'misclassified_dir': './misclassified_enhanced',
        'c_model_dir': './c_model_enhanced',
        'auto_convert_to_c': True,
        'test_size': 0.0,  # ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ
        'random_state': 42,
    }

    if config:
        default_config.update(config)

    # åˆ›å»ºåˆ†ç±»å™¨
    classifier = ObstacleHeightClassifier(default_config)

    # è®­ç»ƒ
    print("1. æ•°æ®åŠ è½½å’Œç‰¹å¾å·¥ç¨‹")
    train_df = classifier.load_data(train_file)
    train_df_processed = classifier.feature_engineering(train_df)

    print("\n2. æ¨¡å‹è®­ç»ƒ")
    classifier.train(train_df_processed)

    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    print("\n3. ç‰¹å¾é‡è¦æ€§åˆ†æ")
    feature_imp = classifier.plot_feature_importance()

    # é¢„æµ‹æµ‹è¯•æ•°æ®
    if test_file and os.path.exists(test_file):
        print("\n4. æµ‹è¯•æ•°æ®é¢„æµ‹")
        results = classifier.predict(test_file)
    else:
        print("\n4. è·³è¿‡é¢„æµ‹é˜¶æ®µï¼ˆæ— æµ‹è¯•æ–‡ä»¶ï¼‰")

    print("\n=== è¿è¡Œå®Œæˆ ===")
    print(f"ğŸ“Š Pythonæ¨¡å‹æ–‡ä»¶: {classifier.config['model_dir']}")
    print(f"ğŸ”§ Cè¯­è¨€ä»£ç : {classifier.config['c_model_dir']}")

    return classifier


def predict_with_c_model(c_model_dir, features):
    """
    ä½¿ç”¨ç”Ÿæˆçš„Cè¯­è¨€æ¨¡å‹è¿›è¡Œé¢„æµ‹çš„PythonåŒ…è£…å‡½æ•°

    Parameters:
    -----------
    c_model_dir : str
        Cè¯­è¨€æ¨¡å‹ç›®å½•
    features : list or numpy.array
        ç‰¹å¾æ•°ç»„

    Returns:
    --------
    dict : é¢„æµ‹ç»“æœ
    """
    import subprocess
    import tempfile
    import json

    try:
        # åˆ›å»ºä¸´æ—¶çš„Cç¨‹åºæ¥è°ƒç”¨æ¨¡å‹
        temp_c_code = f"""
#include <stdio.h>
#include <stdlib.h>
#include "obstacle_height_model.h"

int main() {{
    double features[NUM_FEATURES] = {{{', '.join(map(str, features))}}};

    ObstacleHeightPrediction result = predict_obstacle_height(features);

    // è¾“å‡ºJSONæ ¼å¼ç»“æœ
    printf("{{");
    printf("\\"height_label\\": %d,", result.height_label);
    printf("\\"probability\\": %.6f,", result.probability);
    printf("\\"confidence\\": %.6f,", result.confidence);
    printf("\\"raw_score\\": %.6f", result.raw_score);
    printf("}}");

    return 0;
}}
"""

        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', suffix='.c', delete=False) as f:
            f.write(temp_c_code)
            temp_c_file = f.name

        # ç¼–è¯‘å’Œè¿è¡Œ
        temp_exe = temp_c_file.replace('.c', '')

        compile_cmd = [
            'gcc', '-o', temp_exe, temp_c_file,
            os.path.join(c_model_dir, 'obstacle_height_model.c'),
            '-I', c_model_dir, '-lm'
        ]

        subprocess.run(compile_cmd, check=True, capture_output=True)
        result = subprocess.run([temp_exe], capture_output=True, text=True, check=True)

        # è§£æç»“æœ
        prediction = json.loads(result.stdout)

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(temp_c_file)
        os.unlink(temp_exe)

        return prediction

    except Exception as e:
        print(f"Cæ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
        return None


def convert_existing_model_to_c(model_path, output_dir="./c_model_converted"):
    """
    è½¬æ¢å·²ä¿å­˜çš„.joblibæ¨¡å‹æ–‡ä»¶ä¸ºCè¯­è¨€ä»£ç 

    Parameters:
    -----------
    model_path : str
        .joblibæ¨¡å‹æ–‡ä»¶è·¯å¾„
    output_dir : str
        è¾“å‡ºç›®å½•
    """
    try:
        # åŠ è½½æ¨¡å‹
        model_data = joblib.load(model_path)
        if isinstance(model_data, dict):
            model = model_data['model']
            feature_names = model_data.get('feature_names', [])
        else:
            model = model_data
            feature_names = []

        print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")
        print(f"ğŸ¯ è¾“å‡ºç›®å½•: {output_dir}")

        # åˆ›å»ºè½¬æ¢å™¨å¹¶æ‰§è¡Œè½¬æ¢
        converter = LightGBMToCConverter(model, feature_names, output_dir)
        converter.convert()

        return True

    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    # ==================== é…ç½®åŒºåŸŸ ====================

    # æ–‡ä»¶è·¯å¾„é…ç½®
    TRAIN_FILE = r'D:\PythonProject\data\processed_data\train_group1.csv'
    TEST_FILE = r'D:\PythonProject\data\processed_data\train_group2.csv'

    # è¾“å‡ºè·¯å¾„é…ç½®
    OUTPUT_CONFIG = {
        'model_dir': r'D:\PythonProject\model\saved_model_enhanced',
        'plot_dir': r'D:\PythonProject\results\visualization_results_enhanced',
        'results_dir': r'D:\PythonProject\results\prediction_results_enhanced',
        'misclassified_dir': r'D:\PythonProject\results\misclassified_results_enhanced',
        'c_model_dir': r'D:\PythonProject\c_model',  # Cè¯­è¨€æ¨¡å‹è¾“å‡ºç›®å½•
        'auto_create_dirs': True,
        'encoding': 'utf-8-sig',
        'test_size': 0.0,  # ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ
        'random_state': 42,
        'save_model': True,
        'auto_convert_to_c': True,  # å¯ç”¨è‡ªåŠ¨è½¬æ¢ä¸ºCè¯­è¨€
    }

    print(f"ğŸš€ å¼€å§‹è¿è¡Œå¢å¼ºç‰ˆéšœç¢ç‰©åˆ†ç±»å™¨...")
    print(f"ğŸ“ Cè¯­è¨€æ¨¡å‹å°†ä¿å­˜åœ¨: {OUTPUT_CONFIG['c_model_dir']}")
    print(f"{'=' * 80}")

    # è¿è¡Œå¢å¼ºç‰ˆåˆ†ç±»å™¨ï¼ˆè‡ªåŠ¨è½¬æ¢ä¸ºCè¯­è¨€ï¼‰
    classifier = quick_run_with_c_conversion(TRAIN_FILE, TEST_FILE, OUTPUT_CONFIG)

    print(f"\n{'=' * 80}")
    print(f"âœ… è®­ç»ƒå’Œè½¬æ¢å®Œæˆï¼")
    print(f"\nğŸ“Š Pythonæ¨¡å‹å’Œç»“æœ:")
    print(f"   - æ¨¡å‹æ–‡ä»¶: {OUTPUT_CONFIG['model_dir']}")
    print(f"   - å¯è§†åŒ–ç»“æœ: {OUTPUT_CONFIG['plot_dir']}")
    print(f"   - é¢„æµ‹ç»“æœ: {OUTPUT_CONFIG['results_dir']}")
    print(f"   - é”™è¯¯æ ·æœ¬: {OUTPUT_CONFIG['misclassified_dir']}")

    print(f"\nğŸ”§ Cè¯­è¨€æ¨¡å‹:")
    print(f"   - Cä»£ç ç›®å½•: {OUTPUT_CONFIG['c_model_dir']}")
    print(f"   - ç¼–è¯‘å‘½ä»¤: cd {OUTPUT_CONFIG['c_model_dir']} && make")
    print(f"   - è¿è¡Œæµ‹è¯•: cd {OUTPUT_CONFIG['c_model_dir']} && ./test_obstacle_height")

    print(f"\nğŸ¯ ä¸»è¦æ”¹è¿›:")
    print(f"   âœ“ è‡ªåŠ¨ç”ŸæˆCè¯­è¨€ä»£ç ")
    print(f"   âœ“ åŒ…å«å®Œæ•´çš„ç¼–è¯‘å’Œæµ‹è¯•æ–‡ä»¶")
    print(f"   âœ“ è¯¦ç»†çš„ä½¿ç”¨æ–‡æ¡£å’Œç¤ºä¾‹")
    print(f"   âœ“ ä¼˜åŒ–çš„ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹å‚æ•°")

    # ç¤ºä¾‹ï¼šå¦‚ä½•å•ç‹¬è½¬æ¢å·²æœ‰æ¨¡å‹
    print(f"\nğŸ’¡ æç¤ºï¼šå¦‚æœä½ æœ‰å·²ä¿å­˜çš„.joblibæ¨¡å‹æ–‡ä»¶ï¼Œå¯ä»¥è¿™æ ·è½¬æ¢:")
    print(f"   convert_existing_model_to_c('path/to/your/model.joblib', './output_dir')")

    # ç¤ºä¾‹ï¼šå¦‚ä½•ä½¿ç”¨ç”Ÿæˆçš„Cæ¨¡å‹è¿›è¡Œé¢„æµ‹
    print(f"\nğŸ”® Cæ¨¡å‹é¢„æµ‹ç¤ºä¾‹:")
    print(f"   features = [100.0, 200.0, ...]  # ä½ çš„ç‰¹å¾æ•°æ®")
    print(f"   result = predict_with_c_model('{OUTPUT_CONFIG['c_model_dir']}', features)")
    print(f"   print(result)  # {{'height_label': 1, 'probability': 0.85, ...}}")


# ä½¿ç”¨è¯´æ˜å’Œæµ‹è¯•å‡½æ•°
def test_c_model_integration():
    """æµ‹è¯•Cè¯­è¨€æ¨¡å‹é›†æˆ"""
    print("ğŸ§ª æµ‹è¯•Cè¯­è¨€æ¨¡å‹é›†æˆ...")

    # ç¤ºä¾‹ç‰¹å¾æ•°æ®ï¼ˆéœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
    sample_features = [
        100.0, 200.0, 150.0, 250.0,  # PosDeDis1, PosDeAmp1, PosDeDis2, PosDeAmp2
        120.0, 180.0, 110.0, 190.0,  # PosCeDis1, PosCeAmp1, PosCeDis2, PosCeAmp2
        500.0, 300.0, 400.0,  # TrainObjDist, AvgDeEchoHigh_SameTx, AvgCeEchoHigh_SameTxRx
        50.0, 30.0, 20.0, 40.0,  # OdoDiffObjDis, OdoDiffDeDis, OdoDiff, ObjDiff
        800.0, 25.0, 450.0  # CosAngle, RateOfVhoDeDiff, AngleDist
        # åŠ ä¸Šå·¥ç¨‹ç‰¹å¾ä¼šè‡ªåŠ¨è®¡ç®—
    ]

    # å¦‚æœCæ¨¡å‹ç›®å½•å­˜åœ¨ï¼Œå°è¯•é¢„æµ‹
    c_model_dir = "./c_model"
    if os.path.exists(os.path.join(c_model_dir, "obstacle_height_model.h")):
        try:
            result = predict_with_c_model(c_model_dir, sample_features)
            if result:
                print(f"âœ… Cæ¨¡å‹é¢„æµ‹æˆåŠŸ:")
                print(
                    f"   - é«˜åº¦æ ‡ç­¾: {result['height_label']} ({'é«˜éšœç¢ç‰©' if result['height_label'] else 'ä½éšœç¢ç‰©'})")
                print(f"   - æ¦‚ç‡: {result['probability']:.4f}")
                print(f"   - ç½®ä¿¡åº¦: {result['confidence']:.4f}")
            else:
                print("âŒ Cæ¨¡å‹é¢„æµ‹å¤±è´¥")
        except Exception as e:
            print(f"âš ï¸  Cæ¨¡å‹æµ‹è¯•è·³è¿‡: {e}")
    else:
        print(f"âš ï¸  Cæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {c_model_dir}")
        print(f"   è¯·å…ˆè¿è¡Œä¸»ç¨‹åºç”ŸæˆCæ¨¡å‹")


# å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œæ‰§è¡Œæµ‹è¯•
if __name__ == "__main__" and len(sys.argv) > 1 and sys.argv[1] == "test":
    test_c_model_integration()