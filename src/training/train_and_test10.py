import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, \
    precision_recall_curve
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import joblib
import os
import json
from datetime import datetime
from typing import Dict, Optional, Tuple, List
import logging

warnings.filterwarnings('ignore')

# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class ObstacleHeightClassifier:
    """éšœç¢ç‰©é«˜åº¦åˆ†ç±»å™¨ - æ”¹è¿›ç‰ˆæœ¬"""

    def __init__(self, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–åˆ†ç±»å™¨

        Parameters:
        -----------
        config : dict, optional
            é…ç½®å­—å…¸ï¼ŒåŒ…å«è·¯å¾„è®¾ç½®ç­‰å‚æ•°
        """
        # é»˜è®¤é…ç½®
        self.config = {
            'model_dir': './models',
            'plot_dir': './plots',
            'results_dir': './results',
            'misclassified_dir': './misclassified',
            'log_dir': './logs',
            'auto_create_dirs': True,
            'encoding': 'utf-8-sig',
            'test_size': 0.0,
            'random_state': 42,
            'save_model': True,
            'cv_folds': 5,  # äº¤å‰éªŒè¯æŠ˜æ•°
            'threshold_optimization': True,  # æ˜¯å¦ä¼˜åŒ–åˆ†ç±»é˜ˆå€¼
            'feature_selection': True,  # æ˜¯å¦è¿›è¡Œç‰¹å¾é€‰æ‹©
            'model_params': {  # LightGBMå‚æ•°
                'objective': 'binary',
                'metric': 'binary_logloss',
                'boosting_type': 'gbdt',
                'num_leaves': 31,
                'learning_rate': 0.05,
                'feature_fraction': 0.9,
                'bagging_fraction': 0.8,
                'bagging_freq': 5,
                'verbose': -1,
                'is_unbalance': True
            }
        }

        # æ›´æ–°ç”¨æˆ·é…ç½®
        if config:
            self.config.update(config)
            # æ·±åº¦æ›´æ–°æ¨¡å‹å‚æ•°
            if 'model_params' in config:
                self.config['model_params'].update(config['model_params'])

        # æ¨¡å‹ç›¸å…³å±æ€§
        self.model = None
        self.scaler = None
        self.feature_names = []
        self.selected_features = []
        self.test_results = {}
        self.optimal_threshold = 0.5
        self.cv_scores = {}

        # è®¾ç½®æ—¥å¿—
        self._setup_logging()

        # åˆ›å»ºè¾“å‡ºç›®å½•
        if self.config['auto_create_dirs']:
            self._create_directories()

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        log_dir = self.config['log_dir']
        os.makedirs(log_dir, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = os.path.join(log_dir, f'classifier_{timestamp}.log')

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def _create_directories(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        for dir_path in [self.config['model_dir'], self.config['plot_dir'],
                         self.config['results_dir'], self.config['misclassified_dir'],
                         self.config['log_dir']]:
            os.makedirs(dir_path, exist_ok=True)
            self.logger.info(f"ç¡®ä¿ç›®å½•å­˜åœ¨: {dir_path}")

    def _save_csv(self, df: pd.DataFrame, filepath: str, encoding: Optional[str] = None) -> bool:
        """ä¿å­˜CSVæ–‡ä»¶ï¼Œè§£å†³ä¸­æ–‡ä¹±ç é—®é¢˜"""
        encoding = encoding or self.config['encoding']
        try:
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            df.to_csv(filepath, index=False, encoding=encoding)
            self.logger.info(f"æ–‡ä»¶å·²ä¿å­˜åˆ°: {filepath} ({os.path.getsize(filepath)} bytes)")
            return True
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            # å°è¯•å…¶ä»–ç¼–ç 
            for alt_encoding in ['gbk', 'utf-8']:
                try:
                    df.to_csv(filepath, index=False, encoding=alt_encoding)
                    self.logger.info(f"ä½¿ç”¨{alt_encoding}ç¼–ç ä¿å­˜åˆ°: {filepath}")
                    return True
                except Exception:
                    continue
            self.logger.error("æ‰€æœ‰ç¼–ç å°è¯•å¤±è´¥")
            return False

    def load_data(self, filepath: str) -> pd.DataFrame:
        """åŠ è½½æ•°æ®"""
        self.logger.info(f"åŠ è½½æ•°æ®: {filepath}")

        # å°è¯•ä¸åŒç¼–ç è¯»å–
        for encoding in ['utf-8-sig', 'gbk', 'utf-8']:
            try:
                df = pd.read_csv(filepath, encoding=encoding)
                break
            except Exception as e:
                if encoding == 'utf-8':  # æœ€åä¸€æ¬¡å°è¯•
                    raise e
                continue

        self.logger.info(f"æ•°æ®å½¢çŠ¶: {df.shape}")

        # æ•°æ®è´¨é‡æ£€æŸ¥
        missing_info = df.isnull().sum()
        if missing_info.sum() > 0:
            self.logger.warning(f"å‘ç°ç¼ºå¤±å€¼:\n{missing_info[missing_info > 0]}")

        if 'HeightLabel' in df.columns:
            self.logger.info(f"HeightLabelåˆ†å¸ƒ: {df['HeightLabel'].value_counts().to_dict()}")
            self.logger.info(f"é«˜éšœç¢ç‰©æ¯”ä¾‹: {df['HeightLabel'].mean():.2%}")

            # æ£€æŸ¥ç±»åˆ«ä¸å¹³è¡¡
            class_ratio = df['HeightLabel'].value_counts()
            minority_ratio = min(class_ratio) / max(class_ratio)
            if minority_ratio < 0.1:
                self.logger.warning(f"ç±»åˆ«ä¸¥é‡ä¸å¹³è¡¡ï¼Œå°‘æ•°ç±»æ¯”ä¾‹: {minority_ratio:.2%}")

        return df

    def feature_engineering(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¢å¼ºçš„ç‰¹å¾å·¥ç¨‹"""
        self.logger.info("è¿›è¡Œç‰¹å¾å·¥ç¨‹...")
        df_processed = df.copy()

        # æ’é™¤éç‰¹å¾åˆ—
        exclude_cols = ['HeightLabel', 'Train_OD_Project', 'ObjName', 'Direction']

        # åŸºç¡€ç‰¹å¾å·¥ç¨‹
        feature_ops = {
            # æ¯”å€¼ç‰¹å¾
            'DeEcho_Ratio': lambda x: x['PosDeDis1'] / (x['PosDeDis2'] + 1e-8),
            'CeEcho_Ratio': lambda x: x['PosCeDis1'] / (x['PosCeDis2'] + 1e-8),
            'DeAmp_Ratio': lambda x: x['PosDeAmp1'] / (x['PosDeAmp2'] + 1e-8),
            'CeAmp_Ratio': lambda x: x['PosCeAmp1'] / (x['PosCeAmp2'] + 1e-8),

            # æ€»å’Œç‰¹å¾
            'Total_DeEcho': lambda x: x['PosDeDis1'] + x['PosDeDis2'],
            'Total_CeEcho': lambda x: x['PosCeDis1'] + x['PosCeDis2'],
            'Total_DeAmp': lambda x: x['PosDeAmp1'] + x['PosDeAmp2'],
            'Total_CeAmp': lambda x: x['PosCeAmp1'] + x['PosCeAmp2'],

            # å·®å€¼ç‰¹å¾
            'DeDis_Diff': lambda x: x['PosDeDis1'] - x['PosDeDis2'],
            'CeDis_Diff': lambda x: x['PosCeDis1'] - x['PosCeDis2'],
            'DeAmp_Diff': lambda x: x['PosDeAmp1'] - x['PosDeAmp2'],
            'CeAmp_Diff': lambda x: x['PosCeAmp1'] - x['PosCeAmp2'],

            # ç»„åˆç‰¹å¾
            'Avg_Echo_Strength': lambda x: (x['AvgDeEchoHigh_SameTx'] + x['AvgCeEchoHigh_SameTxRx']) / 2,
            'Distance_Ratio': lambda x: x['TrainObjDist'] / (x['AngleDist'] + 1e-8),

            # æ–°å¢ç‰¹å¾
            'Echo_Consistency': lambda x: 1 - abs(x['PosDeDis1'] - x['PosCeDis1']) / (
                        x['PosDeDis1'] + x['PosCeDis1'] + 1e-8),
            'Amp_Consistency': lambda x: 1 - abs(x['PosDeAmp1'] - x['PosCeAmp1']) / (
                        x['PosDeAmp1'] + x['PosCeAmp1'] + 1e-8),
            'Signal_Quality': lambda x: (x['PosDeAmp1'] + x['PosCeAmp1']) / (x['PosDeAmp2'] + x['PosCeAmp2'] + 1e-8)
        }

        # åº”ç”¨ç‰¹å¾å·¥ç¨‹
        for feature_name, operation in feature_ops.items():
            try:
                df_processed[feature_name] = operation(df_processed)
            except Exception as e:
                self.logger.warning(f"ç‰¹å¾ {feature_name} åˆ›å»ºå¤±è´¥: {e}")

        # å¤„ç†å¼‚å¸¸å€¼å’Œæ— ç©·å€¼
        df_processed = df_processed.replace([np.inf, -np.inf], np.nan)

        # æ›´æ–°ç‰¹å¾åˆ—è¡¨
        self.feature_names = [col for col in df_processed.columns if col not in exclude_cols]
        self.logger.info(f"ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œå…± {len(self.feature_names)} ä¸ªç‰¹å¾")

        # ç‰¹å¾é€‰æ‹©
        if self.config['feature_selection'] and 'HeightLabel' in df_processed.columns:
            self._feature_selection(df_processed)

        return df_processed

    def _feature_selection(self, df: pd.DataFrame):
        """åŸºäºç›¸å…³æ€§å’Œé‡è¦æ€§çš„ç‰¹å¾é€‰æ‹©"""
        X = df[self.feature_names].fillna(0)
        y = df['HeightLabel']

        # è®¡ç®—ç‰¹å¾ä¸ç›®æ ‡çš„ç›¸å…³æ€§
        correlations = []
        for feature in self.feature_names:
            try:
                corr = abs(X[feature].corr(y))
                correlations.append((feature, corr))
            except:
                correlations.append((feature, 0))

        # æŒ‰ç›¸å…³æ€§æ’åºï¼Œé€‰æ‹©å‰80%çš„ç‰¹å¾
        correlations.sort(key=lambda x: x[1], reverse=True)
        n_select = max(10, int(len(self.feature_names) * 0.8))
        self.selected_features = [feat for feat, _ in correlations[:n_select]]

        self.logger.info(f"ç‰¹å¾é€‰æ‹©å®Œæˆï¼Œä¿ç•™ {len(self.selected_features)} ä¸ªç‰¹å¾")

    def cross_validate(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """äº¤å‰éªŒè¯"""
        self.logger.info(f"å¼€å§‹ {self.config['cv_folds']} æŠ˜äº¤å‰éªŒè¯...")

        cv = StratifiedKFold(n_splits=self.config['cv_folds'], shuffle=True,
                             random_state=self.config['random_state'])

        cv_scores = {'accuracy': [], 'auc': [], 'precision': [], 'recall': []}

        for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):
            self.logger.info(f"  è®­ç»ƒç¬¬ {fold + 1}/{self.config['cv_folds']} æŠ˜...")

            X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]
            y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]

            # åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
            train_data = lgb.Dataset(X_train_cv, label=y_train_cv)
            val_data = lgb.Dataset(X_val_cv, label=y_val_cv, reference=train_data)

            # è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨éªŒè¯é›†è¿›è¡Œæ—©åœï¼‰
            model_cv = lgb.train(
                self.config['model_params'],
                train_data,
                valid_sets=[train_data, val_data],
                valid_names=['train', 'valid'],
                num_boost_round=1000,
                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
            )

            # é¢„æµ‹å’Œè¯„ä¼°
            y_pred_proba = model_cv.predict(X_val_cv, num_iteration=model_cv.best_iteration)
            y_pred = (y_pred_proba > 0.5).astype(int)

            from sklearn.metrics import precision_score, recall_score
            cv_scores['accuracy'].append(accuracy_score(y_val_cv, y_pred))
            cv_scores['auc'].append(roc_auc_score(y_val_cv, y_pred_proba))
            cv_scores['precision'].append(precision_score(y_val_cv, y_pred, zero_division=0))
            cv_scores['recall'].append(recall_score(y_val_cv, y_pred, zero_division=0))

        # è®¡ç®—å¹³å‡åˆ†æ•°å’Œæ ‡å‡†å·®
        self.cv_scores = {metric: np.mean(scores) for metric, scores in cv_scores.items()}
        cv_stds = {metric: np.std(scores) for metric, scores in cv_scores.items()}

        self.logger.info("äº¤å‰éªŒè¯ç»“æœ:")
        for metric, score in self.cv_scores.items():
            self.logger.info(f"  {metric}: {score:.4f} Â± {cv_stds[metric]:.4f}")

        return self.cv_scores

    def optimize_threshold(self, X: pd.DataFrame, y: pd.Series):
        """ä¼˜åŒ–åˆ†ç±»é˜ˆå€¼"""
        if not self.config['threshold_optimization']:
            return

        self.logger.info("ä¼˜åŒ–åˆ†ç±»é˜ˆå€¼...")

        # ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹æ¦‚ç‡
        # æ ¹æ®æ¨¡å‹æ˜¯å¦æœ‰best_iterationå±æ€§æ¥å†³å®šé¢„æµ‹æ–¹å¼
        if hasattr(self.model, 'best_iteration') and self.model.best_iteration is not None:
            y_pred_proba = self.model.predict(X, num_iteration=self.model.best_iteration)
        else:
            y_pred_proba = self.model.predict(X)

        # è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡
        precision, recall, thresholds = precision_recall_curve(y, y_pred_proba)

        # é€‰æ‹©F1åˆ†æ•°æœ€é«˜çš„é˜ˆå€¼
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
        best_idx = np.argmax(f1_scores)
        self.optimal_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5

        self.logger.info(f"æœ€ä¼˜é˜ˆå€¼: {self.optimal_threshold:.4f} (F1: {f1_scores[best_idx]:.4f})")

    def train(self, train_df: pd.DataFrame) -> Optional[Dict]:
        """è®­ç»ƒæ¨¡å‹"""
        self.logger.info("å¼€å§‹è®­ç»ƒæ¨¡å‹...")

        # ä¿å­˜åŸå§‹è®­ç»ƒæ•°æ®
        self.train_df_original = train_df.copy()

        # å‡†å¤‡æ•°æ®
        features_to_use = self.selected_features if self.selected_features else self.feature_names
        X = train_df[features_to_use].fillna(0)
        y = train_df['HeightLabel']

        # æ•°æ®æ ‡å‡†åŒ–ï¼ˆå¯é€‰ï¼‰
        if self.config.get('normalize_features', False):
            self.scaler = StandardScaler()
            X = pd.DataFrame(self.scaler.fit_transform(X), columns=X.columns, index=X.index)

        # äº¤å‰éªŒè¯
        if self.config['cv_folds'] > 1:
            self.cross_validate(X, y)

        # åˆ’åˆ†æ•°æ®é›†
        if self.config['test_size'] > 0:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=self.config['test_size'],
                random_state=self.config['random_state'], stratify=y
            )
            self.logger.info(f"è®­ç»ƒé›†: {X_train.shape[0]}, æµ‹è¯•é›†: {X_test.shape[0]}")
        else:
            X_train, y_train = X, y
            X_test = y_test = None
            self.logger.info(f"ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ: {X_train.shape[0]}")

        # è®­ç»ƒ
        train_data = lgb.Dataset(X_train, label=y_train)
        if X_test is not None:
            valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
            valid_sets = [train_data, valid_data]
            valid_names = ['train', 'valid']
            # æœ‰éªŒè¯é›†æ—¶ä½¿ç”¨æ—©åœ
            callbacks = [lgb.early_stopping(50), lgb.log_evaluation(100)]
        else:
            valid_sets = [train_data]
            valid_names = ['train']
            # æ²¡æœ‰éªŒè¯é›†æ—¶ä¸ä½¿ç”¨æ—©åœï¼Œåªç”¨æ—¥å¿—å›è°ƒ
            callbacks = [lgb.log_evaluation(100)]

        self.model = lgb.train(
            self.config['model_params'], train_data,
            valid_sets=valid_sets, valid_names=valid_names,
            num_boost_round=1000,
            callbacks=callbacks
        )

        # ä¼˜åŒ–é˜ˆå€¼
        self.optimize_threshold(X_train, y_train)

        # ä¿å­˜æµ‹è¯•ç»“æœ
        if X_test is not None:
            # æ ¹æ®æ¨¡å‹æ˜¯å¦æœ‰best_iterationå±æ€§æ¥å†³å®šé¢„æµ‹æ–¹å¼
            if hasattr(self.model, 'best_iteration') and self.model.best_iteration is not None:
                y_pred_proba = self.model.predict(X_test, num_iteration=self.model.best_iteration)
            else:
                y_pred_proba = self.model.predict(X_test)
            y_pred = (y_pred_proba > self.optimal_threshold).astype(int)

            self.test_results = {
                'X_test': X_test, 'y_test': y_test,
                'y_pred': y_pred, 'y_pred_proba': y_pred_proba
            }
            self._evaluate_model()

        # ä¿å­˜æ¨¡å‹
        if self.config['save_model']:
            self.save_model()

        return self.test_results if X_test is not None else None

    def _evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        if not self.test_results:
            return

        y_test = self.test_results['y_test']
        y_pred = self.test_results['y_pred']
        y_pred_proba = self.test_results['y_pred_proba']

        self.logger.info("\n=== æ¨¡å‹è¯„ä¼°ç»“æœ ===")
        self.logger.info(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")
        self.logger.info(f"AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")
        self.logger.info(f"ä½¿ç”¨é˜ˆå€¼: {self.optimal_threshold:.4f}")

        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred, target_names=['ä½éšœç¢ç‰©', 'é«˜éšœç¢ç‰©']))

        # ç»˜åˆ¶è¯„ä¼°å›¾è¡¨
        self.plot_confusion_matrix()
        self.plot_roc_curve()
        self.plot_precision_recall_curve()

        # ä¿å­˜é”™è¯¯åˆ†ç±»æ ·æœ¬
        self._save_misclassified_samples(self.train_df_original)

    def plot_roc_curve(self):
        """ç»˜åˆ¶ROCæ›²çº¿"""
        if not self.test_results:
            return

        from sklearn.metrics import roc_curve

        y_test = self.test_results['y_test']
        y_pred_proba = self.test_results['y_pred_proba']

        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        auc_score = roc_auc_score(y_test, y_pred_proba)

        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.4f})')
        plt.plot([0, 1], [0, 1], 'k--', label='Random')
        plt.xlabel('å‡é˜³æ€§ç‡')
        plt.ylabel('çœŸé˜³æ€§ç‡')
        plt.title('ROCæ›²çº¿')
        plt.legend()
        plt.grid(True)

        save_path = os.path.join(self.config['plot_dir'], 'roc_curve.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        self.logger.info(f"ROCæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")
        plt.show()

    def plot_precision_recall_curve(self):
        """ç»˜åˆ¶ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿"""
        if not self.test_results:
            return

        y_test = self.test_results['y_test']
        y_pred_proba = self.test_results['y_pred_proba']

        precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)

        plt.figure(figsize=(8, 6))
        plt.plot(recall, precision, label='PR Curve')
        plt.axvline(x=recall[np.argmin(np.abs(thresholds - self.optimal_threshold))],
                    color='red', linestyle='--', label=f'æœ€ä¼˜é˜ˆå€¼ ({self.optimal_threshold:.3f})')
        plt.xlabel('å¬å›ç‡')
        plt.ylabel('ç²¾ç¡®ç‡')
        plt.title('ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿')
        plt.legend()
        plt.grid(True)

        save_path = os.path.join(self.config['plot_dir'], 'precision_recall_curve.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        self.logger.info(f"PRæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")
        plt.show()

    def plot_confusion_matrix(self, title_suffix=""):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        if not self.test_results:
            self.logger.warning("æ— æµ‹è¯•æ•°æ®ï¼Œè·³è¿‡æ··æ·†çŸ©é˜µç»˜åˆ¶")
            return

        y_test = self.test_results['y_test']
        y_pred = self.test_results['y_pred']
        cm = confusion_matrix(y_test, y_pred)

        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=['ä½éšœç¢ç‰©', 'é«˜éšœç¢ç‰©'],
                    yticklabels=['ä½éšœç¢ç‰©', 'é«˜éšœç¢ç‰©'])
        plt.title(f'æ··æ·†çŸ©é˜µ{title_suffix}')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')

        save_path = os.path.join(self.config['plot_dir'], f'confusion_matrix{title_suffix.replace(" ", "_")}.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        self.logger.info(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {save_path}")
        plt.show()

        return cm

    def plot_feature_importance(self, top_n: int = 20):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§"""
        if self.model is None:
            self.logger.warning("æ¨¡å‹æœªè®­ç»ƒ")
            return

        importance = self.model.feature_importance(importance_type='gain')
        features_to_use = self.selected_features if self.selected_features else self.feature_names

        feature_imp = pd.DataFrame({
            'feature': features_to_use,
            'importance': importance
        }).sort_values('importance', ascending=False)

        plt.figure(figsize=(12, 8))
        sns.barplot(data=feature_imp.head(top_n), x='importance', y='feature')
        plt.title(f'å‰{top_n}ä¸ªé‡è¦ç‰¹å¾')
        plt.xlabel('é‡è¦æ€§')

        save_path = os.path.join(self.config['plot_dir'], f'feature_importance_top{top_n}.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        self.logger.info(f"ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜åˆ°: {save_path}")
        plt.show()

        # ä¿å­˜ç‰¹å¾é‡è¦æ€§æ•°æ®
        importance_file = os.path.join(self.config['results_dir'], 'feature_importance.csv')
        self._save_csv(feature_imp, importance_file)

        return feature_imp

    def _save_misclassified_samples(self, train_df: Optional[pd.DataFrame] = None, suffix: str = ""):
        """ä¿å­˜é”™è¯¯åˆ†ç±»æ ·æœ¬"""
        if not self.test_results:
            self.logger.warning("æ— æµ‹è¯•ç»“æœï¼Œè·³è¿‡é”™è¯¯æ ·æœ¬ä¿å­˜")
            return

        X_test = self.test_results['X_test']
        y_test = self.test_results['y_test']
        y_pred = self.test_results['y_pred']
        y_pred_proba = self.test_results['y_pred_proba']

        misclassified_mask = (y_test != y_pred)
        if not any(misclassified_mask):
            self.logger.info("æ‰€æœ‰æ ·æœ¬é¢„æµ‹æ­£ç¡®ï¼Œæ— é”™è¯¯åˆ†ç±»æ ·æœ¬")
            return

        self.logger.info(f"å‘ç° {sum(misclassified_mask)} ä¸ªé”™è¯¯åˆ†ç±»æ ·æœ¬")

        misclassified_indices = X_test[misclassified_mask].index

        if train_df is not None:
            misclassified_samples = train_df.loc[misclassified_indices].copy()
        else:
            misclassified_samples = X_test[misclassified_mask].copy()

        # æ·»åŠ é¢„æµ‹ä¿¡æ¯
        y_test_mis = y_test[misclassified_mask].reset_index(drop=True)
        y_pred_mis = y_pred[misclassified_mask]
        y_pred_proba_mis = y_pred_proba[misclassified_mask]

        misclassified_samples = misclassified_samples.reset_index(drop=True)
        misclassified_samples['True_Label'] = y_test_mis
        misclassified_samples['Predicted_Label'] = y_pred_mis
        misclassified_samples['Prediction_Probability'] = y_pred_proba_mis
        misclassified_samples['Confidence'] = np.abs(y_pred_proba_mis - 0.5) * 2
        misclassified_samples['Error_Type'] = ['False_Positive' if true_label == 0 else 'False_Negative'
                                               for true_label in y_test_mis]
        misclassified_samples['Used_Threshold'] = self.optimal_threshold

        filename = f'misclassified_samples{suffix}.csv'
        save_path = os.path.join(self.config['misclassified_dir'], filename)

        success = self._save_csv(misclassified_samples, save_path)

        if success:
            false_positive_count = sum(misclassified_samples['Error_Type'] == 'False_Positive')
            false_negative_count = sum(misclassified_samples['Error_Type'] == 'False_Negative')

            self.logger.info(f"é”™è¯¯åˆ†ç±»æ ·æœ¬æ•°: {len(misclassified_samples)}")
            self.logger.info(f"å‡é˜³æ€§æ ·æœ¬æ•°: {false_positive_count}")
            self.logger.info(f"å‡é˜´æ€§æ ·æœ¬æ•°: {false_negative_count}")
            self.logger.info(f"å¹³å‡é¢„æµ‹ç½®ä¿¡åº¦: {misclassified_samples['Confidence'].mean():.4f}")

        return misclassified_samples

    def predict(self, test_filepath: str, output_filepath: Optional[str] = None) -> Optional[pd.DataFrame]:
        """é¢„æµ‹æµ‹è¯•æ•°æ®"""
        if self.model is None:
            self.logger.error("æ¨¡å‹æœªè®­ç»ƒ")
            return None

        self.logger.info(f"é¢„æµ‹æµ‹è¯•æ•°æ®: {test_filepath}")

        # åŠ è½½æµ‹è¯•æ•°æ®
        test_df = self.load_data(test_filepath)
        test_df_processed = self.feature_engineering(test_df)

        # é¢„æµ‹
        features_to_use = self.selected_features if self.selected_features else self.feature_names
        X_test = test_df_processed[features_to_use].fillna(0)

        # åº”ç”¨æ ‡å‡†åŒ–ï¼ˆå¦‚æœè®­ç»ƒæ—¶ä½¿ç”¨äº†ï¼‰
        if self.scaler is not None:
            X_test = pd.DataFrame(self.scaler.transform(X_test), columns=X_test.columns, index=X_test.index)

        # æ ¹æ®æ¨¡å‹æ˜¯å¦æœ‰best_iterationå±æ€§æ¥å†³å®šé¢„æµ‹æ–¹å¼
        if hasattr(self.model, 'best_iteration') and self.model.best_iteration is not None:
            y_pred_proba = self.model.predict(X_test, num_iteration=self.model.best_iteration)
        else:
            y_pred_proba = self.model.predict(X_test)
        y_pred = (y_pred_proba > self.optimal_threshold).astype(int)

        # åˆ›å»ºç»“æœ
        results = test_df.copy()
        if 'HeightLabel' in test_df.columns:
            results['True_Label'] = test_df['HeightLabel']

        results['Predicted_HeightLabel'] = y_pred
        results['Prediction_Probability'] = y_pred_proba
        results['Confidence'] = np.abs(y_pred_proba - 0.5) * 2
        results['Used_Threshold'] = self.optimal_threshold

        if 'HeightLabel' in test_df.columns:
            results['Prediction_Correct'] = (results['True_Label'] == results['Predicted_HeightLabel'])
            results['Error_Type'] = results.apply(
                lambda row: 'Correct' if row['Prediction_Correct'] else
                ('False_Positive' if row['True_Label'] == 0 else 'False_Negative'), axis=1
            )

        # ä¿å­˜ç»“æœ
        if output_filepath is None:
            filename = f"prediction_results_{os.path.splitext(os.path.basename(test_filepath))[0]}.csv"
            output_filepath = os.path.join(self.config['results_dir'], filename)

        self._save_csv(results, output_filepath)

        # ç»Ÿè®¡ä¿¡æ¯
        self.logger.info(f"\né¢„æµ‹ç»Ÿè®¡:")
        self.logger.info(f"é¢„æµ‹ä¸ºé«˜éšœç¢ç‰©æ¯”ä¾‹: {y_pred.mean():.2%}")
        self.logger.info(f"å¹³å‡é¢„æµ‹æ¦‚ç‡: {y_pred_proba.mean():.4f}")
        self.logger.info(f"ä½¿ç”¨é˜ˆå€¼: {self.optimal_threshold:.4f}")

        # å¦‚æœæœ‰çœŸå®æ ‡ç­¾ï¼Œè¿›è¡Œè¯„ä¼°
        if 'HeightLabel' in test_df.columns:
            y_true = test_df['HeightLabel'].values
            accuracy = accuracy_score(y_true, y_pred)
            auc = roc_auc_score(y_true, y_pred_proba)

            self.logger.info(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.4f}")
            self.logger.info(f"æµ‹è¯•é›†AUC: {auc:.4f}")

            # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
            test_name = os.path.splitext(os.path.basename(test_filepath))[0]
            self.test_results = {
                'y_test': y_true, 'y_pred': y_pred, 'y_pred_proba': y_pred_proba
            }
            self.plot_confusion_matrix(f"_{test_name}")

            # ä¿å­˜æµ‹è¯•é›†é”™è¯¯æ ·æœ¬
            self.logger.info(f"\nä¿å­˜é”™è¯¯åˆ†ç±»æ ·æœ¬")
            self._save_test_misclassified_samples(test_df, y_true, y_pred, y_pred_proba, test_filepath)

        return results

    def _save_test_misclassified_samples(self, test_df: pd.DataFrame, y_true: np.ndarray,
                                         y_pred: np.ndarray, y_pred_proba: np.ndarray,
                                         test_filepath: str):
        """ä¿å­˜æµ‹è¯•é›†ä¸­çš„é”™è¯¯åˆ†ç±»æ ·æœ¬"""
        misclassified_mask = (y_true != y_pred)
        if not any(misclassified_mask):
            self.logger.info("æµ‹è¯•é›†æ‰€æœ‰æ ·æœ¬é¢„æµ‹æ­£ç¡®ï¼Œæ— é”™è¯¯åˆ†ç±»æ ·æœ¬")
            return

        self.logger.info(f"æµ‹è¯•é›†å‘ç° {sum(misclassified_mask)} ä¸ªé”™è¯¯åˆ†ç±»æ ·æœ¬")

        misclassified_samples = test_df[misclassified_mask].copy().reset_index(drop=True)

        # æ·»åŠ é¢„æµ‹ä¿¡æ¯
        y_true_mis = y_true[misclassified_mask]
        y_pred_mis = y_pred[misclassified_mask]
        y_pred_proba_mis = y_pred_proba[misclassified_mask]

        misclassified_samples['True_Label'] = y_true_mis
        misclassified_samples['Predicted_Label'] = y_pred_mis
        misclassified_samples['Prediction_Probability'] = y_pred_proba_mis
        misclassified_samples['Confidence'] = np.abs(y_pred_proba_mis - 0.5) * 2
        misclassified_samples['Error_Type'] = ['False_Positive' if true_label == 0 else 'False_Negative'
                                               for true_label in y_true_mis]
        misclassified_samples['Used_Threshold'] = self.optimal_threshold

        # ç”Ÿæˆæ–‡ä»¶å
        test_name = os.path.splitext(os.path.basename(test_filepath))[0]
        filename = f'misclassified_samples_test_{test_name}.csv'
        save_path = os.path.join(self.config['misclassified_dir'], filename)

        success = self._save_csv(misclassified_samples, save_path)

        if success:
            false_positive_count = sum(misclassified_samples['Error_Type'] == 'False_Positive')
            false_negative_count = sum(misclassified_samples['Error_Type'] == 'False_Negative')

            self.logger.info(f"æµ‹è¯•é›†é”™è¯¯åˆ†ç±»æ ·æœ¬æ•°: {len(misclassified_samples)}")
            self.logger.info(f"æµ‹è¯•é›†å‡é˜³æ€§æ ·æœ¬æ•°: {false_positive_count}")
            self.logger.info(f"æµ‹è¯•é›†å‡é˜´æ€§æ ·æœ¬æ•°: {false_negative_count}")

        return misclassified_samples

    def save_model(self, filepath: Optional[str] = None) -> str:
        """ä¿å­˜æ¨¡å‹å’Œç›¸å…³ä¿¡æ¯"""
        if self.model is None:
            self.logger.error("æ¨¡å‹æœªè®­ç»ƒ")
            return ""

        if filepath is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = os.path.join(self.config['model_dir'], f'obstacle_classifier_{timestamp}.joblib')

        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'selected_features': self.selected_features,
            'optimal_threshold': self.optimal_threshold,
            'cv_scores': self.cv_scores,
            'config': self.config,
            'timestamp': datetime.now().isoformat()
        }

        joblib.dump(model_data, filepath)
        self.logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")

        # ä¿å­˜æ¨¡å‹ä¿¡æ¯æ‘˜è¦
        summary_file = filepath.replace('.joblib', '_summary.json')
        summary = {
            'timestamp': model_data['timestamp'],
            'optimal_threshold': self.optimal_threshold,
            'cv_scores': self.cv_scores,
            'feature_count': len(self.feature_names),
            'selected_feature_count': len(self.selected_features) if self.selected_features else len(
                self.feature_names),
            'config': self.config
        }

        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        # ä¿å­˜ç‰¹å¾åˆ—è¡¨
        feature_file = filepath.replace('.joblib', '_features.txt')
        with open(feature_file, 'w', encoding='utf-8') as f:
            f.write("æ¨¡å‹ç‰¹å¾åˆ—è¡¨:\n")
            features_to_use = self.selected_features if self.selected_features else self.feature_names
            for i, feature in enumerate(features_to_use, 1):
                f.write(f"{i}. {feature}\n")

        return filepath

    def load_model(self, filepath: str) -> bool:
        """åŠ è½½æ¨¡å‹"""
        try:
            model_data = joblib.load(filepath)
            self.model = model_data['model']
            self.scaler = model_data.get('scaler')
            self.feature_names = model_data['feature_names']
            self.selected_features = model_data.get('selected_features', [])
            self.optimal_threshold = model_data.get('optimal_threshold', 0.5)
            self.cv_scores = model_data.get('cv_scores', {})

            if 'config' in model_data:
                self.config.update(model_data['config'])

            self.logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {filepath}")
            self.logger.info(f"è®­ç»ƒæ—¶é—´: {model_data.get('timestamp', 'æœªçŸ¥')}")
            self.logger.info(f"ç‰¹å¾æ•°é‡: {len(self.feature_names)}")
            self.logger.info(f"æœ€ä¼˜é˜ˆå€¼: {self.optimal_threshold}")
            if self.cv_scores:
                self.logger.info(f"äº¤å‰éªŒè¯AUC: {self.cv_scores.get('auc', 'N/A')}")
            return True
        except Exception as e:
            self.logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False

    def generate_report(self, output_path: Optional[str] = None) -> str:
        """ç”Ÿæˆæ¨¡å‹æŠ¥å‘Š"""
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = os.path.join(self.config['results_dir'], f'model_report_{timestamp}.md')

        report_content = f"""# éšœç¢ç‰©é«˜åº¦åˆ†ç±»æ¨¡å‹æŠ¥å‘Š

## æ¨¡å‹åŸºæœ¬ä¿¡æ¯
- ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- æ¨¡å‹ç±»å‹: LightGBM äºŒåˆ†ç±»å™¨
- æœ€ä¼˜åˆ†ç±»é˜ˆå€¼: {self.optimal_threshold:.4f}

## ç‰¹å¾å·¥ç¨‹
- æ€»ç‰¹å¾æ•°: {len(self.feature_names)}
- é€‰æ‹©ç‰¹å¾æ•°: {len(self.selected_features) if self.selected_features else len(self.feature_names)}
- ç‰¹å¾é€‰æ‹©: {'æ˜¯' if self.config['feature_selection'] else 'å¦'}

## æ¨¡å‹é…ç½®
```json
{json.dumps(self.config['model_params'], indent=2)}
```

## äº¤å‰éªŒè¯ç»“æœ
"""
        if self.cv_scores:
            for metric, score in self.cv_scores.items():
                report_content += f"- {metric.upper()}: {score:.4f}\n"
        else:
            report_content += "- æœªè¿›è¡Œäº¤å‰éªŒè¯\n"

        report_content += f"""
## æµ‹è¯•ç»“æœ
"""
        if self.test_results:
            y_test = self.test_results['y_test']
            y_pred = self.test_results['y_pred']
            y_pred_proba = self.test_results['y_pred_proba']

            report_content += f"""- æµ‹è¯•æ ·æœ¬æ•°: {len(y_test)}
- å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}
- AUC: {roc_auc_score(y_test, y_pred_proba):.4f}
- é”™è¯¯æ ·æœ¬æ•°: {sum(y_test != y_pred)}
"""

        report_content += f"""
## è¾“å‡ºæ–‡ä»¶ä½ç½®
- æ¨¡å‹æ–‡ä»¶: {self.config['model_dir']}
- å¯è§†åŒ–å›¾è¡¨: {self.config['plot_dir']}
- é¢„æµ‹ç»“æœ: {self.config['results_dir']}
- é”™è¯¯æ ·æœ¬: {self.config['misclassified_dir']}
- æ—¥å¿—æ–‡ä»¶: {self.config['log_dir']}

## ä½¿ç”¨è¯´æ˜
1. åŠ è½½æ¨¡å‹: `classifier.load_model(model_path)`
2. é¢„æµ‹æ–°æ•°æ®: `classifier.predict(test_file_path)`
3. æŸ¥çœ‹é”™è¯¯æ ·æœ¬: æ£€æŸ¥ `{self.config['misclassified_dir']}` ç›®å½•
"""

        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_content)

        self.logger.info(f"æ¨¡å‹æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        return output_path


def quick_run(train_file: str, test_file: Optional[str] = None, config: Optional[Dict] = None):
    """å¿«é€Ÿè¿è¡Œå‡½æ•° - æ”¹è¿›ç‰ˆ"""
    print("=== éšœç¢ç‰©é«˜åº¦åˆ†ç±»å™¨ - æ”¹è¿›ç‰ˆå¿«é€Ÿè¿è¡Œ ===\n")

    # åˆ›å»ºåˆ†ç±»å™¨
    classifier = ObstacleHeightClassifier(config)

    try:
        # 1. è®­ç»ƒé˜¶æ®µ
        print("1. æ•°æ®åŠ è½½å’Œç‰¹å¾å·¥ç¨‹")
        train_df = classifier.load_data(train_file)
        train_df_processed = classifier.feature_engineering(train_df)

        print("\n2. æ¨¡å‹è®­ç»ƒ")
        classifier.train(train_df_processed)

        # 2. ç‰¹å¾é‡è¦æ€§åˆ†æ
        print("\n3. ç‰¹å¾é‡è¦æ€§åˆ†æ")
        classifier.plot_feature_importance()

        # 3. é¢„æµ‹é˜¶æ®µ
        if test_file and os.path.exists(test_file):
            print("\n4. é¢„æµ‹é˜¶æ®µ")
            results = classifier.predict(test_file)
        else:
            print("\n4. è·³è¿‡é¢„æµ‹é˜¶æ®µï¼ˆæ— æµ‹è¯•æ–‡ä»¶ï¼‰")
            results = None

        # 4. ç”ŸæˆæŠ¥å‘Š
        print("\n5. ç”Ÿæˆæ¨¡å‹æŠ¥å‘Š")
        report_path = classifier.generate_report()

        print(f"\n=== è¿è¡Œå®Œæˆ ===")
        print(f"è¯¦ç»†æ—¥å¿—è¯·æŸ¥çœ‹: {classifier.config['log_dir']}")
        print(f"å®Œæ•´æŠ¥å‘Šè¯·æŸ¥çœ‹: {report_path}")

        return classifier

    except Exception as e:
        classifier.logger.error(f"è¿è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    # ==================== é…ç½®åŒºåŸŸ ====================

    # æ–‡ä»¶è·¯å¾„é…ç½®
    TRAIN_FILE = r'D:\PythonProject\data\processed_data\train_group1.csv'
    TEST_FILE = r'D:\PythonProject\data\processed_data\train_group2.csv'

    # æ”¹è¿›çš„é…ç½®
    OUTPUT_CONFIG = {
        # è·¯å¾„é…ç½®
        'model_dir': r'D:\PythonProject\model\saved_model',
        'plot_dir': r'D:\PythonProject\results\visualization_results',
        'results_dir': r'D:\PythonProject\results\prediction_results',
        'misclassified_dir': r'D:\PythonProject\results\misclassified_results',
        'log_dir': r'D:\PythonProject\results\logs',
        'auto_create_dirs': True,
        'encoding': 'utf-8-sig',

        # è®­ç»ƒé…ç½®
        'test_size': 0.0,  # ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ
        'random_state': 42,
        'save_model': True,
        'cv_folds': 5,  # 5æŠ˜äº¤å‰éªŒè¯
        'threshold_optimization': True,  # ä¼˜åŒ–åˆ†ç±»é˜ˆå€¼
        'feature_selection': True,  # ç‰¹å¾é€‰æ‹©
        'normalize_features': False,  # æ˜¯å¦æ ‡å‡†åŒ–ç‰¹å¾

        # æ¨¡å‹å‚æ•°ä¼˜åŒ–
        'model_params': {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 63,  # å¢åŠ å¤æ‚åº¦
            'learning_rate': 0.03,  # é™ä½å­¦ä¹ ç‡
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'min_data_in_leaf': 20,  # é˜²è¿‡æ‹Ÿåˆ
            'lambda_l1': 0.1,  # L1æ­£åˆ™åŒ–
            'lambda_l2': 0.1,  # L2æ­£åˆ™åŒ–
            'verbose': -1,
            'is_unbalance': True,
            'random_state': 42
        }
    }

    # ==================== è¿è¡ŒåŒºåŸŸ ====================

    # è¿è¡Œæ”¹è¿›ç‰ˆåˆ†ç±»å™¨
    classifier = quick_run(TRAIN_FILE, TEST_FILE, OUTPUT_CONFIG)

    print(f"\nğŸ“ è¾“å‡ºç›®å½•:")
    print(f"â”œâ”€â”€ æ¨¡å‹ä¿å­˜: {OUTPUT_CONFIG['model_dir']}")
    print(f"â”œâ”€â”€ å›¾ç‰‡ä¿å­˜: {OUTPUT_CONFIG['plot_dir']}")
    print(f"â”œâ”€â”€ ç»“æœä¿å­˜: {OUTPUT_CONFIG['results_dir']}")
    print(f"â”œâ”€â”€ é”™è¯¯æ ·æœ¬: {OUTPUT_CONFIG['misclassified_dir']}")
    print(f"â””â”€â”€ æ—¥å¿—æ–‡ä»¶: {OUTPUT_CONFIG['log_dir']}")

    # æ£€æŸ¥å…³é”®æ–‡ä»¶
    print(f"\nğŸ” å…³é”®æ–‡ä»¶æ£€æŸ¥:")

    # æ£€æŸ¥é”™è¯¯æ ·æœ¬æ–‡ä»¶
    misclassified_dir = OUTPUT_CONFIG['misclassified_dir']
    if os.path.exists(misclassified_dir):
        files = [f for f in os.listdir(misclassified_dir) if f.endswith('.csv')]
        if files:
            print(f"âœ… é”™è¯¯æ ·æœ¬æ–‡ä»¶ ({len(files)} ä¸ª):")
            for file in files:
                file_path = os.path.join(misclassified_dir, file)
                file_size = os.path.getsize(file_path)
                print(f"   â””â”€â”€ {file} ({file_size} bytes)")
        else:
            print(f"âš ï¸  é”™è¯¯æ ·æœ¬ç›®å½•ä¸ºç©º - å¯èƒ½æ¨¡å‹é¢„æµ‹100%æ­£ç¡®")

    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_dir = OUTPUT_CONFIG['model_dir']
    if os.path.exists(model_dir):
        model_files = [f for f in os.listdir(model_dir) if f.endswith('.joblib')]
        if model_files:
            latest_model = max(model_files)
            print(f"âœ… æœ€æ–°æ¨¡å‹: {latest_model}")

    print(f"\nğŸ¯ æœ€ä¼˜åˆ†ç±»é˜ˆå€¼: {classifier.optimal_threshold:.4f}")
    if classifier.cv_scores:
        print(f"ğŸ“Š äº¤å‰éªŒè¯AUC: {classifier.cv_scores.get('auc', 'N/A'):.4f}")