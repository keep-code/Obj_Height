import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import joblib
import os
from datetime import datetime

warnings.filterwarnings('ignore')

# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class ObstacleHeightClassifier:
    """éšœç¢ç‰©é«˜åº¦åˆ†ç±»å™¨ - æ”¹è¿›ç‰ˆæœ¬"""

    def __init__(self, config=None):
        """
        åˆå§‹åŒ–åˆ†ç±»å™¨

        Parameters:
        -----------
        config : dict, optional
            é…ç½®å­—å…¸ï¼ŒåŒ…å«è·¯å¾„è®¾ç½®ç­‰å‚æ•°
        """
        # é»˜è®¤é…ç½®
        self.config = {
            'model_dir': './models',
            'plot_dir': './plots',
            'results_dir': './results',
            'misclassified_dir': './misclassified',
            'auto_create_dirs': True,
            'encoding': 'utf-8-sig',
            'test_size': 0.3,
            'random_state': 42,
            'save_model': True
        }

        # æ›´æ–°ç”¨æˆ·é…ç½®
        if config:
            self.config.update(config)

        # æ¨¡å‹ç›¸å…³å±æ€§
        self.model = None
        self.feature_names = []
        self.test_results = {}

        # åˆ›å»ºè¾“å‡ºç›®å½•
        if self.config['auto_create_dirs']:
            self._create_directories()

    def _create_directories(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        for dir_path in [self.config['model_dir'], self.config['plot_dir'],
                         self.config['results_dir'], self.config['misclassified_dir']]:
            os.makedirs(dir_path, exist_ok=True)
            print(f"ç¡®ä¿ç›®å½•å­˜åœ¨: {dir_path}")

    def _save_csv(self, df, filepath, encoding=None):
        """ä¿å­˜CSVæ–‡ä»¶ï¼Œè§£å†³ä¸­æ–‡ä¹±ç é—®é¢˜"""
        encoding = encoding or self.config['encoding']
        try:
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            df.to_csv(filepath, index=False, encoding=encoding)
            print(f"æ–‡ä»¶å·²ä¿å­˜åˆ°: {filepath}")
            print(f"æ–‡ä»¶å¤§å°: {os.path.getsize(filepath)} bytes")
            return True
        except Exception as e:
            print(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            try:
                df.to_csv(filepath, index=False, encoding='gbk')
                print(f"ä½¿ç”¨GBKç¼–ç ä¿å­˜åˆ°: {filepath}")
                return True
            except Exception as e2:
                try:
                    df.to_csv(filepath, index=False, encoding='utf-8')
                    print(f"ä½¿ç”¨UTF-8ç¼–ç ä¿å­˜åˆ°: {filepath}")
                    return True
                except Exception as e3:
                    print(f"æ‰€æœ‰ç¼–ç å°è¯•å¤±è´¥: {e3}")
                    return False

    def load_data(self, filepath):
        """åŠ è½½æ•°æ®"""
        print(f"åŠ è½½æ•°æ®: {filepath}")
        try:
            df = pd.read_csv(filepath, encoding='utf-8-sig')
        except:
            try:
                df = pd.read_csv(filepath, encoding='gbk')
            except:
                df = pd.read_csv(filepath)

        print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
        if 'HeightLabel' in df.columns:
            print(f"HeightLabelåˆ†å¸ƒ: {df['HeightLabel'].value_counts().to_dict()}")
            print(f"é«˜éšœç¢ç‰©æ¯”ä¾‹: {df['HeightLabel'].mean():.2%}")

        return df

    def feature_engineering(self, df):
        """ç‰¹å¾å·¥ç¨‹ - æ”¹è¿›ç‰ˆæœ¬"""
        print("è¿›è¡Œç‰¹å¾å·¥ç¨‹...")
        df_processed = df.copy()

        # ============ å…³é”®æ”¹è¿›ï¼šæ›´ä¸¥æ ¼çš„ç‰¹å¾è¿‡æ»¤ ============
        # æ˜ç¡®æ’é™¤æ‰€æœ‰æè¿°æ€§å’Œæ— å…³ç‰¹å¾
        exclude_cols = [
            # ç›®æ ‡å˜é‡
            'HeightLabel',
            # æè¿°æ€§ä¿¡æ¯ï¼ˆä¸éšœç¢ç‰©é«˜ä½æ— å…³ï¼‰
            'Train_OD_Project',  # é¡¹ç›®åç§°
            'ObjName',  # éšœç¢ç‰©åç§°
            'Direction',  # æ–¹å‘
            'Obj_ID',  # éšœç¢ç‰©ID - æ–°å¢æ’é™¤
            'CurCyc',  # å½“å‰å‘¨æœŸ - æ–°å¢æ’é™¤
            'TxSensID',  # ä¼ æ„Ÿå™¨ID - æ–°å¢æ’é™¤
        ]

        print(f"æ’é™¤çš„ç‰¹å¾åˆ—: {exclude_cols}")

        # éªŒè¯æ’é™¤çš„åˆ—æ˜¯å¦å­˜åœ¨äºæ•°æ®ä¸­
        existing_exclude_cols = [col for col in exclude_cols if col in df_processed.columns]
        print(f"å®é™…æ’é™¤çš„åˆ—: {existing_exclude_cols}")

        # è·å–åŸºç¡€ç‰¹å¾åˆ—ï¼ˆç”¨äºç‰¹å¾å·¥ç¨‹ï¼‰
        base_features = [col for col in df_processed.columns if col not in exclude_cols]
        print(f"åŸºç¡€ç‰¹å¾åˆ—: {base_features}")

        # åˆ›å»ºæ–°ç‰¹å¾
        feature_ops = {
            # æ¯”å€¼ç‰¹å¾
            'DeEcho_Ratio': lambda x: x['PosDeDis1'] / (x['PosDeDis2'] + 1e-8),
            'CeEcho_Ratio': lambda x: x['PosCeDis1'] / (x['PosCeDis2'] + 1e-8),
            'DeAmp_Ratio': lambda x: x['PosDeAmp1'] / (x['PosDeAmp2'] + 1e-8),
            'CeAmp_Ratio': lambda x: x['PosCeAmp1'] / (x['PosCeAmp2'] + 1e-8),

            # æ€»å’Œç‰¹å¾
            'Total_DeEcho': lambda x: x['PosDeDis1'] + x['PosDeDis2'],
            'Total_CeEcho': lambda x: x['PosCeDis1'] + x['PosCeDis2'],
            'Total_DeAmp': lambda x: x['PosDeAmp1'] + x['PosDeAmp2'],
            'Total_CeAmp': lambda x: x['PosCeAmp1'] + x['PosCeAmp2'],

            # å·®å€¼ç‰¹å¾
            'DeDis_Diff': lambda x: x['PosDeDis1'] - x['PosDeDis2'],
            'CeDis_Diff': lambda x: x['PosCeDis1'] - x['PosCeDis2'],
            'DeAmp_Diff': lambda x: x['PosDeAmp1'] - x['PosDeAmp2'],
            'CeAmp_Diff': lambda x: x['PosCeAmp1'] - x['PosCeAmp2'],

            # æ–°å¢ç‰¹å¾
            'Avg_Echo_Strength': lambda x: (x['AvgDeEchoHigh_SameTx'] + x['AvgCeEchoHigh_SameTxRx']) / 2,
            'Distance_Ratio': lambda x: x['TrainObjDist'] / (x['AngleDist'] + 1e-8),
            'Echo_Strength_Ratio': lambda x: x['AvgDeEchoHigh_SameTx'] / (x['AvgCeEchoHigh_SameTxRx'] + 1e-8),
            'Odo_Stability': lambda x: x['OdoDiffObjDis'] / (x['OdoDiffDeDis'] + 1e-8),
        }

        # åº”ç”¨ç‰¹å¾å·¥ç¨‹
        for feature_name, operation in feature_ops.items():
            try:
                df_processed[feature_name] = operation(df_processed)
                print(f"åˆ›å»ºç‰¹å¾: {feature_name}")
            except Exception as e:
                print(f"ç‰¹å¾ {feature_name} åˆ›å»ºå¤±è´¥: {e}")

        # æ›´æ–°ç‰¹å¾åˆ—è¡¨ - ä¸¥æ ¼æ’é™¤æ‰€æœ‰æ— å…³ç‰¹å¾
        self.feature_names = [col for col in df_processed.columns if col not in exclude_cols]
        print(f"ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œå…± {len(self.feature_names)} ä¸ªç‰¹å¾")
        print(f"æœ€ç»ˆç‰¹å¾åˆ—è¡¨: {self.feature_names}")

        # ============ æ–°å¢ï¼šç‰¹å¾è´¨é‡æ£€æŸ¥ ============
        self._check_feature_quality(df_processed)

        return df_processed

    def _check_feature_quality(self, df):
        """æ£€æŸ¥ç‰¹å¾è´¨é‡"""
        print("\n=== ç‰¹å¾è´¨é‡æ£€æŸ¥ ===")

        # æ£€æŸ¥ç¼ºå¤±å€¼
        missing_stats = df[self.feature_names].isnull().sum()
        if missing_stats.sum() > 0:
            print("å‘ç°ç¼ºå¤±å€¼:")
            print(missing_stats[missing_stats > 0])
        else:
            print("âœ“ æ— ç¼ºå¤±å€¼")

        # æ£€æŸ¥å¸¸æ•°ç‰¹å¾
        constant_features = []
        for feature in self.feature_names:
            if df[feature].nunique() <= 1:
                constant_features.append(feature)

        if constant_features:
            print(f"å‘ç°å¸¸æ•°ç‰¹å¾: {constant_features}")
            # ç§»é™¤å¸¸æ•°ç‰¹å¾
            self.feature_names = [f for f in self.feature_names if f not in constant_features]
            print(f"ç§»é™¤å¸¸æ•°ç‰¹å¾åå‰©ä½™: {len(self.feature_names)} ä¸ªç‰¹å¾")
        else:
            print("âœ“ æ— å¸¸æ•°ç‰¹å¾")

        # æ£€æŸ¥æ— ç©·å€¼
        inf_features = []
        for feature in self.feature_names:
            if np.isinf(df[feature]).any():
                inf_features.append(feature)

        if inf_features:
            print(f"å‘ç°æ— ç©·å€¼ç‰¹å¾: {inf_features}")
            # æ›¿æ¢æ— ç©·å€¼
            for feature in inf_features:
                df[feature] = df[feature].replace([np.inf, -np.inf], np.nan)
                df[feature] = df[feature].fillna(df[feature].median())
            print("å·²æ›¿æ¢æ— ç©·å€¼ä¸ºä¸­ä½æ•°")
        else:
            print("âœ“ æ— æ— ç©·å€¼")

    def train(self, train_df):
        """è®­ç»ƒæ¨¡å‹"""
        print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")

        # ä¿å­˜åŸå§‹è®­ç»ƒæ•°æ®ç”¨äºé”™è¯¯æ ·æœ¬åˆ†æ
        self.train_df_original = train_df.copy()

        # å‡†å¤‡æ•°æ®
        X = train_df[self.feature_names]
        y = train_df['HeightLabel']

        print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: X{X.shape}, y{y.shape}")
        print(f"ç‰¹å¾åˆ—è¡¨: {self.feature_names[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª

        # åˆ’åˆ†æ•°æ®é›†
        if self.config['test_size'] > 0:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=self.config['test_size'],
                random_state=self.config['random_state'], stratify=y
            )
            print(f"è®­ç»ƒé›†: {X_train.shape[0]}, æµ‹è¯•é›†: {X_test.shape[0]}")
        else:
            X_train, y_train = X, y
            X_test = y_test = None
            print(f"ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ: {X_train.shape[0]}")

        # LightGBMå‚æ•° - ä¼˜åŒ–åçš„å‚æ•°
        params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'random_state': self.config['random_state'],
            'is_unbalance': True,
            'min_child_samples': 20,  # é˜²æ­¢è¿‡æ‹Ÿåˆ
            'min_child_weight': 0.001,  # é˜²æ­¢è¿‡æ‹Ÿåˆ
            'subsample_for_bin': 200000,  # æé«˜è®­ç»ƒé€Ÿåº¦
            'reg_alpha': 0.1,  # L1æ­£åˆ™åŒ–
            'reg_lambda': 0.1,  # L2æ­£åˆ™åŒ–
        }

        # è®­ç»ƒ
        train_data = lgb.Dataset(X_train, label=y_train)
        if X_test is not None:
            valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
            valid_sets = [train_data, valid_data]
            valid_names = ['train', 'valid']
        else:
            valid_sets = [train_data]
            valid_names = ['train']

        self.model = lgb.train(
            params, train_data, valid_sets=valid_sets, valid_names=valid_names,
            num_boost_round=1000, callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]
        )

        # ä¿å­˜æµ‹è¯•ç»“æœ
        if X_test is not None:
            y_pred_proba = self.model.predict(X_test, num_iteration=self.model.best_iteration)
            y_pred = (y_pred_proba > 0.5).astype(int)

            self.test_results = {
                'X_test': X_test, 'y_test': y_test,
                'y_pred': y_pred, 'y_pred_proba': y_pred_proba
            }

            # è¯„ä¼°
            self._evaluate_model()

        # ä¿å­˜æ¨¡å‹
        if self.config['save_model']:
            self.save_model()

        return self.test_results if X_test is not None else None

    def _evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        if not self.test_results:
            return

        y_test = self.test_results['y_test']
        y_pred = self.test_results['y_pred']
        y_pred_proba = self.test_results['y_pred_proba']

        print("\n=== æ¨¡å‹è¯„ä¼°ç»“æœ ===")
        print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")
        print(f"AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred, target_names=['ä½éšœç¢ç‰©', 'é«˜éšœç¢ç‰©']))

        # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        self.plot_confusion_matrix()

        # ä¿å­˜é”™è¯¯åˆ†ç±»æ ·æœ¬
        self._save_misclassified_samples(self.train_df_original)

    def plot_confusion_matrix(self, title_suffix=""):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        if not self.test_results:
            print("æ— æµ‹è¯•æ•°æ®ï¼Œè·³è¿‡æ··æ·†çŸ©é˜µç»˜åˆ¶")
            return

        y_test = self.test_results['y_test']
        y_pred = self.test_results['y_pred']
        cm = confusion_matrix(y_test, y_pred)

        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=['ä½éšœç¢ç‰©', 'é«˜éšœç¢ç‰©'],
                    yticklabels=['ä½éšœç¢ç‰©', 'é«˜éšœç¢ç‰©'])
        plt.title(f'æ··æ·†çŸ©é˜µ{title_suffix}')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')

        # ä¿å­˜å›¾ç‰‡
        save_path = os.path.join(self.config['plot_dir'], f'confusion_matrix{title_suffix.replace(" ", "_")}.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {save_path}")
        plt.show()

        return cm

    def plot_feature_importance(self, top_n=20):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§"""
        if self.model is None:
            print("æ¨¡å‹æœªè®­ç»ƒ")
            return

        importance = self.model.feature_importance(importance_type='gain')
        feature_imp = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)

        plt.figure(figsize=(10, 8))
        sns.barplot(data=feature_imp.head(top_n), x='importance', y='feature')
        plt.title(f'å‰{top_n}ä¸ªé‡è¦ç‰¹å¾')
        plt.xlabel('é‡è¦æ€§')

        save_path = os.path.join(self.config['plot_dir'], f'feature_importance_top{top_n}.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜åˆ°: {save_path}")
        plt.show()

        return feature_imp

    def _save_misclassified_samples(self, train_df=None, suffix=""):
        """ä¿å­˜é”™è¯¯åˆ†ç±»æ ·æœ¬"""
        if not self.test_results:
            print("æ— æµ‹è¯•ç»“æœï¼Œè·³è¿‡é”™è¯¯æ ·æœ¬ä¿å­˜")
            return

        X_test = self.test_results['X_test']
        y_test = self.test_results['y_test']
        y_pred = self.test_results['y_pred']
        y_pred_proba = self.test_results['y_pred_proba']

        # æ‰¾å‡ºé”™è¯¯æ ·æœ¬
        misclassified_mask = (y_test != y_pred)
        if not any(misclassified_mask):
            print("æ‰€æœ‰æ ·æœ¬é¢„æµ‹æ­£ç¡®ï¼Œæ— é”™è¯¯åˆ†ç±»æ ·æœ¬")
            return

        print(f"å‘ç° {sum(misclassified_mask)} ä¸ªé”™è¯¯åˆ†ç±»æ ·æœ¬")

        # è·å–é”™è¯¯æ ·æœ¬çš„ç´¢å¼•
        misclassified_indices = X_test[misclassified_mask].index

        # å¦‚æœæœ‰åŸå§‹è®­ç»ƒæ•°æ®ï¼Œä»ä¸­æå–å®Œæ•´ä¿¡æ¯
        if train_df is not None:
            misclassified_samples = train_df.loc[misclassified_indices].copy()
        else:
            misclassified_samples = X_test[misclassified_mask].copy()

        # æ·»åŠ é¢„æµ‹ç›¸å…³ä¿¡æ¯
        y_test_mis = y_test[misclassified_mask].reset_index(drop=True)
        y_pred_mis = y_pred[misclassified_mask]
        y_pred_proba_mis = y_pred_proba[misclassified_mask]

        misclassified_samples = misclassified_samples.reset_index(drop=True)
        misclassified_samples['True_Label'] = y_test_mis
        misclassified_samples['Predicted_Label'] = y_pred_mis
        misclassified_samples['Prediction_Probability'] = y_pred_proba_mis
        misclassified_samples['Confidence'] = np.abs(y_pred_proba_mis - 0.5) * 2
        misclassified_samples['Error_Type'] = ['False_Positive' if true_label == 0 else 'False_Negative'
                                               for true_label in y_test_mis]

        # ä¿å­˜åˆ°æŒ‡å®šç›®å½•
        filename = f'misclassified_samples{suffix}.csv'
        save_path = os.path.join(self.config['misclassified_dir'], filename)

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.config['misclassified_dir'], exist_ok=True)
        print(f"ä¿å­˜é”™è¯¯æ ·æœ¬åˆ°: {save_path}")

        # ä¿å­˜æ–‡ä»¶
        success = self._save_csv(misclassified_samples, save_path)

        if success:
            # ç»Ÿè®¡ä¿¡æ¯
            false_positive_count = sum(misclassified_samples['Error_Type'] == 'False_Positive')
            false_negative_count = sum(misclassified_samples['Error_Type'] == 'False_Negative')

            print(f"é”™è¯¯åˆ†ç±»æ ·æœ¬æ•°: {len(misclassified_samples)}")
            print(f"å‡é˜³æ€§æ ·æœ¬æ•°: {false_positive_count}")
            print(f"å‡é˜´æ€§æ ·æœ¬æ•°: {false_negative_count}")
            print(f"å¹³å‡é¢„æµ‹ç½®ä¿¡åº¦: {misclassified_samples['Confidence'].mean():.4f}")
        else:
            print("é”™è¯¯æ ·æœ¬ä¿å­˜å¤±è´¥")

        return misclassified_samples

    def predict(self, test_filepath, output_filepath=None):
        """é¢„æµ‹æµ‹è¯•æ•°æ®"""
        if self.model is None:
            print("æ¨¡å‹æœªè®­ç»ƒ")
            return None

        print(f"é¢„æµ‹æµ‹è¯•æ•°æ®: {test_filepath}")

        # åŠ è½½æµ‹è¯•æ•°æ®
        test_df = self.load_data(test_filepath)
        test_df_processed = self.feature_engineering(test_df)

        # é¢„æµ‹
        X_test = test_df_processed[self.feature_names]
        y_pred_proba = self.model.predict(X_test, num_iteration=self.model.best_iteration)
        y_pred = (y_pred_proba > 0.5).astype(int)

        # åˆ›å»ºç»“æœ
        results = test_df.copy()

        # æ·»åŠ é¢„æµ‹ç»“æœ
        if 'HeightLabel' in test_df.columns:
            results['True_Label'] = test_df['HeightLabel']

        results['Predicted_HeightLabel'] = y_pred
        results['Prediction_Probability'] = y_pred_proba
        results['Confidence'] = np.abs(y_pred_proba - 0.5) * 2

        if 'HeightLabel' in test_df.columns:
            results['Prediction_Correct'] = (results['True_Label'] == results['Predicted_HeightLabel'])
            results['Error_Type'] = results.apply(
                lambda row: 'Correct' if row['Prediction_Correct'] else
                ('False_Positive' if row['True_Label'] == 0 else 'False_Negative'), axis=1
            )

        # ä¿å­˜ç»“æœ
        if output_filepath is None:
            filename = f"prediction_results_{os.path.splitext(os.path.basename(test_filepath))[0]}.csv"
            output_filepath = os.path.join(self.config['results_dir'], filename)

        self._save_csv(results, output_filepath)

        # ç»Ÿè®¡ä¿¡æ¯
        print(f"\né¢„æµ‹ç»Ÿè®¡:")
        print(f"é¢„æµ‹ä¸ºé«˜éšœç¢ç‰©æ¯”ä¾‹: {y_pred.mean():.2%}")
        print(f"å¹³å‡é¢„æµ‹æ¦‚ç‡: {y_pred_proba.mean():.4f}")

        # å¦‚æœæœ‰çœŸå®æ ‡ç­¾ï¼Œè¿›è¡Œè¯„ä¼°
        if 'HeightLabel' in test_df.columns:
            y_true = test_df['HeightLabel'].values
            accuracy = accuracy_score(y_true, y_pred)
            auc = roc_auc_score(y_true, y_pred_proba)

            print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.4f}")
            print(f"æµ‹è¯•é›†AUC: {auc:.4f}")

            # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
            test_name = os.path.splitext(os.path.basename(test_filepath))[0]
            self.test_results = {
                'y_test': y_true, 'y_pred': y_pred, 'y_pred_proba': y_pred_proba
            }
            self.plot_confusion_matrix(f"_{test_name}")

            # ä¿å­˜æµ‹è¯•é›†é”™è¯¯æ ·æœ¬
            print(f"\nä¿å­˜é”™è¯¯åˆ†ç±»æ ·æœ¬")
            self._save_test_misclassified_samples(test_df, y_true, y_pred, y_pred_proba, test_filepath)

        return results

    def _save_test_misclassified_samples(self, test_df, y_true, y_pred, y_pred_proba, test_filepath):
        """ä¿å­˜æµ‹è¯•é›†ä¸­çš„é”™è¯¯åˆ†ç±»æ ·æœ¬"""
        misclassified_mask = (y_true != y_pred)
        if not any(misclassified_mask):
            print("æµ‹è¯•é›†æ‰€æœ‰æ ·æœ¬é¢„æµ‹æ­£ç¡®ï¼Œæ— é”™è¯¯åˆ†ç±»æ ·æœ¬")
            return

        print(f"æµ‹è¯•é›†å‘ç° {sum(misclassified_mask)} ä¸ªé”™è¯¯åˆ†ç±»æ ·æœ¬")

        # è·å–é”™è¯¯æ ·æœ¬
        misclassified_samples = test_df[misclassified_mask].copy().reset_index(drop=True)

        # æ·»åŠ é¢„æµ‹ä¿¡æ¯
        y_true_mis = y_true[misclassified_mask]
        y_pred_mis = y_pred[misclassified_mask]
        y_pred_proba_mis = y_pred_proba[misclassified_mask]

        misclassified_samples['True_Label'] = y_true_mis
        misclassified_samples['Predicted_Label'] = y_pred_mis
        misclassified_samples['Prediction_Probability'] = y_pred_proba_mis
        misclassified_samples['Confidence'] = np.abs(y_pred_proba_mis - 0.5) * 2
        misclassified_samples['Error_Type'] = ['False_Positive' if true_label == 0 else 'False_Negative'
                                               for true_label in y_true_mis]

        # ç”Ÿæˆæ–‡ä»¶å
        test_name = os.path.splitext(os.path.basename(test_filepath))[0]
        filename = f'misclassified_samples_test_{test_name}.csv'
        save_path = os.path.join(self.config['misclassified_dir'], filename)

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.config['misclassified_dir'], exist_ok=True)
        print(f"ä¿å­˜æµ‹è¯•é›†é”™è¯¯æ ·æœ¬åˆ°: {save_path}")

        # ä¿å­˜æ–‡ä»¶
        success = self._save_csv(misclassified_samples, save_path)

        if success:
            # ç»Ÿè®¡ä¿¡æ¯
            false_positive_count = sum(misclassified_samples['Error_Type'] == 'False_Positive')
            false_negative_count = sum(misclassified_samples['Error_Type'] == 'False_Negative')

            print(f"æµ‹è¯•é›†é”™è¯¯åˆ†ç±»æ ·æœ¬æ•°: {len(misclassified_samples)}")
            print(f"æµ‹è¯•é›†å‡é˜³æ€§æ ·æœ¬æ•°: {false_positive_count}")
            print(f"æµ‹è¯•é›†å‡é˜´æ€§æ ·æœ¬æ•°: {false_negative_count}")
        else:
            print("æµ‹è¯•é›†é”™è¯¯æ ·æœ¬ä¿å­˜å¤±è´¥")

        return misclassified_samples

    def save_model(self, filepath=None):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None:
            print("æ¨¡å‹æœªè®­ç»ƒ")
            return

        if filepath is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = os.path.join(self.config['model_dir'], f'obstacle_classifier_{timestamp}.joblib')

        model_data = {
            'model': self.model,
            'feature_names': self.feature_names,
            'config': self.config,
            'timestamp': datetime.now().isoformat()
        }

        joblib.dump(model_data, filepath)
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")

        # ä¿å­˜ç‰¹å¾åˆ—è¡¨
        feature_file = filepath.replace('.joblib', '_features.txt')
        with open(feature_file, 'w', encoding='utf-8') as f:
            f.write("æ¨¡å‹ç‰¹å¾åˆ—è¡¨:\n")
            f.write("=" * 50 + "\n")
            f.write(f"æ€»ç‰¹å¾æ•°: {len(self.feature_names)}\n")
            f.write(f"åˆ›å»ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 50 + "\n\n")

            # åˆ†ç±»æ˜¾ç¤ºç‰¹å¾
            base_features = [f for f in self.feature_names if not any(
                keyword in f for keyword in ['_Ratio', '_Diff', '_Total', 'Avg_', 'Distance_', 'Odo_', 'Echo_'])]
            engineered_features = [f for f in self.feature_names if f not in base_features]

            f.write("åŸºç¡€ç‰¹å¾:\n")
            for i, feature in enumerate(base_features, 1):
                f.write(f"{i:2d}. {feature}\n")

            f.write(f"\nå·¥ç¨‹ç‰¹å¾ ({len(engineered_features)}ä¸ª):\n")
            for i, feature in enumerate(engineered_features, 1):
                f.write(f"{i:2d}. {feature}\n")

        return filepath

    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        try:
            model_data = joblib.load(filepath)
            self.model = model_data['model']
            self.feature_names = model_data['feature_names']
            if 'config' in model_data:
                self.config.update(model_data['config'])

            print(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {filepath}")
            print(f"è®­ç»ƒæ—¶é—´: {model_data.get('timestamp', 'æœªçŸ¥')}")
            print(f"ç‰¹å¾æ•°é‡: {len(self.feature_names)}")
            return True
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False

    def compare_feature_importance_with_original(self):
        """ä¸åŸå§‹ä»£ç çš„ç‰¹å¾é‡è¦æ€§å¯¹æ¯”"""
        if self.model is None:
            print("æ¨¡å‹æœªè®­ç»ƒ")
            return

        importance = self.model.feature_importance(importance_type='gain')
        feature_imp = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)

        # æ£€æŸ¥æ˜¯å¦åŒ…å«äº†åº”è¯¥æ’é™¤çš„ç‰¹å¾
        problematic_features = ['Obj_ID', 'CurCyc', 'TxSensID']
        found_problematic = [f for f in problematic_features if f in self.feature_names]

        if found_problematic:
            print(f"âš ï¸  è­¦å‘Š: å‘ç°åº”è¯¥æ’é™¤çš„ç‰¹å¾: {found_problematic}")
        else:
            print("âœ… å·²æ­£ç¡®æ’é™¤æ‰€æœ‰æè¿°æ€§ç‰¹å¾")

        print(f"\nç‰¹å¾é‡è¦æ€§ Top 10:")
        print(feature_imp.head(10).to_string(index=False))

        return feature_imp


def quick_run(train_file, test_file=None, config=None):
    """å¿«é€Ÿè¿è¡Œå‡½æ•° - æ”¹è¿›ç‰ˆæœ¬"""
    print("=== éšœç¢ç‰©é«˜åº¦åˆ†ç±»å™¨ - æ”¹è¿›ç‰ˆæœ¬ ===\n")

    # åˆ›å»ºåˆ†ç±»å™¨
    classifier = ObstacleHeightClassifier(config)

    # è®­ç»ƒ
    print("1. è®­ç»ƒé˜¶æ®µ")
    train_df = classifier.load_data(train_file)
    train_df_processed = classifier.feature_engineering(train_df)
    classifier.train(train_df_processed)

    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    print("\n2. ç‰¹å¾é‡è¦æ€§åˆ†æ")
    classifier.plot_feature_importance()

    # æ¯”è¾ƒç‰¹å¾é‡è¦æ€§
    print("\n3. ç‰¹å¾è´¨é‡éªŒè¯")
    classifier.compare_feature_importance_with_original()

    # é¢„æµ‹
    if test_file and os.path.exists(test_file):
        print("\n4. é¢„æµ‹é˜¶æ®µ")
        results = classifier.predict(test_file)
    else:
        print("\n4. è·³è¿‡é¢„æµ‹é˜¶æ®µï¼ˆæ— æµ‹è¯•æ–‡ä»¶ï¼‰")

    print("\n=== è¿è¡Œå®Œæˆ ===")
    return classifier


def validate_features_exclusion(df):
    """éªŒè¯ç‰¹å¾æ’é™¤æ˜¯å¦æ­£ç¡®"""
    print("=== ç‰¹å¾æ’é™¤éªŒè¯ ===")

    # åº”è¯¥æ’é™¤çš„ç‰¹å¾
    should_exclude = [
        'HeightLabel', 'Train_OD_Project', 'ObjName', 'Direction',
        'Obj_ID', 'CurCyc', 'TxSensID'
    ]

    # æ£€æŸ¥è¿™äº›ç‰¹å¾æ˜¯å¦å­˜åœ¨äºæ•°æ®ä¸­
    existing_features = df.columns.tolist()
    found_exclude = [col for col in should_exclude if col in existing_features]

    print(f"æ•°æ®ä¸­å­˜åœ¨çš„åº”æ’é™¤ç‰¹å¾: {found_exclude}")
    print(f"æ•°æ®æ€»åˆ—æ•°: {len(existing_features)}")
    print(f"åº”æ’é™¤çš„åˆ—æ•°: {len(found_exclude)}")
    print(f"é¢„æœŸçš„ç‰¹å¾åˆ—æ•°: {len(existing_features) - len(found_exclude)}")

    # æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®ä¸­è¿™äº›ç‰¹å¾çš„æƒ…å†µ
    if found_exclude:
        print(f"\nè¿™äº›ç‰¹å¾çš„æ ·ä¾‹æ•°æ®:")
        for col in found_exclude:
            if col in df.columns:
                unique_vals = df[col].unique()[:5]  # åªæ˜¾ç¤ºå‰5ä¸ªå”¯ä¸€å€¼
                print(f"  {col}: {unique_vals} (å…±{df[col].nunique()}ä¸ªå”¯ä¸€å€¼)")

    return found_exclude


if __name__ == "__main__":
    # ==================== é…ç½®åŒºåŸŸ ====================

    # æ–‡ä»¶è·¯å¾„é…ç½®
    TRAIN_FILE = r'D:\PythonProject\data\processed_data\merged_train_data_fixed.csv'
    TEST_FILE = r'D:\PythonProject\data\processed_data\train_group2.csv'
    # TEST_FILE = r'D:\PythonProject\data\processed_data\merged_train_data_fixed.csv'

    # è¾“å‡ºè·¯å¾„é…ç½®
    OUTPUT_CONFIG = {
        'model_dir': r'D:\PythonProject\model\saved_model_improved',
        'plot_dir': r'D:\PythonProject\results\visualization_results_improved',
        'results_dir': r'D:\PythonProject\results\prediction_results_improved',
        'misclassified_dir': r'D:\PythonProject\results\misclassified_results_improved',
        'auto_create_dirs': True,
        'encoding': 'utf-8-sig',

        # è®­ç»ƒå‚æ•°
        'test_size': 0.0,  # ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ
        'random_state': 42,
        'save_model': True  # ä¿å­˜æ”¹è¿›åçš„æ¨¡å‹
    }

    print("ğŸ” é¦–å…ˆéªŒè¯æ•°æ®ä¸­çš„ç‰¹å¾...")

    # è¯»å–æ•°æ®å¹¶éªŒè¯ç‰¹å¾
    try:
        import pandas as pd

        df_sample = pd.read_csv(TEST_FILE, encoding='utf-8-sig')
        validate_features_exclusion(df_sample)
    except Exception as e:
        print(f"æ•°æ®è¯»å–å¤±è´¥: {e}")

    print(f"\n{'=' * 60}")
    print("ğŸš€ å¼€å§‹è¿è¡Œæ”¹è¿›ç‰ˆåˆ†ç±»å™¨...")
    print(f"{'=' * 60}")

    # è¿è¡Œæ”¹è¿›ç‰ˆåˆ†ç±»å™¨
    classifier = quick_run(TRAIN_FILE, TEST_FILE, OUTPUT_CONFIG)

    print(f"\nğŸ“ è¾“å‡ºç›®å½•:")
    print(f"- æ¨¡å‹ä¿å­˜: {OUTPUT_CONFIG['model_dir']}")
    print(f"- å›¾ç‰‡ä¿å­˜: {OUTPUT_CONFIG['plot_dir']}")
    print(f"- ç»“æœä¿å­˜: {OUTPUT_CONFIG['results_dir']}")
    print(f"- é”™è¯¯æ ·æœ¬: {OUTPUT_CONFIG['misclassified_dir']}")

    # print(f"\nâœ… æ”¹è¿›å®Œæˆï¼ä¸»è¦å˜åŒ–:")
    # print(f"   1. æ’é™¤äº† Obj_ID, CurCyc, TxSensID ç­‰æè¿°æ€§ç‰¹å¾")
    # print(f"   2. å¢åŠ äº†ç‰¹å¾è´¨é‡æ£€æŸ¥")
    # print(f"   3. ä¼˜åŒ–äº†æ¨¡å‹å‚æ•°ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ")
    # print(f"   4. å¢åŠ äº†ç‰¹å¾éªŒè¯åŠŸèƒ½")